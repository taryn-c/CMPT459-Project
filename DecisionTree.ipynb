{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocessing import *;\n",
    "from sklearn.model_selection import StratifiedKFold,KFold,train_test_split;\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder;\n",
    "from sklearn.tree import DecisionTreeClassifier;\n",
    "from sklearn.ensemble import RandomForestClassifier;\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report;\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer;\n",
    "from pandas.api.types import CategoricalDtype\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = train_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json(\"test.json\")\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert column to datetime\n",
    "test[\"created\"] = pd.to_datetime(test[\"created\"])\n",
    "# add hour created column\n",
    "test[\"hour_created\"] = test[\"created\"].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode\n",
    "test[\"address\"] = test[\"display_address\"].astype('category')\n",
    "test[\"address\"] = test[\"address\"].cat.codes\n",
    "test[\"manager_id\"] = test[\"manager_id\"].astype('category')\n",
    "test[\"manager\"] = test[\"manager_id\"].cat.codes\n",
    "test.drop(['manager_id'], axis=1, inplace=True)\n",
    "test[\"building_id\"] = test[\"building_id\"].astype('category')\n",
    "test[\"building\"] = test[\"building_id\"].cat.codes\n",
    "test.drop(['building_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.drop(['photos','street_address','features','description','created','display_address'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature Selection\n",
    "\n",
    "Since the listing_id is unique to each lising, it really isn't relevant for our model and will be removed. We will also remove the photos column because we won't be using this in our model either. We will also remove street_address because we can use the display_address as a categorical value since this is just the general street that the listing is on. This helps identify the general neighbourhood the listing is in for the buyer. The description, created, and features columns are removed because these are represented by newly extracted feature columns from the previous milestone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['photos','listing_id','street_address','features','description','created'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarize data by label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalDtype(categories=[u'high', u'medium', u'low'], ordered=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_type = CategoricalDtype(categories=[\"high\", \"medium\", \"low\"],ordered=True)\n",
    "cat_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high-0, medium-1, low-2\n",
    "data[\"interest_level\"] = data[\"interest_level\"].astype(cat_type)\n",
    "data[\"target\"] = data[\"interest_level\"].cat.codes\n",
    "data.drop(['interest_level'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"address\"] = data[\"display_address\"].astype('category')\n",
    "data[\"address\"] = data[\"address\"].cat.codes\n",
    "data.drop(['display_address'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"building_id\"] = data[\"building_id\"].astype('category')\n",
    "data[\"building\"] = data[\"building_id\"].cat.codes\n",
    "data.drop(['building_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"manager_id\"] = data[\"manager_id\"].astype('category')\n",
    "data[\"manager\"] = data[\"manager_id\"].cat.codes\n",
    "data.drop(['manager_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data\n",
    "\n",
    "I will split our input and target variables into X and y respectively. We will also split into training and test data so we can see if our models are overfitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['target'], axis=1)\n",
    "y = data['target']\n",
    "X_tr, X_test, y_tr, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.reset_index(inplace=True,drop=True)\n",
    "X_tr.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher score\n",
    "\n",
    "Since this is only valid for numerica features, we must first take a subset of our data. We will separate the data based on interest level to calculate the mean and standard deviation of each class. These values will be used to calculate the fisher score for each feature. We can then use a filtering technique to find the most relevant features based on the scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take subset\n",
    "numeric = ['bathrooms','bedrooms','latitude','longitude','price','hour_created','address','manager','building']\n",
    "num_data = data[['bathrooms','bedrooms','latitude','longitude','price','hour_created','address','manager','building','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# separate data by interest level\n",
    "high = num_data[num_data['target'] == 2].drop(['target'], axis=1)\n",
    "med = num_data[num_data['target'] == 1].drop(['target'], axis=1)\n",
    "low = num_data[num_data['target'] == 0].drop(['target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean of each feature in each class\n",
    "avg_if = np.array([high.mean(),\n",
    "      med.mean(),\n",
    "      low.mean()])\n",
    "#calculate mean of each feature\n",
    "avg_f = np.array(num_data.drop(['target'], axis=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate variance for each feature\n",
    "var = np.square(np.array([high.std(),\n",
    "      med.std(),\n",
    "      low.std()]))\n",
    "\n",
    "# calculate probability of each class\n",
    "prob = np.array([high.shape[0]/float(data.shape[0]),\n",
    "             med.shape[0]/float(data.shape[0]),\n",
    "             low.shape[0]/float(data.shape[0])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.4391986856950884e-10,\n",
       " 1.5495308224893388e-10,\n",
       " 1.0336764419360136e-10,\n",
       " 7.75380886324851e-11,\n",
       " 0.0008348517025325086,\n",
       " 0.000695716318490876,\n",
       " 0.0011911021471335808,\n",
       " 0.0010447074295786081,\n",
       " 0.0019208781511415576]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fisher = []\n",
    "numerator = []\n",
    "denomerator = []\n",
    "\n",
    "for feat in np.arange(0,avg_if.shape[1]):\n",
    "    for class_i in np.arange(0,avg_if.shape[0]):\n",
    "        numerator.append(np.array(prob[class_i]*np.square(avg_if[class_i][feat]-avg_f[feat])))\n",
    "        denomerator.append(np.array(prob[class_i]*var[class_i]))      \n",
    "    fisher.append(np.array(numerator).sum()/np.array(denomerator).sum())\n",
    "fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3 = ['building', 'manager', 'address']\n",
    "top5 = ['building', 'manager', 'address','price', 'hour_created']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Fisher scores, it looks like the building, address, and manager have the top 3 fisher scores, in that order.  Price and hour the listing was created also have decent fisher scores. The worst was longitude and the rest weren't that far off either with 10 significant digits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Version\n",
    "\n",
    "The classifier doesn't accept the extracted text features from the previous milestone so for the first classifier they were removed and the classifier was trained for the numerical data only. The 3 features with the highest fisher scores are selected for the first classifier. These are building, address, and manager. The default Gini index was used to determine the best split for each node. The classifier predicted with average 62.33% accuracy. You can see the classifier is biased and has higher precision, recall, and f1 scores for the low interest class. More than triple the high interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6208672086720867,\n",
       " 0.6334688346883469,\n",
       " 0.6313863667163573,\n",
       " 0.6250169399647649,\n",
       " 0.6305732484076433]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform cross-validation for first set of classifiers\n",
    "kf = KFold(n_splits=5)\n",
    "acc_scores = []\n",
    "best_acc = 0\n",
    "for train_index, valid_index in kf.split(X_tr):\n",
    "    # split data\n",
    "    X_train, X_valid = X_tr.iloc[train_index], X_tr.iloc[valid_index]\n",
    "    y_train, y_valid = y_tr.iloc[train_index], y_tr.iloc[valid_index]\n",
    "    # Create classifier object\n",
    "    clf = DecisionTreeClassifier(criterion=\"gini\", random_state=0)\n",
    "    # Train Decision Tree Classifer\n",
    "    clf = clf.fit(np.array(X_train[top3]),np.array(y_train),check_input=False)\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_valid[top3])\n",
    "    acc = accuracy_score(y_valid, y_pred)\n",
    "    acc_scores.append(acc)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_clf = clf\n",
    "acc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6282625196898398"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(acc_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.22      0.24      0.23       573\n",
      "      medium       0.34      0.36      0.35      1635\n",
      "         low       0.78      0.76      0.77      5171\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      7379\n",
      "   macro avg       0.45      0.45      0.45      7379\n",
      "weighted avg       0.64      0.63      0.64      7379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6224390243902439"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check overfitting\n",
    "y_pred = clf.predict(X_test[top3])\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use best classifier on test dataset\n",
    "y_pred = best_clf.predict_proba(test[top3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_id = test['listing_id'].values\n",
    "listing_id = listing_id.reshape(listing_id.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test output for log loss\n",
    "test_prob = np.concatenate((listing_id,y_pred),axis=1)\n",
    "np.savetxt('test_v1.csv',test_prob, delimiter=',',header='listing_id,high,medium,low')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Version\n",
    "\n",
    "Top 5 features according to Fisher scores were used for this classifier. Gini index was used to determine the best split for each node. The classifier predicted with the same average accuracy (62.49%) as the first version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6235772357723577,\n",
       " 0.631029810298103,\n",
       " 0.6347743596693319,\n",
       " 0.6072638568911777,\n",
       " 0.6279983737633825]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform cross-validation for first set of classifiers\n",
    "kf = KFold(n_splits=5)\n",
    "acc_scores = []\n",
    "for train_index, valid_index in kf.split(X_tr):\n",
    "    # split data\n",
    "    X_train, X_valid = X_tr.iloc[train_index], X_tr.iloc[valid_index]\n",
    "    y_train, y_valid = y_tr.iloc[train_index], y_tr.iloc[valid_index]\n",
    "    # Create classifier object\n",
    "    clf = DecisionTreeClassifier(criterion=\"gini\", random_state=0)\n",
    "    # Train Decision Tree Classifer\n",
    "    clf = clf.fit(np.array(X_train[top5]),np.array(y_train),check_input=False)\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_valid[top5])\n",
    "    acc = accuracy_score(y_valid, y_pred)\n",
    "    acc_scores.append(acc)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_clf = clf\n",
    "acc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6249287272788706"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(acc_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.23      0.24      0.24       573\n",
      "      medium       0.32      0.33      0.32      1635\n",
      "         low       0.78      0.76      0.77      5171\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      7379\n",
      "   macro avg       0.44      0.45      0.44      7379\n",
      "weighted avg       0.63      0.63      0.63      7379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625040650406504"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check overfitting\n",
    "y_pred = clf.predict(X_test[top5])\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Version\n",
    "\n",
    "All numeric features were used for this classifier. The default Gini index was used to determine the best split for each node like before. The classifier predicted with average 64.72% accuracy. It looks like the classifier has slightly better accuracy with all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6390243902439025,\n",
       " 0.6490514905149052,\n",
       " 0.6509012061254913,\n",
       " 0.6468356145819216,\n",
       " 0.6499525680986583]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform cross-validation for first set of classifiers\n",
    "kf = KFold(n_splits=5)\n",
    "acc_scores = []\n",
    "for train_index, valid_index in kf.split(X_tr):\n",
    "    # split data\n",
    "    X_train, X_valid = X_tr.iloc[train_index], X_tr.iloc[valid_index]\n",
    "    y_train, y_valid = y_tr.iloc[train_index], y_tr.iloc[valid_index]\n",
    "    # Create classifier object\n",
    "    clf = DecisionTreeClassifier(criterion=\"gini\", random_state=0)\n",
    "    # Train Decision Tree Classifer\n",
    "    clf = clf.fit(np.array(X_train[numeric]),np.array(y_train),check_input=False)\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_valid[numeric])\n",
    "    acc = accuracy_score(y_valid, y_pred)\n",
    "    acc_scores.append(acc)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_clf = clf\n",
    "acc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6471530539129758"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(acc_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.29      0.30      0.29       573\n",
      "      medium       0.34      0.36      0.35      1635\n",
      "         low       0.80      0.78      0.79      5171\n",
      "\n",
      "   micro avg       0.65      0.65      0.65      7379\n",
      "   macro avg       0.48      0.48      0.48      7379\n",
      "weighted avg       0.66      0.65      0.65      7379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6448780487804878"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check overfitting\n",
    "y_pred = clf.predict(X_test[numeric])\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth Version\n",
    "\n",
    "We can check the distribution of the target to see if stratifying the samples will help us identify any under-represented classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "int_level = train_df['interest_level'].value_counts()\n",
    "plt.figure(figsize=(15,6))\n",
    "sns.barplot(int_level.index, int_level.values, alpha=1, order=['low','medium','high'],color=color[0])\n",
    "plt.ylabel('Number of Occurrences', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Interest level', fontsize=14, fontweight='bold')\n",
    "plt.title('Target distribution', fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6947578104355957, 0.2276358314531374, 0.07760635811126694)\n"
     ]
    }
   ],
   "source": [
    "int_all = int_level.sum()\n",
    "prob_h = int_level[0]/float(int_all)\n",
    "prob_m = int_level[1]/float(int_all)\n",
    "prob_l = int_level[2]/float(int_all)\n",
    "print(prob_h, prob_m, prob_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the distribution of our target is skewed, we should stratify our data when spliting so it represents our data well. All numeric features were used for this classifier. Gini index was used to determine the best split for each node. A stratified k-fold cross validation method was used. The classifier predicted with average 65.02% accuracy, which is ever so slightly better than regular k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6395663956639567,\n",
       " 0.6504065040650406,\n",
       " 0.6535230352303523,\n",
       " 0.652798482179157,\n",
       " 0.654513418270534]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform cross-validation for first set of classifiers\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "acc_scores = []\n",
    "for train_index, valid_index in kf.split(X_tr, y_tr):\n",
    "    # split data\n",
    "    X_train, X_valid = X_tr.iloc[train_index], X_tr.iloc[valid_index]\n",
    "    y_train, y_valid = y_tr.iloc[train_index], y_tr.iloc[valid_index]\n",
    "    # Create classifier object\n",
    "    clf = DecisionTreeClassifier(criterion=\"gini\", random_state=0)\n",
    "    # Train Decision Tree Classifer\n",
    "    clf = clf.fit(np.array(X_train[numeric]),np.array(y_train),check_input=False)\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_valid[numeric])\n",
    "    acc = accuracy_score(y_valid, y_pred)\n",
    "    acc_scores.append(acc)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_clf = clf\n",
    "acc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6501615670818081"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(acc_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.28      0.30      0.29       571\n",
      "      medium       0.36      0.36      0.36      1668\n",
      "         low       0.80      0.79      0.79      5139\n",
      "\n",
      "   micro avg       0.65      0.65      0.65      7378\n",
      "   macro avg       0.48      0.48      0.48      7378\n",
      "weighted avg       0.66      0.65      0.66      7378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6455284552845528"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check overfitting\n",
    "y_pred = clf.predict(X_test[numeric])\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fifth Version\n",
    "\n",
    "All numeric features were used for this classifier. Entropy was used to determine the best split for each node. A stratified k-fold cross validation method was used. The classifier predicted with average 65.41% accuracy, which is ever so slightly worse than using gini index to find the best split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6490514905149052,\n",
       " 0.6574525745257452,\n",
       " 0.6621951219512195,\n",
       " 0.6548312779509419,\n",
       " 0.6470588235294118]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform cross-validation for first set of classifiers\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "acc_scores = []\n",
    "best_acc = 0\n",
    "for train_index, valid_index in kf.split(X_tr, y_tr):\n",
    "    # split data\n",
    "    X_train, X_valid = X_tr.iloc[train_index], X_tr.iloc[valid_index]\n",
    "    y_train, y_valid = y_tr.iloc[train_index], y_tr.iloc[valid_index]\n",
    "    # Create classifier object\n",
    "    clf = DecisionTreeClassifier(criterion=\"entropy\", random_state=0)\n",
    "    # Train Decision Tree Classifer\n",
    "    clf = clf.fit(np.array(X_train[numeric]),np.array(y_train),check_input=False)\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_valid[numeric])\n",
    "    acc = accuracy_score(y_valid, y_pred)\n",
    "    acc_scores.append(acc)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_clf = clf\n",
    "acc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6541178576944447"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(acc_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.29      0.31      0.30       571\n",
      "      medium       0.34      0.35      0.34      1668\n",
      "         low       0.80      0.78      0.79      5139\n",
      "\n",
      "   micro avg       0.65      0.65      0.65      7378\n",
      "   macro avg       0.47      0.48      0.48      7378\n",
      "weighted avg       0.66      0.65      0.65      7378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6452032520325204"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check overfitting\n",
    "y_pred = clf.predict(X_test[numeric])\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sixth Version\n",
    "\n",
    "All numeric features were used for this classifier. Gini index was used to determine the best split for each node since it performed slightly better. A stratified k-fold cross validation method was used. Minimum support was changed to 3 to combat overfitting. The classifier predicted with average 65.49% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6478319783197832,\n",
       " 0.6554200542005421,\n",
       " 0.6570460704607046,\n",
       " 0.6632335004743191,\n",
       " 0.6511249661154784]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform cross-validation for first set of classifiers\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "acc_scores = []\n",
    "for train_index, valid_index in kf.split(X_tr, y_tr):\n",
    "    # split data\n",
    "    X_train, X_valid = X_tr.iloc[train_index], X_tr.iloc[valid_index]\n",
    "    y_train, y_valid = y_tr.iloc[train_index], y_tr.iloc[valid_index]\n",
    "    # Create classifier object\n",
    "    clf = DecisionTreeClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=3)\n",
    "    # Train Decision Tree Classifer\n",
    "    clf = clf.fit(np.array(X_train[numeric]),np.array(y_train),check_input=False)\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_valid[numeric])\n",
    "    acc = accuracy_score(y_valid, y_pred)\n",
    "    acc_scores.append(acc)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_clf = clf\n",
    "acc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6549313139141655"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(acc_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.29      0.32      0.30       571\n",
      "      medium       0.35      0.36      0.35      1668\n",
      "         low       0.80      0.78      0.79      5139\n",
      "\n",
      "   micro avg       0.65      0.65      0.65      7378\n",
      "   macro avg       0.48      0.49      0.48      7378\n",
      "weighted avg       0.66      0.65      0.65      7378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6499186991869919"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check overfitting\n",
    "y_pred = clf.predict(X_test[numeric])\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seventh Version\n",
    "\n",
    "We can introduce our extracted text features for this version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hot encode features vector\n",
    "\n",
    "The Decision Tree classifier in the sklearn sorts each feature to split the data and doesn't support features that are vectors. So to use any text features we must hot encode them. However, the description vectors are 1119 long so we will only do the features. Looking at the two text features, it seems like description might not be that helpful because there are a lot of irrelevant words/strings in most of them like 'XXX'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = X_tr['feat_vect'].apply(lambda x: x.toarray().tolist())\n",
    "feat_list = []\n",
    "for i in feat:\n",
    "    feat_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X2 = pd.DataFrame(np.array(feat_list).reshape(np.array(feat_list).shape[0],np.array(feat_list).shape[2]))\n",
    "X2 = X_tr[numeric].join(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = X_test['feat_vect'].apply(lambda x: x.toarray().tolist())\n",
    "feat_list = []\n",
    "for i in feat:\n",
    "    feat_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test2 = pd.DataFrame(np.array(feat_list).reshape(np.array(feat_list).shape[0],np.array(feat_list).shape[2]))\n",
    "X_test = X_test[numeric].join(X_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All numeric features and the hot encoded features were used for this classifier. Gini was used to determine the best split for each node. A stratified k-fold cross validation method was used. Minimum support was changed to 3 to combat overfitting seen from the previous versions. The classifier predicted with average 53.55% accuracy, which is significantly worse than the previous. Looking at the tfidf vectors of the features column, it looks like each one only has one value out of the 79 word dictionary. The features of most listing must be very similar or have very rare words. The classifier is no longer overfitting but the hot encoded data is throwing off the classifier. It must be splitting the data using these columns closer to the root node, which isn't very helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6547425474254742,\n",
       " 0.6592140921409214,\n",
       " 0.6593495934959349,\n",
       " 0.654018159642228,\n",
       " 0.6603415559772297]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform cross-validation for first set of classifiers\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "acc_scores = []\n",
    "for train_index, valid_index in kf.split(X2, y_tr):\n",
    "    # split data\n",
    "    X_train, X_valid = X2.iloc[train_index], X2.iloc[valid_index]\n",
    "    y_train, y_valid = y_tr.iloc[train_index], y_tr.iloc[valid_index]\n",
    "    # Create classifier object\n",
    "    clf = DecisionTreeClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=3)\n",
    "    # Train Decision Tree Classifer\n",
    "    clf = clf.fit(np.array(X_train),np.array(y_train),check_input=False)\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_valid)\n",
    "    acc = accuracy_score(y_valid, y_pred)\n",
    "    acc_scores.append(acc)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_clf = clf\n",
    "acc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6575331897363575"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(acc_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.28      0.29      0.28       571\n",
      "      medium       0.37      0.37      0.37      1668\n",
      "         low       0.80      0.80      0.80      5139\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      7378\n",
      "   macro avg       0.48      0.48      0.48      7378\n",
      "weighted avg       0.66      0.66      0.66      7378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6514634146341464"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check overfitting\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eighth Version\n",
    "\n",
    "All numeric features and the features were used for this classifier. Gini index seemed to do slightly better so it was used to determine the best split for each node. A stratified k-fold cross validation method was used. Minimum support was changed to 100 and the classifier predicted with average 71.13% accuracy, which is a large improvement. Recall the low interest class was at an all time high with 1.00. The classifier doesn't seem to be predicting the medium or high interest classes corectly at all. There seems to be no overfitting since the performance on the test dataset is only off by less than 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7065040650406504,\n",
       " 0.7149051490514905,\n",
       " 0.7116531165311654,\n",
       " 0.7048380539368478,\n",
       " 0.7186229330441855]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform cross-validation for first set of classifiers\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "acc_scores = []\n",
    "for train_index, valid_index in kf.split(X2, y_tr):\n",
    "    # split data\n",
    "    X_train, X_valid = X2.iloc[train_index], X2.iloc[valid_index]\n",
    "    y_train, y_valid = y_tr.iloc[train_index], y_tr.iloc[valid_index]\n",
    "    # Create classifier object\n",
    "    clf = DecisionTreeClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=100)\n",
    "    # Train Decision Tree Classifer\n",
    "    clf = clf.fit(np.array(X_train),np.array(y_train),check_input=False)\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_valid)\n",
    "    acc = accuracy_score(y_valid, y_pred)\n",
    "    acc_scores.append(acc)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_clf = clf\n",
    "acc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.711304663520868"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(acc_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.51      0.17      0.25       571\n",
      "      medium       0.44      0.26      0.33      1668\n",
      "         low       0.77      0.93      0.84      5139\n",
      "\n",
      "   micro avg       0.72      0.72      0.72      7378\n",
      "   macro avg       0.57      0.45      0.48      7378\n",
      "weighted avg       0.68      0.72      0.68      7378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7078048780487805"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check overfitting\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ninth Version\n",
    "\n",
    "Here we're trying a Random Forest to try and combat inconsistent data creating very different trees. Playing with the parameters, number of folds, and features also gets the same results, on average, as before. Although we do get one classifier that performs the best with an accuracy of 69.92%. This classifier will be used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6998103494987808,\n",
       " 0.697913844486589,\n",
       " 0.6987266323489569,\n",
       " 0.7010840108401084,\n",
       " 0.6994579945799458,\n",
       " 0.7001897533206831,\n",
       " 0.6985632962862565,\n",
       " 0.700731905665492,\n",
       " 0.698292220113852,\n",
       " 0.6968546637744034]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform cross-validation for first set of classifiers\n",
    "kf = StratifiedKFold(n_splits=10)\n",
    "acc_scores = []\n",
    "for train_index, valid_index in kf.split(X2, y_tr):\n",
    "    # split data\n",
    "    X_train, X_valid = X2.iloc[train_index], X2.iloc[valid_index]\n",
    "    y_train, y_valid = y_tr.iloc[train_index], y_tr.iloc[valid_index]\n",
    "    # Create classifier object\n",
    "    clf = RandomForestClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=20,n_estimators=50,min_samples_split=100)\n",
    "    # Train Decision Tree Classifer\n",
    "    clf = clf.fit(np.array(X_train),np.array(y_train))\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_valid)\n",
    "    acc = accuracy_score(y_valid, y_pred)\n",
    "    acc_scores.append(acc)\n",
    "    if acc > best_acc:\n",
    "        best_clf = clf\n",
    "acc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6991624670915069"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(acc_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.67      0.01      0.01       285\n",
      "      medium       0.30      0.01      0.02       834\n",
      "         low       0.70      1.00      0.82      2569\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      3688\n",
      "   macro avg       0.55      0.34      0.28      3688\n",
      "weighted avg       0.61      0.70      0.58      3688\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6895121951219512"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check overfitting\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tenth Version\n",
    "\n",
    "Here we're trying a Random Forest to try and combat inconsistent data creating very different trees. Playing with the parameters, number of folds, and features also gets the same results, on average, as before. Although we do get one classifier that performs the best with an accuracy of 72.55%. This classifier will be used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7233811975074506,\n",
       " 0.7206719046328908,\n",
       " 0.7282579246816581,\n",
       " 0.7284552845528456,\n",
       " 0.724390243902439,\n",
       " 0.7300081322851721,\n",
       " 0.7199783139062076,\n",
       " 0.720791542423421,\n",
       " 0.7343453510436433,\n",
       " 0.7242407809110629]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform cross-validation for first set of classifiers\n",
    "kf = StratifiedKFold(n_splits=10)\n",
    "acc_scores = []\n",
    "for train_index, valid_index in kf.split(X2, y_tr):\n",
    "    # split data\n",
    "    X_train, X_valid = X2.iloc[train_index], X2.iloc[valid_index]\n",
    "    y_train, y_valid = y_tr.iloc[train_index], y_tr.iloc[valid_index]\n",
    "    # Create classifier object\n",
    "    clf = RandomForestClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=20,n_estimators=50)\n",
    "    # Train Decision Tree Classifer\n",
    "    clf = clf.fit(np.array(X_train[numeric]),np.array(y_train))\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_valid[numeric])\n",
    "    acc = accuracy_score(y_valid, y_pred)\n",
    "    acc_scores.append(acc)\n",
    "    if acc > best_acc:\n",
    "        best_clf = clf\n",
    "acc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7254520675846791"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(acc_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.47      0.14      0.22       285\n",
      "      medium       0.48      0.20      0.28       834\n",
      "         low       0.76      0.96      0.85      2569\n",
      "\n",
      "   micro avg       0.72      0.72      0.72      3688\n",
      "   macro avg       0.57      0.43      0.45      3688\n",
      "weighted avg       0.67      0.72      0.67      3688\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7169105691056911"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check overfitting\n",
    "y_pred = clf.predict(X_test[numeric])\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predictions on  test dataset\n",
    "\n",
    "Score (multiclass log-loss) = 0.65105 using a Random Forest classifier, the eighth version of classifiers. The benchmark of sample submission is 0.78598 so our classifier isnt that far off. \n",
    "\n",
    "- Decision-tree learners can create over-complex trees that do not generalise the data well. This is called overfitting. Mechanisms such as pruning (not currently supported), setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n",
    "\n",
    "- Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use best classifier on test dataset\n",
    "y_pred = best_clf.predict_proba(test[numeric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test output for log loss\n",
    "test_prob = np.concatenate((listing_id,y_pred),axis=1)\n",
    "np.savetxt('test_best.csv',test_prob, delimiter=',',header='listing_id,high,medium,low')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,Rmd"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
