{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from Preprocessing import *;\n",
    "from sklearn.model_selection import StratifiedKFold,KFold,train_test_split;\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder;\n",
    "from sklearn.tree import DecisionTreeClassifier;\n",
    "from sklearn.ensemble import RandomForestClassifier;\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report;\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer;\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = train_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature Selection\n",
    "\n",
    "Since the listing_id is unique to each lising, it really isn't relevant for our model and will be removed. We will also remove the photos column because we won't be using this in our model either. We will also remove street_address because we can use the display_address as a categorical value since this is just the general street that the listing is on. This helps identify the general neighbourhood the listing is in for the buyer. The description, created, and features columns are removed because these are represented by newly extracted feature columns from the previous milestone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['photos','listing_id','street_address','features','description','created'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarize data by label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalDtype(categories=[u'high', u'medium', u'low'], ordered=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_type = CategoricalDtype(categories=[\"high\", \"medium\", \"low\"],ordered=True)\n",
    "cat_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high-0, medium-1, low-2\n",
    "data[\"interest_level\"] = data[\"interest_level\"].astype(cat_type)\n",
    "data[\"target\"] = data[\"interest_level\"].cat.codes\n",
    "data.drop(['interest_level'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"address\"] = data[\"display_address\"].astype('category')\n",
    "data[\"address\"] = data[\"address\"].cat.codes\n",
    "data.drop(['display_address'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"building_id\"] = data[\"building_id\"].astype('category')\n",
    "data[\"building\"] = data[\"building_id\"].cat.codes\n",
    "data.drop(['building_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"manager_id\"] = data[\"manager_id\"].astype('category')\n",
    "data[\"manager\"] = data[\"manager_id\"].cat.codes\n",
    "data.drop(['manager_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split our data\n",
    "\n",
    "I will split our input and target variables into X and y respectively. Next, we will check the distribution of our target variable to see if we must stratify our training and testing data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['target'], axis=1)\n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher score\n",
    "\n",
    "Since this is only valid for numerica features, we must first take a subset of our data. We will separate the data based on interest level to calculate the mean and standard deviation of each class. These values will be used to calculate the fisher score for each feature. We can then use a filtering technique to find the most relevant features based on the scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take subset\n",
    "numeric = ['bathrooms','bedrooms','latitude','longitude','price','hour_created','address','manager','building']\n",
    "num_data = data[['bathrooms','bedrooms','latitude','longitude','price','hour_created','address','manager','building','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# separate data by interest level\n",
    "high = num_data[num_data['target'] == 2].drop(['target'], axis=1)\n",
    "med = num_data[num_data['target'] == 1].drop(['target'], axis=1)\n",
    "low = num_data[num_data['target'] == 0].drop(['target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean of each feature in each class\n",
    "avg_if = np.array([high.mean(),\n",
    "      med.mean(),\n",
    "      low.mean()])\n",
    "#calculate mean of each feature\n",
    "avg_f = np.array(num_data.drop(['target'], axis=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate variance for each feature\n",
    "var = np.square(np.array([high.std(),\n",
    "      med.std(),\n",
    "      low.std()]))\n",
    "\n",
    "# calculate probability of each class\n",
    "prob = np.array([high.shape[0]/float(data.shape[0]),\n",
    "             med.shape[0]/float(data.shape[0]),\n",
    "             low.shape[0]/float(data.shape[0])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.4391986856950884e-10,\n",
       " 1.5495308224893388e-10,\n",
       " 1.0336764419360136e-10,\n",
       " 7.75380886324851e-11,\n",
       " 0.0008348517025325086,\n",
       " 0.000695716318490876,\n",
       " 0.0011911021471335808,\n",
       " 0.0010447074295786081,\n",
       " 0.0019208781511415576]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fisher = []\n",
    "numerator = []\n",
    "denomerator = []\n",
    "\n",
    "for feat in np.arange(0,avg_if.shape[1]):\n",
    "    for class_i in np.arange(0,avg_if.shape[0]):\n",
    "        numerator.append(np.array(prob[class_i]*np.square(avg_if[class_i][feat]-avg_f[feat])))\n",
    "        denomerator.append(np.array(prob[class_i]*var[class_i]))      \n",
    "    fisher.append(np.array(numerator).sum()/np.array(denomerator).sum())\n",
    "fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3 = ['building', 'manager', 'address']\n",
    "top5 = ['building', 'manager', 'address','price', 'hour_created']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the fisher scores, it looks like the building, address, and manager have the top 3 fisher scores, in that order.  Price and hour the listing was created also have decent fisher scores. The worst was longitude and the rest weren't that far off either with 10 significant digits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Version\n",
    "\n",
    "The classifier doesn't accept the extracted text features from the previous milestone so for the first classifier they were removed and the classifier was trained for the numerical data only. The 3 features with the highest fisher scores are selected for the first classifier. These are price, hour created, and bedrooms. The default Gini index was used to determine the best split for each node. The classifier predicted with average 63.32% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6341463414634146,\n",
       " 0.6279471544715447,\n",
       " 0.6371582477894094,\n",
       " 0.6363451570281533,\n",
       " 0.6324829759121862]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform cross-validation for first set of classifiers\n",
    "kf = KFold(n_splits=5)\n",
    "acc_scores = []\n",
    "for train_index, valid_index in kf.split(X):\n",
    "    # split data\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "    y_train, y_valid = y[train_index], y[valid_index]\n",
    "    # Create classifier object\n",
    "    clf = DecisionTreeClassifier(criterion=\"gini\", random_state=0)\n",
    "    # Train Decision Tree Classifer\n",
    "    clf = clf.fit(np.array(X_train[top3]),np.array(y_train),check_input=False)\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_valid[top3])\n",
    "    acc_scores.append(accuracy_score(y_valid, y_pred))\n",
    "acc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6336159753329417"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(acc_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.23      0.26      0.24       781\n",
      "      medium       0.36      0.38      0.37      2267\n",
      "        high       0.79      0.76      0.77      6791\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      9839\n",
      "   macro avg       0.46      0.47      0.46      9839\n",
      "weighted avg       0.64      0.63      0.64      9839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Version\n",
    "\n",
    "Top 5 features according to Fisher scores were used for this classifier. Gini index was used to determine the best split for each node. The classifier predicted with the same average accuracy (63.32%) as the first version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6330284552845529,\n",
       " 0.6270325203252033,\n",
       " 0.6285191584510621,\n",
       " 0.6381746112409797,\n",
       " 0.6302469763187316]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform cross-validation for first set of classifiers\n",
    "kf = KFold(n_splits=5)\n",
    "acc_scores = []\n",
    "for train_index, valid_index in kf.split(X):\n",
    "    # split data\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "    y_train, y_valid = y[train_index], y[valid_index]\n",
    "    # Create classifier object\n",
    "    clf = DecisionTreeClassifier(criterion=\"gini\", random_state=0)\n",
    "    # Train Decision Tree Classifer\n",
    "    clf = clf.fit(np.array(X_train[top5]),np.array(y_train),check_input=False)\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_valid[top5])\n",
    "    acc_scores.append(accuracy_score(y_valid, y_pred))\n",
    "acc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.631400344324106"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(acc_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.27      0.30      0.28       781\n",
      "      medium       0.34      0.34      0.34      2267\n",
      "        high       0.78      0.77      0.77      6791\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      9839\n",
      "   macro avg       0.46      0.47      0.46      9839\n",
      "weighted avg       0.64      0.63      0.63      9839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Version\n",
    "\n",
    "All numeric features were used for this classifier. The default Gini index was used to determine the best split for each node like before. The classifier predicted with average 65.66% accuracy. It looks like the classifier has slightly better accuracy with all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6579268292682927,\n",
       " 0.6576219512195122,\n",
       " 0.6517938814920216,\n",
       " 0.6518955178371786,\n",
       " 0.6563675170240878]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform cross-validation for first set of classifiers\n",
    "kf = KFold(n_splits=5)\n",
    "acc_scores = []\n",
    "for train_index, valid_index in kf.split(X):\n",
    "    # split data\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "    y_train, y_valid = y[train_index], y[valid_index]\n",
    "    # Create classifier object\n",
    "    clf = DecisionTreeClassifier(criterion=\"gini\", random_state=0)\n",
    "    # Train Decision Tree Classifer\n",
    "    clf = clf.fit(np.array(X_train[numeric]),np.array(y_train),check_input=False)\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_valid[numeric])\n",
    "    acc_scores.append(accuracy_score(y_valid, y_pred))\n",
    "acc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6551211393682186"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(acc_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.30      0.33      0.32       781\n",
      "      medium       0.38      0.39      0.39      2267\n",
      "        high       0.80      0.78      0.79      6791\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      9839\n",
      "   macro avg       0.49      0.50      0.50      9839\n",
      "weighted avg       0.66      0.66      0.66      9839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth Version\n",
    "\n",
    "We can check the distribution of the target to see if stratifying the samples will help us identify any under-represented classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "int_level = train_df['interest_level'].value_counts()\n",
    "plt.figure(figsize=(15,6))\n",
    "sns.barplot(int_level.index, int_level.values, alpha=1, order=['low','medium','high'],color=color[0])\n",
    "plt.ylabel('Number of Occurrences', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Interest level', fontsize=14, fontweight='bold')\n",
    "plt.title('Target distribution', fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the distribution of our target is skewed, we should stratify our data when spliting so it represents our data well. All numeric features were used for this classifier. Gini index was used to determine the best split for each node. A stratified k-fold cross validation method was used. The classifier predicted with average 65.74% accuracy, which is ever so slightly better than regular k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6536585365853659,\n",
       " 0.65630081300813,\n",
       " 0.6578252032520325,\n",
       " 0.6506758816952942,\n",
       " 0.659890221589754]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform cross-validation for first set of classifiers\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "acc_scores = []\n",
    "for train_index, valid_index in kf.split(X, y):\n",
    "    # split data\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "    y_train, y_valid = y[train_index], y[valid_index]\n",
    "    # Create classifier object\n",
    "    clf = DecisionTreeClassifier(criterion=\"gini\", random_state=0)\n",
    "    # Train Decision Tree Classifer\n",
    "    clf = clf.fit(np.array(X_train[numeric]),np.array(y_train),check_input=False)\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_valid[numeric])\n",
    "    acc_scores.append(accuracy_score(y_valid, y_pred))\n",
    "acc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6556701312261153"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(acc_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.32      0.35      0.34       763\n",
      "      medium       0.37      0.39      0.38      2239\n",
      "        high       0.80      0.78      0.79      6836\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      9838\n",
      "   macro avg       0.50      0.51      0.50      9838\n",
      "weighted avg       0.67      0.66      0.66      9838\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fifth Version\n",
    "\n",
    "All numeric features were used for this classifier. Entropy was used to determine the best split for each node. A stratified k-fold cross validation method was used. The classifier predicted with average 65.62% accuracy, which is ever so slightly worse than using gini index to find the best split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6553861788617886,\n",
       " 0.6532520325203252,\n",
       " 0.6630081300813008,\n",
       " 0.6530135176339059,\n",
       " 0.6511486074405367]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform cross-validation for first set of classifiers\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "acc_scores = []\n",
    "for train_index, valid_index in kf.split(X, y):\n",
    "    # split data\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "    y_train, y_valid = y[train_index], y[valid_index]\n",
    "    # Create classifier object\n",
    "    clf = DecisionTreeClassifier(criterion=\"entropy\", random_state=0)\n",
    "    # Train Decision Tree Classifer\n",
    "    clf = clf.fit(np.array(X_train[numeric]),np.array(y_train),check_input=False)\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_valid[numeric])\n",
    "    acc_scores.append(accuracy_score(y_valid, y_pred))\n",
    "acc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6551616933075715"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(acc_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.32      0.35      0.34       763\n",
      "      medium       0.35      0.36      0.36      2239\n",
      "        high       0.79      0.78      0.79      6836\n",
      "\n",
      "   micro avg       0.65      0.65      0.65      9838\n",
      "   macro avg       0.49      0.50      0.49      9838\n",
      "weighted avg       0.66      0.65      0.65      9838\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sixth Version\n",
    "\n",
    "All numeric features were used for this classifier. Gini index was used to determine the best split for each node since it performed slightly better. A stratified k-fold cross validation method was used. Minimum support was changed to 3 to combat overfitting. The classifier predicted with average 66.46% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6525406504065041,\n",
       " 0.6616869918699188,\n",
       " 0.6590447154471545,\n",
       " 0.6619575160077243,\n",
       " 0.655621061191299]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform cross-validation for first set of classifiers\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "acc_scores = []\n",
    "for train_index, valid_index in kf.split(X, y):\n",
    "    # split data\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "    y_train, y_valid = y[train_index], y[valid_index]\n",
    "    # Create classifier object\n",
    "    clf = DecisionTreeClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=3)\n",
    "    # Train Decision Tree Classifer\n",
    "    clf = clf.fit(np.array(X_train[numeric]),np.array(y_train),check_input=False)\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_valid[numeric])\n",
    "    acc_scores.append(accuracy_score(y_valid, y_pred))\n",
    "acc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6581701869845202"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(acc_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.32      0.36      0.34       763\n",
      "      medium       0.36      0.38      0.37      2239\n",
      "        high       0.80      0.78      0.79      6836\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      9838\n",
      "   macro avg       0.49      0.50      0.50      9838\n",
      "weighted avg       0.66      0.66      0.66      9838\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hot encode features vector\n",
    "\n",
    "The Decision Tree classifier in the sklearn sorts each feature to split the data and doesn't support features that are vectors. So to use any text features we must hot encode them. However, the description vectors are 1119 long so we will only do the features. Looking at the two text features, it seems like description might not be that helpful because there are a lot of irrelevant words/strings in most of them like 'XXX'.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = data['feat_vect'].apply(lambda x: x.toarray().tolist())\n",
    "feat_list = []\n",
    "for i in feat:\n",
    "    feat_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X2 = pd.DataFrame(np.array(feat_list).reshape(np.array(feat_list).shape[0],np.array(feat_list).shape[2]))\n",
    "X2 = X[numeric].join(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seventh Version\n",
    "\n",
    "All numeric features and the hot encoded features were used for this classifier. Gini was used to determine the best split for each node. A stratified k-fold cross validation method was used. Minimum support was changed to 3 to combat overfitting. The classifier predicted with average 66.67% accuracy, which is slightly better than the previous. Looking at the tfidf vectors of the features column, it looks like each one only has one value out of the 79 word dictionary. The features of most listing must be very similar or have very rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6539634146341463,\n",
       " 0.6600609756097561,\n",
       " 0.6626016260162602,\n",
       " 0.6632787884947657,\n",
       " 0.6591786948566782]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform cross-validation for first set of classifiers\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "acc_scores = []\n",
    "for train_index, valid_index in kf.split(X2, y):\n",
    "    # split data\n",
    "    X_train, X_valid = X2.iloc[train_index], X2.iloc[valid_index]\n",
    "    y_train, y_valid = y[train_index], y[valid_index]\n",
    "    # Create classifier object\n",
    "    clf = DecisionTreeClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=3)\n",
    "    # Train Decision Tree Classifer\n",
    "    clf = clf.fit(np.array(X_train),np.array(y_train),check_input=False)\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_valid)\n",
    "    acc_scores.append(accuracy_score(y_valid, y_pred))\n",
    "acc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6598166999223213"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(acc_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.33      0.37      0.35       763\n",
      "      medium       0.37      0.40      0.38      2239\n",
      "        high       0.81      0.78      0.79      6836\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      9838\n",
      "   macro avg       0.50      0.52      0.51      9838\n",
      "weighted avg       0.67      0.66      0.66      9838\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seventh Version\n",
    "\n",
    "All numeric features and the features were used for this classifier. Gini index seemed to do slightly better so it was used to determine the best split for each node. A stratified k-fold cross validation method was used. Minimum support was changed to 100 and the classifier predicted with average 71.21% accuracy, which is an improvement. Recall and f1-score for medium interest class was at an all time high with 0.92 and 0.83 respectively. Precision seemed to improve for all classes by 0.17 for low and 0.06 for high. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7078252032520326,\n",
       " 0.7138211382113822,\n",
       " 0.714329268292683,\n",
       " 0.7145035064539079,\n",
       " 0.7076641593819882]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform cross-validation for first set of classifiers\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "acc_scores = []\n",
    "best_acc = 0\n",
    "for train_index, valid_index in kf.split(X2, y):\n",
    "    # split data\n",
    "    X_train, X_valid = X2.iloc[train_index], X2.iloc[valid_index]\n",
    "    y_train, y_valid = y[train_index], y[valid_index]\n",
    "    # Create classifier object\n",
    "    clf = DecisionTreeClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=100)\n",
    "    # Train Decision Tree Classifer\n",
    "    clf = clf.fit(np.array(X_train),np.array(y_train),check_input=False)\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_valid)\n",
    "    acc = accuracy_score(y_valid, y_pred)\n",
    "    acc_scores.append(acc)\n",
    "    if acc > best_acc:\n",
    "        best_clf = clf\n",
    "acc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7116286551183988"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(acc_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.48      0.14      0.22       763\n",
      "      medium       0.42      0.28      0.33      2239\n",
      "        high       0.77      0.91      0.83      6836\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      9838\n",
      "   macro avg       0.56      0.44      0.46      9838\n",
      "weighted avg       0.67      0.71      0.67      9838\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Testing\n",
    "\n",
    "## Processing test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json(\"test.json\")\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert column to datetime\n",
    "test[\"created\"] = pd.to_datetime(test[\"created\"])\n",
    "# add hour created column\n",
    "test[\"hour_created\"] = test[\"created\"].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode\n",
    "test[\"address\"] = test[\"display_address\"].astype('category')\n",
    "test[\"address\"] = test[\"address\"].cat.codes\n",
    "test[\"manager_id\"] = test[\"manager_id\"].astype('category')\n",
    "test[\"manager\"] = test[\"manager_id\"].cat.codes\n",
    "test.drop(['manager_id'], axis=1, inplace=True)\n",
    "test[\"building_id\"] = test[\"building_id\"].astype('category')\n",
    "test[\"building\"] = test[\"building_id\"].cat.codes\n",
    "test.drop(['building_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert features list to string\n",
    "test['features'] = test['features'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features using tfidf\n",
    "vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', token_pattern=r'^[a-zA-Z][a-zA-Z]+',max_features=79)\n",
    "vectorizer.fit(test['features'].values);\n",
    "test['feat_vect'] = test['features'].apply(lambda x: vectorizer.transform([x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert features vectors to list\n",
    "feat = test['feat_vect'].apply(lambda x: x.toarray().tolist())\n",
    "feat_list = []\n",
    "for i in feat:\n",
    "    feat_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create dataframe of hot encoded feature vector\n",
    "test2 = pd.DataFrame(np.array(feat_list).reshape(np.array(feat_list).shape[0],np.array(feat_list).shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join hot encoded features to test dataset\n",
    "test = test.join(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>latitude</th>\n",
       "      <th>listing_id</th>\n",
       "      <th>longitude</th>\n",
       "      <th>price</th>\n",
       "      <th>hour_created</th>\n",
       "      <th>address</th>\n",
       "      <th>manager</th>\n",
       "      <th>building</th>\n",
       "      <th>...</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.7185</td>\n",
       "      <td>7142618</td>\n",
       "      <td>-73.9865</td>\n",
       "      <td>2950</td>\n",
       "      <td>5</td>\n",
       "      <td>9506</td>\n",
       "      <td>2694</td>\n",
       "      <td>4412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7278</td>\n",
       "      <td>7210040</td>\n",
       "      <td>-74.0000</td>\n",
       "      <td>2850</td>\n",
       "      <td>6</td>\n",
       "      <td>9589</td>\n",
       "      <td>3145</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.7306</td>\n",
       "      <td>7103890</td>\n",
       "      <td>-73.9890</td>\n",
       "      <td>3758</td>\n",
       "      <td>4</td>\n",
       "      <td>660</td>\n",
       "      <td>2346</td>\n",
       "      <td>2257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7109</td>\n",
       "      <td>7143442</td>\n",
       "      <td>-73.9571</td>\n",
       "      <td>3300</td>\n",
       "      <td>6</td>\n",
       "      <td>318</td>\n",
       "      <td>179</td>\n",
       "      <td>4368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7650</td>\n",
       "      <td>6860601</td>\n",
       "      <td>-73.9845</td>\n",
       "      <td>4900</td>\n",
       "      <td>5</td>\n",
       "      <td>8767</td>\n",
       "      <td>2764</td>\n",
       "      <td>3530</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>40.7901</td>\n",
       "      <td>6840081</td>\n",
       "      <td>-73.9774</td>\n",
       "      <td>9000</td>\n",
       "      <td>6</td>\n",
       "      <td>10826</td>\n",
       "      <td>2259</td>\n",
       "      <td>5678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7730</td>\n",
       "      <td>6922337</td>\n",
       "      <td>-73.9571</td>\n",
       "      <td>2800</td>\n",
       "      <td>3</td>\n",
       "      <td>7117</td>\n",
       "      <td>1205</td>\n",
       "      <td>4676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.6751</td>\n",
       "      <td>6913616</td>\n",
       "      <td>-73.9511</td>\n",
       "      <td>1900</td>\n",
       "      <td>6</td>\n",
       "      <td>9283</td>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7597</td>\n",
       "      <td>6937820</td>\n",
       "      <td>-73.9929</td>\n",
       "      <td>3000</td>\n",
       "      <td>5</td>\n",
       "      <td>11128</td>\n",
       "      <td>3467</td>\n",
       "      <td>2338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.7208</td>\n",
       "      <td>6893933</td>\n",
       "      <td>-73.9887</td>\n",
       "      <td>2300</td>\n",
       "      <td>3</td>\n",
       "      <td>9009</td>\n",
       "      <td>381</td>\n",
       "      <td>4516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.7330</td>\n",
       "      <td>6832604</td>\n",
       "      <td>-73.9960</td>\n",
       "      <td>4800</td>\n",
       "      <td>6</td>\n",
       "      <td>4693</td>\n",
       "      <td>3480</td>\n",
       "      <td>1267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.7575</td>\n",
       "      <td>6915282</td>\n",
       "      <td>-73.9656</td>\n",
       "      <td>2195</td>\n",
       "      <td>2</td>\n",
       "      <td>6923</td>\n",
       "      <td>561</td>\n",
       "      <td>6760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7418</td>\n",
       "      <td>7127565</td>\n",
       "      <td>-73.9798</td>\n",
       "      <td>4150</td>\n",
       "      <td>13</td>\n",
       "      <td>2801</td>\n",
       "      <td>214</td>\n",
       "      <td>3612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>40.7399</td>\n",
       "      <td>6827899</td>\n",
       "      <td>-73.9864</td>\n",
       "      <td>5200</td>\n",
       "      <td>1</td>\n",
       "      <td>9040</td>\n",
       "      <td>383</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>40.7554</td>\n",
       "      <td>6934855</td>\n",
       "      <td>-73.9692</td>\n",
       "      <td>4395</td>\n",
       "      <td>1</td>\n",
       "      <td>7708</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.7412</td>\n",
       "      <td>6861826</td>\n",
       "      <td>-73.9772</td>\n",
       "      <td>3050</td>\n",
       "      <td>6</td>\n",
       "      <td>7624</td>\n",
       "      <td>3467</td>\n",
       "      <td>5783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.7476</td>\n",
       "      <td>6871643</td>\n",
       "      <td>-73.9760</td>\n",
       "      <td>2395</td>\n",
       "      <td>3</td>\n",
       "      <td>6793</td>\n",
       "      <td>3321</td>\n",
       "      <td>5926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.7509</td>\n",
       "      <td>6842542</td>\n",
       "      <td>-73.9704</td>\n",
       "      <td>3950</td>\n",
       "      <td>2</td>\n",
       "      <td>126</td>\n",
       "      <td>3008</td>\n",
       "      <td>3142</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.7075</td>\n",
       "      <td>6934145</td>\n",
       "      <td>-74.0113</td>\n",
       "      <td>2977</td>\n",
       "      <td>17</td>\n",
       "      <td>8042</td>\n",
       "      <td>3467</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7460</td>\n",
       "      <td>6829365</td>\n",
       "      <td>-73.9754</td>\n",
       "      <td>2995</td>\n",
       "      <td>3</td>\n",
       "      <td>1874</td>\n",
       "      <td>3467</td>\n",
       "      <td>7038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.7586</td>\n",
       "      <td>7167858</td>\n",
       "      <td>-73.9838</td>\n",
       "      <td>2200</td>\n",
       "      <td>3</td>\n",
       "      <td>11145</td>\n",
       "      <td>1632</td>\n",
       "      <td>897</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.7746</td>\n",
       "      <td>6859483</td>\n",
       "      <td>-73.9535</td>\n",
       "      <td>2295</td>\n",
       "      <td>3</td>\n",
       "      <td>7157</td>\n",
       "      <td>790</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7297</td>\n",
       "      <td>6861377</td>\n",
       "      <td>-73.9860</td>\n",
       "      <td>3675</td>\n",
       "      <td>5</td>\n",
       "      <td>7491</td>\n",
       "      <td>1798</td>\n",
       "      <td>86</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>40.7318</td>\n",
       "      <td>6848960</td>\n",
       "      <td>-73.9822</td>\n",
       "      <td>4400</td>\n",
       "      <td>4</td>\n",
       "      <td>577</td>\n",
       "      <td>3691</td>\n",
       "      <td>1119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.7412</td>\n",
       "      <td>6918850</td>\n",
       "      <td>-73.9772</td>\n",
       "      <td>2970</td>\n",
       "      <td>3</td>\n",
       "      <td>7624</td>\n",
       "      <td>1964</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.7762</td>\n",
       "      <td>6916867</td>\n",
       "      <td>-73.9775</td>\n",
       "      <td>2000</td>\n",
       "      <td>12</td>\n",
       "      <td>10431</td>\n",
       "      <td>3227</td>\n",
       "      <td>3578</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7191</td>\n",
       "      <td>6895840</td>\n",
       "      <td>-74.0109</td>\n",
       "      <td>6195</td>\n",
       "      <td>5</td>\n",
       "      <td>8300</td>\n",
       "      <td>2447</td>\n",
       "      <td>6165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.7816</td>\n",
       "      <td>6813539</td>\n",
       "      <td>-73.9822</td>\n",
       "      <td>1650</td>\n",
       "      <td>2</td>\n",
       "      <td>10465</td>\n",
       "      <td>321</td>\n",
       "      <td>7193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.6768</td>\n",
       "      <td>7116900</td>\n",
       "      <td>-73.9666</td>\n",
       "      <td>3208</td>\n",
       "      <td>17</td>\n",
       "      <td>9147</td>\n",
       "      <td>524</td>\n",
       "      <td>3153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.8029</td>\n",
       "      <td>6890328</td>\n",
       "      <td>-73.9510</td>\n",
       "      <td>1650</td>\n",
       "      <td>3</td>\n",
       "      <td>10886</td>\n",
       "      <td>2952</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74629</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.6969</td>\n",
       "      <td>6855560</td>\n",
       "      <td>-73.9830</td>\n",
       "      <td>2895</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>1215</td>\n",
       "      <td>7388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74630</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>40.7530</td>\n",
       "      <td>6816731</td>\n",
       "      <td>-73.9652</td>\n",
       "      <td>4200</td>\n",
       "      <td>6</td>\n",
       "      <td>6035</td>\n",
       "      <td>3536</td>\n",
       "      <td>7259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74631</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7239</td>\n",
       "      <td>6925764</td>\n",
       "      <td>-73.9834</td>\n",
       "      <td>3250</td>\n",
       "      <td>3</td>\n",
       "      <td>7701</td>\n",
       "      <td>1626</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74632</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.7878</td>\n",
       "      <td>7139280</td>\n",
       "      <td>-73.9799</td>\n",
       "      <td>2895</td>\n",
       "      <td>2</td>\n",
       "      <td>11261</td>\n",
       "      <td>484</td>\n",
       "      <td>7879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74633</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7805</td>\n",
       "      <td>6913068</td>\n",
       "      <td>-73.9464</td>\n",
       "      <td>4500</td>\n",
       "      <td>5</td>\n",
       "      <td>7904</td>\n",
       "      <td>3188</td>\n",
       "      <td>2964</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74634</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.7704</td>\n",
       "      <td>6828445</td>\n",
       "      <td>-73.9566</td>\n",
       "      <td>2995</td>\n",
       "      <td>2</td>\n",
       "      <td>7087</td>\n",
       "      <td>567</td>\n",
       "      <td>3791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74635</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7318</td>\n",
       "      <td>6867865</td>\n",
       "      <td>-73.9875</td>\n",
       "      <td>5750</td>\n",
       "      <td>6</td>\n",
       "      <td>6569</td>\n",
       "      <td>3205</td>\n",
       "      <td>3281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74636</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.7482</td>\n",
       "      <td>6820397</td>\n",
       "      <td>-73.9890</td>\n",
       "      <td>4495</td>\n",
       "      <td>2</td>\n",
       "      <td>9372</td>\n",
       "      <td>2510</td>\n",
       "      <td>4254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74637</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>40.6937</td>\n",
       "      <td>6852197</td>\n",
       "      <td>-73.9323</td>\n",
       "      <td>2599</td>\n",
       "      <td>3</td>\n",
       "      <td>6407</td>\n",
       "      <td>1682</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74638</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.7688</td>\n",
       "      <td>7122934</td>\n",
       "      <td>-73.9135</td>\n",
       "      <td>2150</td>\n",
       "      <td>1</td>\n",
       "      <td>2441</td>\n",
       "      <td>1656</td>\n",
       "      <td>4768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74639</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>40.7760</td>\n",
       "      <td>6907838</td>\n",
       "      <td>-73.9556</td>\n",
       "      <td>9000</td>\n",
       "      <td>6</td>\n",
       "      <td>7855</td>\n",
       "      <td>2579</td>\n",
       "      <td>6356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74640</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.6939</td>\n",
       "      <td>6865896</td>\n",
       "      <td>-73.9919</td>\n",
       "      <td>3805</td>\n",
       "      <td>4</td>\n",
       "      <td>8790</td>\n",
       "      <td>2979</td>\n",
       "      <td>5841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74641</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>40.7604</td>\n",
       "      <td>6840250</td>\n",
       "      <td>-73.9870</td>\n",
       "      <td>4150</td>\n",
       "      <td>6</td>\n",
       "      <td>11145</td>\n",
       "      <td>156</td>\n",
       "      <td>3806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74642</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.7571</td>\n",
       "      <td>6926011</td>\n",
       "      <td>-73.9647</td>\n",
       "      <td>2500</td>\n",
       "      <td>3</td>\n",
       "      <td>7723</td>\n",
       "      <td>2058</td>\n",
       "      <td>787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74643</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>40.7865</td>\n",
       "      <td>6893100</td>\n",
       "      <td>-73.9544</td>\n",
       "      <td>3795</td>\n",
       "      <td>2</td>\n",
       "      <td>7316</td>\n",
       "      <td>2361</td>\n",
       "      <td>2620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74644</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7896</td>\n",
       "      <td>6867538</td>\n",
       "      <td>-73.9753</td>\n",
       "      <td>6500</td>\n",
       "      <td>6</td>\n",
       "      <td>10545</td>\n",
       "      <td>2259</td>\n",
       "      <td>6838</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74645</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7269</td>\n",
       "      <td>6884360</td>\n",
       "      <td>-73.9786</td>\n",
       "      <td>2500</td>\n",
       "      <td>5</td>\n",
       "      <td>6554</td>\n",
       "      <td>1399</td>\n",
       "      <td>5149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74646</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.7378</td>\n",
       "      <td>6903964</td>\n",
       "      <td>-74.0015</td>\n",
       "      <td>3495</td>\n",
       "      <td>1</td>\n",
       "      <td>10907</td>\n",
       "      <td>3203</td>\n",
       "      <td>5698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74647</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7299</td>\n",
       "      <td>6907851</td>\n",
       "      <td>-73.9819</td>\n",
       "      <td>3695</td>\n",
       "      <td>6</td>\n",
       "      <td>7542</td>\n",
       "      <td>2372</td>\n",
       "      <td>4165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74648</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7075</td>\n",
       "      <td>7211166</td>\n",
       "      <td>-74.0059</td>\n",
       "      <td>3025</td>\n",
       "      <td>7</td>\n",
       "      <td>8422</td>\n",
       "      <td>1663</td>\n",
       "      <td>7641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74649</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.7749</td>\n",
       "      <td>6844290</td>\n",
       "      <td>-73.9535</td>\n",
       "      <td>1850</td>\n",
       "      <td>5</td>\n",
       "      <td>9333</td>\n",
       "      <td>3074</td>\n",
       "      <td>742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74650</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7652</td>\n",
       "      <td>6947597</td>\n",
       "      <td>-73.9843</td>\n",
       "      <td>3000</td>\n",
       "      <td>5</td>\n",
       "      <td>11176</td>\n",
       "      <td>2786</td>\n",
       "      <td>279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74651</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>40.7936</td>\n",
       "      <td>6895423</td>\n",
       "      <td>-73.9731</td>\n",
       "      <td>9500</td>\n",
       "      <td>5</td>\n",
       "      <td>10576</td>\n",
       "      <td>2621</td>\n",
       "      <td>3066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74652</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.8160</td>\n",
       "      <td>6812077</td>\n",
       "      <td>-73.9445</td>\n",
       "      <td>2100</td>\n",
       "      <td>1</td>\n",
       "      <td>10915</td>\n",
       "      <td>3412</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74653</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.7301</td>\n",
       "      <td>6903956</td>\n",
       "      <td>-73.9927</td>\n",
       "      <td>4250</td>\n",
       "      <td>1</td>\n",
       "      <td>5884</td>\n",
       "      <td>1298</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74654</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7267</td>\n",
       "      <td>6881005</td>\n",
       "      <td>-73.8569</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>4963</td>\n",
       "      <td>3497</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74655</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.7061</td>\n",
       "      <td>6835379</td>\n",
       "      <td>-74.0111</td>\n",
       "      <td>3649</td>\n",
       "      <td>18</td>\n",
       "      <td>6106</td>\n",
       "      <td>2149</td>\n",
       "      <td>8555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74656</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.7661</td>\n",
       "      <td>6882352</td>\n",
       "      <td>-73.9859</td>\n",
       "      <td>2195</td>\n",
       "      <td>3</td>\n",
       "      <td>11176</td>\n",
       "      <td>313</td>\n",
       "      <td>8139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74657</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.7792</td>\n",
       "      <td>6884758</td>\n",
       "      <td>-73.9484</td>\n",
       "      <td>1775</td>\n",
       "      <td>15</td>\n",
       "      <td>7891</td>\n",
       "      <td>1474</td>\n",
       "      <td>3779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74658</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7145</td>\n",
       "      <td>6924212</td>\n",
       "      <td>-73.9383</td>\n",
       "      <td>2850</td>\n",
       "      <td>2</td>\n",
       "      <td>8741</td>\n",
       "      <td>1495</td>\n",
       "      <td>6927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74659 rows  89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       bathrooms  bedrooms  latitude  listing_id  longitude  price  \\\n",
       "0            1.0         1   40.7185     7142618   -73.9865   2950   \n",
       "1            1.0         2   40.7278     7210040   -74.0000   2850   \n",
       "2            1.0         1   40.7306     7103890   -73.9890   3758   \n",
       "3            1.0         2   40.7109     7143442   -73.9571   3300   \n",
       "4            2.0         2   40.7650     6860601   -73.9845   4900   \n",
       "5            3.0         3   40.7901     6840081   -73.9774   9000   \n",
       "6            1.0         2   40.7730     6922337   -73.9571   2800   \n",
       "7            1.0         0   40.6751     6913616   -73.9511   1900   \n",
       "8            1.0         2   40.7597     6937820   -73.9929   3000   \n",
       "9            1.0         0   40.7208     6893933   -73.9887   2300   \n",
       "10           1.0         1   40.7330     6832604   -73.9960   4800   \n",
       "11           1.0         0   40.7575     6915282   -73.9656   2195   \n",
       "12           1.0         2   40.7418     7127565   -73.9798   4150   \n",
       "13           1.0         3   40.7399     6827899   -73.9864   5200   \n",
       "14           1.0         3   40.7554     6934855   -73.9692   4395   \n",
       "15           1.0         1   40.7412     6861826   -73.9772   3050   \n",
       "16           1.0         0   40.7476     6871643   -73.9760   2395   \n",
       "17           1.0         0   40.7509     6842542   -73.9704   3950   \n",
       "18           1.0         0   40.7075     6934145   -74.0113   2977   \n",
       "19           1.0         2   40.7460     6829365   -73.9754   2995   \n",
       "20           1.0         0   40.7586     7167858   -73.9838   2200   \n",
       "21           1.0         1   40.7746     6859483   -73.9535   2295   \n",
       "22           2.0         2   40.7297     6861377   -73.9860   3675   \n",
       "23           1.0         3   40.7318     6848960   -73.9822   4400   \n",
       "24           1.0         1   40.7412     6918850   -73.9772   2970   \n",
       "25           1.0         0   40.7762     6916867   -73.9775   2000   \n",
       "26           2.0         2   40.7191     6895840   -74.0109   6195   \n",
       "27           1.0         0   40.7816     6813539   -73.9822   1650   \n",
       "28           1.0         2   40.6768     7116900   -73.9666   3208   \n",
       "29           1.0         0   40.8029     6890328   -73.9510   1650   \n",
       "...          ...       ...       ...         ...        ...    ...   \n",
       "74629        1.0         2   40.6969     6855560   -73.9830   2895   \n",
       "74630        2.0         3   40.7530     6816731   -73.9652   4200   \n",
       "74631        1.0         2   40.7239     6925764   -73.9834   3250   \n",
       "74632        1.0         1   40.7878     7139280   -73.9799   2895   \n",
       "74633        2.0         2   40.7805     6913068   -73.9464   4500   \n",
       "74634        1.0         0   40.7704     6828445   -73.9566   2995   \n",
       "74635        2.0         2   40.7318     6867865   -73.9875   5750   \n",
       "74636        1.0         1   40.7482     6820397   -73.9890   4495   \n",
       "74637        2.0         3   40.6937     6852197   -73.9323   2599   \n",
       "74638        1.0         1   40.7688     7122934   -73.9135   2150   \n",
       "74639        0.0         3   40.7760     6907838   -73.9556   9000   \n",
       "74640        1.0         1   40.6939     6865896   -73.9919   3805   \n",
       "74641        2.0         3   40.7604     6840250   -73.9870   4150   \n",
       "74642        1.0         0   40.7571     6926011   -73.9647   2500   \n",
       "74643        1.0         3   40.7865     6893100   -73.9544   3795   \n",
       "74644        2.0         2   40.7896     6867538   -73.9753   6500   \n",
       "74645        1.0         2   40.7269     6884360   -73.9786   2500   \n",
       "74646        1.0         1   40.7378     6903964   -74.0015   3495   \n",
       "74647        1.0         2   40.7299     6907851   -73.9819   3695   \n",
       "74648        1.0         2   40.7075     7211166   -74.0059   3025   \n",
       "74649        1.0         0   40.7749     6844290   -73.9535   1850   \n",
       "74650        1.0         2   40.7652     6947597   -73.9843   3000   \n",
       "74651        2.0         3   40.7936     6895423   -73.9731   9500   \n",
       "74652        1.0         2   40.8160     6812077   -73.9445   2100   \n",
       "74653        1.0         1   40.7301     6903956   -73.9927   4250   \n",
       "74654        1.0         2   40.7267     6881005   -73.8569   2000   \n",
       "74655        1.0         1   40.7061     6835379   -74.0111   3649   \n",
       "74656        1.0         0   40.7661     6882352   -73.9859   2195   \n",
       "74657        1.0         1   40.7792     6884758   -73.9484   1775   \n",
       "74658        1.0         2   40.7145     6924212   -73.9383   2850   \n",
       "\n",
       "       hour_created  address  manager  building  ...   69   70   71   72   73  \\\n",
       "0                 5     9506     2694      4412  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "1                 6     9589     3145         0  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "2                 4      660     2346      2257  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "3                 6      318      179      4368  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "4                 5     8767     2764      3530  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "5                 6    10826     2259      5678  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "6                 3     7117     1205      4676  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "7                 6     9283      198         0  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "8                 5    11128     3467      2338  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "9                 3     9009      381      4516  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "10                6     4693     3480      1267  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "11                2     6923      561      6760  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "12               13     2801      214      3612  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "13                1     9040      383         0  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "14                1     7708     1955         0  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "15                6     7624     3467      5783  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "16                3     6793     3321      5926  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "17                2      126     3008      3142  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "18               17     8042     3467         0  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "19                3     1874     3467      7038  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "20                3    11145     1632       897  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "21                3     7157      790         0  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "22                5     7491     1798        86  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "23                4      577     3691      1119  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "24                3     7624     1964         0  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "25               12    10431     3227      3578  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "26                5     8300     2447      6165  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "27                2    10465      321      7193  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "28               17     9147      524      3153  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "29                3    10886     2952         0  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "...             ...      ...      ...       ...  ...  ...  ...  ...  ...  ...   \n",
       "74629             3      222     1215      7388  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74630             6     6035     3536      7259  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74631             3     7701     1626         0  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74632             2    11261      484      7879  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74633             5     7904     3188      2964  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74634             2     7087      567      3791  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74635             6     6569     3205      3281  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74636             2     9372     2510      4254  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74637             3     6407     1682         0  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74638             1     2441     1656      4768  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74639             6     7855     2579      6356  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74640             4     8790     2979      5841  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74641             6    11145      156      3806  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74642             3     7723     2058       787  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74643             2     7316     2361      2620  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74644             6    10545     2259      6838  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74645             5     6554     1399      5149  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74646             1    10907     3203      5698  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74647             6     7542     2372      4165  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74648             7     8422     1663      7641  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74649             5     9333     3074       742  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74650             5    11176     2786       279  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74651             5    10576     2621      3066  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74652             1    10915     3412         0  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74653             1     5884     1298         0  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74654             1     4963     3497         0  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74655            18     6106     2149      8555  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74656             3    11176      313      8139  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74657            15     7891     1474      3779  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "74658             2     8741     1495      6927  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "        74   75   76   77   78  \n",
       "0      0.0  0.0  0.0  0.0  0.0  \n",
       "1      0.0  0.0  0.0  0.0  0.0  \n",
       "2      0.0  0.0  0.0  0.0  0.0  \n",
       "3      0.0  0.0  0.0  0.0  0.0  \n",
       "4      0.0  0.0  0.0  0.0  0.0  \n",
       "5      0.0  0.0  0.0  0.0  0.0  \n",
       "6      0.0  0.0  0.0  0.0  0.0  \n",
       "7      0.0  0.0  0.0  0.0  0.0  \n",
       "8      0.0  0.0  0.0  0.0  0.0  \n",
       "9      0.0  0.0  0.0  0.0  0.0  \n",
       "10     0.0  0.0  0.0  0.0  0.0  \n",
       "11     0.0  0.0  0.0  0.0  0.0  \n",
       "12     0.0  0.0  0.0  0.0  0.0  \n",
       "13     0.0  0.0  0.0  0.0  0.0  \n",
       "14     0.0  0.0  0.0  0.0  0.0  \n",
       "15     0.0  0.0  0.0  0.0  0.0  \n",
       "16     0.0  0.0  0.0  0.0  0.0  \n",
       "17     0.0  0.0  0.0  0.0  0.0  \n",
       "18     0.0  0.0  0.0  0.0  0.0  \n",
       "19     0.0  0.0  0.0  0.0  0.0  \n",
       "20     0.0  0.0  0.0  0.0  0.0  \n",
       "21     0.0  0.0  0.0  0.0  0.0  \n",
       "22     0.0  0.0  0.0  0.0  0.0  \n",
       "23     0.0  0.0  0.0  0.0  0.0  \n",
       "24     0.0  0.0  0.0  0.0  0.0  \n",
       "25     0.0  0.0  0.0  0.0  0.0  \n",
       "26     0.0  0.0  0.0  0.0  0.0  \n",
       "27     0.0  0.0  0.0  0.0  0.0  \n",
       "28     0.0  0.0  0.0  0.0  0.0  \n",
       "29     0.0  0.0  0.0  0.0  0.0  \n",
       "...    ...  ...  ...  ...  ...  \n",
       "74629  0.0  0.0  0.0  0.0  0.0  \n",
       "74630  0.0  0.0  0.0  0.0  0.0  \n",
       "74631  0.0  0.0  0.0  0.0  0.0  \n",
       "74632  0.0  0.0  0.0  0.0  0.0  \n",
       "74633  0.0  0.0  0.0  0.0  0.0  \n",
       "74634  0.0  0.0  0.0  0.0  0.0  \n",
       "74635  0.0  0.0  0.0  0.0  0.0  \n",
       "74636  0.0  0.0  0.0  0.0  0.0  \n",
       "74637  0.0  0.0  0.0  0.0  0.0  \n",
       "74638  0.0  0.0  0.0  0.0  0.0  \n",
       "74639  0.0  0.0  0.0  0.0  0.0  \n",
       "74640  0.0  0.0  0.0  0.0  0.0  \n",
       "74641  0.0  0.0  0.0  0.0  0.0  \n",
       "74642  0.0  0.0  0.0  0.0  0.0  \n",
       "74643  0.0  0.0  0.0  0.0  0.0  \n",
       "74644  0.0  0.0  0.0  0.0  0.0  \n",
       "74645  0.0  0.0  0.0  0.0  0.0  \n",
       "74646  0.0  0.0  0.0  0.0  0.0  \n",
       "74647  0.0  0.0  0.0  0.0  0.0  \n",
       "74648  0.0  0.0  0.0  0.0  0.0  \n",
       "74649  0.0  0.0  0.0  0.0  0.0  \n",
       "74650  0.0  0.0  0.0  0.0  0.0  \n",
       "74651  0.0  0.0  0.0  0.0  0.0  \n",
       "74652  0.0  0.0  0.0  0.0  0.0  \n",
       "74653  0.0  0.0  0.0  0.0  0.0  \n",
       "74654  0.0  0.0  0.0  0.0  0.0  \n",
       "74655  0.0  0.0  0.0  0.0  0.0  \n",
       "74656  0.0  0.0  0.0  0.0  0.0  \n",
       "74657  0.0  0.0  0.0  0.0  0.0  \n",
       "74658  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[74659 rows x 89 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.drop(['photos','street_address','features','description','created','display_address','feat_vect'], axis=1, inplace=True)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predictions on  test dataset\n",
    "\n",
    "Score (multiclass log-loss) = 1.09861"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use best classifier on test dataset\n",
    "y_pred = best_clf.predict_proba(test.drop(['listing_id'],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_id = test['listing_id'].values\n",
    "listing_id = listing_id.reshape(listing_id.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test output for log loss\n",
    "test_prob = np.concatenate((listing_id,y_pred),axis=1)\n",
    "np.savetxt('test.csv',test_prob, delimiter=',',header='listing_id,high,medium,low')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
