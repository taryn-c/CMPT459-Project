{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "m3v4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6LFUuISUwKnV",
        "mD012lbmwKnZ",
        "tVGt6PHNwKnc",
        "pVdIrT-8wKnl",
        "g_iu6axswKn_",
        "r_jmeh4awKoG",
        "wb-PpXRpwKoL",
        "dTXrk_OmwKoR",
        "8LYP9PJkwKoR",
        "nnd-w8ehwKob",
        "_ozCvN5pwKof",
        "pZ28gMYwwKol",
        "dhNrAgASwKon",
        "ROwCTQ4iwKop",
        "OUWW0S0rwKot",
        "IWywuFEbwKoy"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "70OSIJ_cwKmJ"
      },
      "source": [
        "# Phase 1: Exploratory data analysis and data pre-processing\n",
        "\n",
        "### Taryn Chung (301239706), ZeYu Zhu (301341176), Kearro Chow (301279961)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vmW3vdCnwSIs",
        "outputId": "9c56a64d-3a80-4754-d7b2-2877beb53925",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7mERw8zZwKmM",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "color = sns.color_palette()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IRr3jxGiwKmP",
        "outputId": "cd90ddff-810a-476f-ddf8-d149a24eb4ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "train_df = pd.read_json(\"train.json\")\n",
        "train_df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>building_id</th>\n",
              "      <th>created</th>\n",
              "      <th>description</th>\n",
              "      <th>display_address</th>\n",
              "      <th>features</th>\n",
              "      <th>latitude</th>\n",
              "      <th>listing_id</th>\n",
              "      <th>longitude</th>\n",
              "      <th>manager_id</th>\n",
              "      <th>photos</th>\n",
              "      <th>price</th>\n",
              "      <th>street_address</th>\n",
              "      <th>interest_level</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>8579a0b0d54db803821a35a4a615e97a</td>\n",
              "      <td>2016-06-16 05:55:27</td>\n",
              "      <td>Spacious 1 Bedroom 1 Bathroom in Williamsburg!...</td>\n",
              "      <td>145 Borinquen Place</td>\n",
              "      <td>[Dining Room, Pre-War, Laundry in Building, Di...</td>\n",
              "      <td>40.7108</td>\n",
              "      <td>7170325</td>\n",
              "      <td>-73.9539</td>\n",
              "      <td>a10db4590843d78c784171a107bdacb4</td>\n",
              "      <td>[https://photos.renthop.com/2/7170325_3bb5ac84...</td>\n",
              "      <td>2400</td>\n",
              "      <td>145 Borinquen Place</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>b8e75fc949a6cd8225b455648a951712</td>\n",
              "      <td>2016-06-01 05:44:33</td>\n",
              "      <td>BRAND NEW GUT RENOVATED TRUE 2 BEDROOMFind you...</td>\n",
              "      <td>East 44th</td>\n",
              "      <td>[Doorman, Elevator, Laundry in Building, Dishw...</td>\n",
              "      <td>40.7513</td>\n",
              "      <td>7092344</td>\n",
              "      <td>-73.9722</td>\n",
              "      <td>955db33477af4f40004820b4aed804a0</td>\n",
              "      <td>[https://photos.renthop.com/2/7092344_7663c19a...</td>\n",
              "      <td>3800</td>\n",
              "      <td>230 East 44th</td>\n",
              "      <td>low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>cd759a988b8f23924b5a2058d5ab2b49</td>\n",
              "      <td>2016-06-14 15:19:59</td>\n",
              "      <td>**FLEX 2 BEDROOM WITH FULL PRESSURIZED WALL**L...</td>\n",
              "      <td>East 56th Street</td>\n",
              "      <td>[Doorman, Elevator, Laundry in Building, Laund...</td>\n",
              "      <td>40.7575</td>\n",
              "      <td>7158677</td>\n",
              "      <td>-73.9625</td>\n",
              "      <td>c8b10a317b766204f08e613cef4ce7a0</td>\n",
              "      <td>[https://photos.renthop.com/2/7158677_c897a134...</td>\n",
              "      <td>3495</td>\n",
              "      <td>405 East 56th Street</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1.5</td>\n",
              "      <td>3</td>\n",
              "      <td>53a5b119ba8f7b61d4e010512e0dfc85</td>\n",
              "      <td>2016-06-24 07:54:24</td>\n",
              "      <td>A Brand New 3 Bedroom 1.5 bath ApartmentEnjoy ...</td>\n",
              "      <td>Metropolitan Avenue</td>\n",
              "      <td>[]</td>\n",
              "      <td>40.7145</td>\n",
              "      <td>7211212</td>\n",
              "      <td>-73.9425</td>\n",
              "      <td>5ba989232d0489da1b5f2c45f6688adc</td>\n",
              "      <td>[https://photos.renthop.com/2/7211212_1ed4542e...</td>\n",
              "      <td>3000</td>\n",
              "      <td>792 Metropolitan Avenue</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>bfb9405149bfff42a92980b594c28234</td>\n",
              "      <td>2016-06-28 03:50:23</td>\n",
              "      <td>Over-sized Studio w abundant closets. Availabl...</td>\n",
              "      <td>East 34th Street</td>\n",
              "      <td>[Doorman, Elevator, Fitness Center, Laundry in...</td>\n",
              "      <td>40.7439</td>\n",
              "      <td>7225292</td>\n",
              "      <td>-73.9743</td>\n",
              "      <td>2c3b41f588fbb5234d8a1e885a436cfa</td>\n",
              "      <td>[https://photos.renthop.com/2/7225292_901f1984...</td>\n",
              "      <td>2795</td>\n",
              "      <td>340 East 34th Street</td>\n",
              "      <td>low</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    bathrooms  bedrooms  ...           street_address interest_level\n",
              "4         1.0         1  ...      145 Borinquen Place         medium\n",
              "6         1.0         2  ...            230 East 44th            low\n",
              "9         1.0         2  ...     405 East 56th Street         medium\n",
              "10        1.5         3  ...  792 Metropolitan Avenue         medium\n",
              "15        1.0         0  ...     340 East 34th Street            low\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6LFUuISUwKnV"
      },
      "source": [
        "### Display Address"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tMdc05rtwKnW",
        "outputId": "2d9f1fae-7961-42d8-f147-9ae87096bdfe",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "whichrow = 0\n",
        "count = 0\n",
        "for row in train_df['display_address']:\n",
        "    if any(c.isalpha() for c in row) == False:\n",
        "        train_df.loc[train_df.index[whichrow], 'display_address'] = np.nan\n",
        "        count+=1\n",
        "        whichrow+=1\n",
        "    else:\n",
        "        whichrow += 1\n",
        "print (\"number of missing values: \", count)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of missing values:  141\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E2xJXsc_wKnY"
      },
      "source": [
        "Since there are relatively few missing values and an address is important to have, we removed these values by changing them to NAN and dropping every row with a NAN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mD012lbmwKnZ"
      },
      "source": [
        "### Street Address"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aPa_XnWFwKnZ",
        "outputId": "ed6b456c-0a7e-4ab6-cdd9-8850df05a5ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "whichrow = 0\n",
        "count = 0\n",
        "for row in train_df['street_address']:\n",
        "    if any(c.isalpha() for c in row) == False :\n",
        "        train_df.loc[train_df.index[whichrow], 'street_address'] = np.nan\n",
        "        count+=1\n",
        "        whichrow+=1\n",
        "    else:\n",
        "        whichrow += 1\n",
        "print (\"number of missing values: \", count)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of missing values:  13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6OEEylUEwKnc"
      },
      "source": [
        "Since there are relatively few missing values and an address is important to have, we removed these values by changing them to NAN and dropping every row with a NAN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tVGt6PHNwKnc"
      },
      "source": [
        "### Latitude"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t4JHbPQwwKnd",
        "outputId": "64dd664b-5476-4af1-8a0e-68d9c170d4af",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "whichrow = 0\n",
        "count = 0\n",
        "for row in train_df['latitude']:\n",
        "    if row==0:\n",
        "        train_df.loc[train_df.index[whichrow], 'latitude'] = np.nan\n",
        "        count+=1\n",
        "        whichrow+=1\n",
        "    else:\n",
        "        whichrow += 1\n",
        "print (\"number of missing values: \", count)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of missing values:  12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dRV81IgowKnh"
      },
      "source": [
        "Since there are relatively few missing values and an coordinates are important to have, we removed these values by changing them to NAN and dropping every row with a NAN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pVdIrT-8wKnl"
      },
      "source": [
        "### Longitude"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q0ZMvVs-wKnl",
        "outputId": "40239d3c-7b99-446a-f395-f13e6de4cc08",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "whichrow = 0\n",
        "count = 0\n",
        "for row in train_df['longitude']:\n",
        "    if row==0:\n",
        "        train_df.loc[train_df.index[whichrow], 'longitude'] = np.nan\n",
        "        count+=1\n",
        "        whichrow+=1\n",
        "    else:\n",
        "        whichrow += 1\n",
        "print (\"number of missing values: \", count)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of missing values:  12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g_iu6axswKn_"
      },
      "source": [
        "## Handling outliers\n",
        "\n",
        "### Prices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cdeXt8VBwKoB",
        "outputId": "344d4600-a3dc-438a-879a-8e5af979e4e1",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "q25, q75 = np.percentile(train_df['price'].values, 25), np.percentile(train_df['price'].values, 75)\n",
        "iqr = q75-q25\n",
        "cutoff = iqr * 3\n",
        "lower, upper = q25 - cutoff, q75 + cutoff\n",
        "outliers = [x for x in train_df['price'].values if x > upper or x < lower]\n",
        "print (\"number of extreme outliers:\", len(outliers))\n",
        "#print(lower,upper)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of extreme outliers: 1223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CkCUIUD1wKoE",
        "outputId": "6a083123-5520-4841-f76e-372ead2971d5",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "q25, q75 = np.percentile(train_df['price'].values, 25), np.percentile(train_df['price'].values, 75)\n",
        "iqr = q75-q25\n",
        "cutoff = iqr * 1.5\n",
        "lower, upper = q25 - cutoff, q75 + cutoff\n",
        "outliers = [x for x in train_df['price'].values if x > upper or x < lower]\n",
        "print (\"number of mild outliers:\", len(outliers))\n",
        "#print(lower,upper)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of mild outliers: 2788\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r_jmeh4awKoG"
      },
      "source": [
        "#### Before Handling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lS8EJIjZwKoH",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# sns.boxplot(y=train_df['price'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wb-PpXRpwKoL"
      },
      "source": [
        "#### After Handling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cx-hf1OVwKoM",
        "outputId": "ed2193d6-4c1d-4d48-d5a5-a938d369a46c",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "from scipy import nanmean\n",
        "import math\n",
        "\n",
        "whichrow = 0\n",
        "for row in train_df['price']:\n",
        "    if row>upper or row<lower:\n",
        "        train_df.loc[train_df.index[whichrow], 'price'] = np.nan\n",
        "        whichrow+=1\n",
        "    else:\n",
        "        whichrow += 1\n",
        "        \n",
        "mean_val =round(nanmean(train_df['price']),2)\n",
        "\n",
        "whichrow = 0\n",
        "for row in train_df['price']:\n",
        "    if math.isnan(row):\n",
        "        train_df.loc[train_df.index[whichrow],'price'] = mean_val\n",
        "        whichrow+=1\n",
        "    else:\n",
        "        whichrow+=1\n",
        "\n",
        "# sns.boxplot(y=train_df['price'], whis = [0,100])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: scipy.nanmean is deprecated and will be removed in SciPy 2.0.0, use numpy.nanmean instead\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zsZZcLjswKoQ"
      },
      "source": [
        "We found the Tukey inner and outer fences to find the mild and extreme outliers. Extreme fences included a cutoff that left listings with a negative price range. We decided to only remove the mild outliers instead.  We replaced them with the mean value 3272.00 of so we have more data points to train with. The upper fence cutoff was 6,500 and the lower cutoff was 100. This dealt with outliers such as listings prices at 4,490,000, 43, and negative numbers, which are unrealistic. The new minimum price is 401 and the maximum is now 6,500."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dTXrk_OmwKoR"
      },
      "source": [
        "### Latitude"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8LYP9PJkwKoR"
      },
      "source": [
        "#### Before Handling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ulrvp4H-wKoS",
        "outputId": "15caa406-8acc-46c5-e822-7bdee0d13ad7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "q25, q75 = np.percentile(train_df['latitude'].values, 25), np.percentile(train_df['latitude'].values, 75)\n",
        "iqr = q75-q25\n",
        "cutoff = iqr * 1.5\n",
        "lower, upper = q25 - cutoff, q75 + cutoff\n",
        "outliers = [x for x in train_df['latitude'].values if x > upper or x < lower]\n",
        "print (\"number of mild outliers:\", len(outliers))\n",
        "#print(lower, upper)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of mild outliers: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BIh68c-CwKoW",
        "outputId": "c34959a0-4875-47dc-ef1d-04d347634bc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "q25, q75 = np.percentile(train_df['latitude'].values, 25), np.percentile(train_df['latitude'].values, 75)\n",
        "iqr = q75-q25\n",
        "cutoff = iqr * 3\n",
        "lower, upper = q25 - cutoff, q75 + cutoff\n",
        "outliers = [x for x in train_df['latitude'].values if x > upper or x < lower]\n",
        "print (\"number of extreme outliers:\", len(outliers))\n",
        "#print(lower, upper)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of extreme outliers: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wyew8eJdwKoZ",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# sns.boxplot(y=train_df['latitude'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nnd-w8ehwKob"
      },
      "source": [
        "#### After Handling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OCBGBpM7wKoc",
        "outputId": "9c56f232-76ad-48ff-8ce6-314f88340e79",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "whichrow = 0\n",
        "for row in train_df['latitude']:\n",
        "    if row>upper or row<lower:\n",
        "        train_df.loc[train_df.index[whichrow], 'latitude'] = np.nan\n",
        "        whichrow+=1\n",
        "    else:\n",
        "        whichrow += 1\n",
        "        \n",
        "mean_val = nanmean(train_df['latitude'])\n",
        "#print(mean_val)\n",
        "whichrow = 0\n",
        "for row in train_df['latitude']:\n",
        "    if math.isnan(row):\n",
        "        train_df.loc[train_df.index[whichrow],'latitude'] = mean_val\n",
        "        whichrow+=1\n",
        "    else:\n",
        "        whichrow+=1\n",
        "\n",
        "# sns.boxplot(y=train_df['latitude'], whis = [0,100])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: scipy.nanmean is deprecated and will be removed in SciPy 2.0.0, use numpy.nanmean instead\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7cFwKD24wKoe"
      },
      "source": [
        "We found the Tukey inner and outer fences to find the mild and extreme outliers. The inner fences still included listings that were within the NYC bounds, accoding to Google Maps. So we decided to only deal with the extreme outliers that were a little further out than NYC vicinity. We replaced with the mean value  of 40.7513098874893 so we have more data points to train with. The fence cutoffs we used were 40.5903 and 40.912299999999995. This dealt with outliers such as listings located in LA because we wanted to keep the listings within NYC. The new maximum and minimum longitude and latitude are now 40.9121 and 40.5904, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_ozCvN5pwKof"
      },
      "source": [
        "### Longitude"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yVLoUglowKog",
        "outputId": "20ca3db4-c116-4c6f-903e-7a9c5374df86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "q25, q75 = np.percentile(train_df['longitude'].values, 25), np.percentile(train_df['longitude'].values, 75)\n",
        "iqr = q75-q25\n",
        "cutoff = iqr * 3\n",
        "lower, upper = q25 - cutoff, q75 + cutoff\n",
        "outliers = [x for x in train_df['longitude'].values if x > upper or x < lower]\n",
        "print (\"number of extreme outliers:\", len(outliers))\n",
        "#print(lower, upper)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of extreme outliers: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gD7xfgqTwKoj",
        "outputId": "4a118083-0070-4830-ef34-af1fcdbd91ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "q25, q75 = np.percentile(train_df['longitude'].values, 25), np.percentile(train_df['longitude'].values, 75)\n",
        "iqr = q75-q25\n",
        "cutoff = iqr * 1.5\n",
        "lower, upper = q25 - cutoff, q75 + cutoff\n",
        "outliers = [x for x in train_df['longitude'].values if x > upper or x < lower]\n",
        "print (\"number of mild outliers:\", len(outliers))\n",
        "#print(lower, upper)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of mild outliers: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pZ28gMYwwKol"
      },
      "source": [
        "#### Before Handling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bPUjQEg_wKom",
        "colab": {}
      },
      "source": [
        "# sns.boxplot(y=train_df['longitude'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dhNrAgASwKon"
      },
      "source": [
        "#### After Handling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6N3uYXFvwKon",
        "outputId": "ac81962a-2503-4e0a-e225-f91218d51d89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "whichrow = 0\n",
        "for row in train_df['longitude']:\n",
        "    if row>upper or row<lower:\n",
        "        train_df.loc[train_df.index[whichrow], 'longitude'] = np.nan\n",
        "        whichrow+=1\n",
        "    else:\n",
        "        whichrow += 1\n",
        "        \n",
        "mean_val = nanmean(train_df['longitude'])\n",
        "print(mean_val)\n",
        "\n",
        "whichrow = 0\n",
        "for row in train_df['longitude']:\n",
        "    if math.isnan(row):\n",
        "        train_df.loc[train_df.index[whichrow],'longitude'] = mean_val\n",
        "        whichrow+=1\n",
        "    else:\n",
        "        whichrow+=1\n",
        "\n",
        "# sns.boxplot(y=train_df['longitude'] , whis = [0,100])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-73.97370231860559\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: scipy.nanmean is deprecated and will be removed in SciPy 2.0.0, use numpy.nanmean instead\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kijVewxgwKop"
      },
      "source": [
        "We found the Tukey inner and outer fences to find the mild and extreme outliers. The outer fences still included listings that were within the NYC borough bounds, accoding to Google Maps. The very bottom tip of New York the island is around -74.01 and -73.88 is in the Bronx. So we decided to only deal with the mild outliers that encompassed this. We replaced with the mean value  of 40.7513098874893 so we have more data points to train with. The fence cutoffs we used were -74.04704999999998 and -73.89945000000003. This dealt with outliers such as listings located in LA because we wanted to keep the listings within NYC. The new minimum and maximum longitude and latitude are now -74.0454 and -73.8995, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ROwCTQ4iwKop"
      },
      "source": [
        "### Bedrooms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7A920fOowKoq",
        "outputId": "0eb84d07-582f-4c8b-de90-9e28c2082a8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "q25, q75 = np.percentile(train_df['bedrooms'].values, 25), np.percentile(train_df['bedrooms'].values, 75)\n",
        "iqr = q75-q25\n",
        "cutoff = iqr * 1.5\n",
        "lower, upper = q25 - cutoff, q75 + cutoff\n",
        "outliers = [x for x in train_df['bedrooms'].values if x > upper or x < lower]\n",
        "print (\"number of mild outliers:\", len(outliers))\n",
        "print(lower, upper)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of mild outliers: 2226\n",
            "-0.5 3.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nqrpKtFEwKor",
        "outputId": "cae0905e-a1d7-4ec2-dccb-0b1cdd1bb8ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "q25, q75 = np.percentile(train_df['bedrooms'].values, 25), np.percentile(train_df['bedrooms'].values, 75)\n",
        "iqr = q75-q25\n",
        "cutoff = iqr * 3\n",
        "lower, upper = q25 - cutoff, q75 + cutoff\n",
        "outliers = [x for x in train_df['bedrooms'].values if x > upper or x < lower]\n",
        "print (\"number of mild outliers:\", len(outliers))\n",
        "print(lower, upper)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of mild outliers: 50\n",
            "-2.0 5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OUWW0S0rwKot"
      },
      "source": [
        "#### Before Handling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gGMwsxA1wKou",
        "colab": {}
      },
      "source": [
        "# sns.boxplot(y=train_df['bedrooms'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-8MI9CBrwKox"
      },
      "source": [
        "We found the Tukey inner and outer fences to find the mild and extreme outliers. There were 'outliers' but looking at the values, they seem to make sense, given the prices of our listings. It would make sense for a penthouse or large properties to have many bedrooms so a value like 8 isn't an error. We decided to leave this many values unchanged."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IWywuFEbwKoy"
      },
      "source": [
        "### Bathroom"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gYipA4JMwKoz",
        "outputId": "c7191b83-f75b-420a-b577-f552d7e9a258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "q25, q75 = np.percentile(train_df['bathrooms'].values, 25), np.percentile(train_df['bathrooms'].values, 75)\n",
        "iqr = q75-q25\n",
        "cutoff = iqr * 1.5\n",
        "lower, upper = q25 - cutoff, q75 + cutoff\n",
        "outliers = [x for x in train_df['bathrooms'].values if x > upper or x < lower]\n",
        "print (\"number of mild outliers:\", len(outliers))\n",
        "print(lower,upper)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of mild outliers: 9930\n",
            "1.0 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pbsbJHj1wKo1",
        "outputId": "e8a9b620-d736-4706-970b-43ea98d6a903",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "q25, q75 = np.percentile(train_df['bathrooms'].values, 25), np.percentile(train_df['bathrooms'].values, 75)\n",
        "iqr = q75-q25\n",
        "cutoff = iqr * 3\n",
        "lower, upper = q25 - cutoff, q75 + cutoff\n",
        "outliers = [x for x in train_df['bathrooms'].values if x > upper or x < lower]\n",
        "print (\"number of mild outliers:\", len(outliers))\n",
        "print(lower,upper)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of mild outliers: 9930\n",
            "1.0 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8ERskIU-dEY",
        "colab_type": "text"
      },
      "source": [
        "## Text processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0PcZgwakwKo9",
        "colab": {}
      },
      "source": [
        "# convert features list to string\n",
        "train_df['features'] = train_df['features'].apply(lambda x: ' '.join(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-HgQy_85n5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', token_pattern=r'^[a-zA-Z][a-zA-Z]+')\n",
        "w = vectorizer.fit_transform(train_df['features']);\n",
        "tempd = pd.DataFrame(w.toarray(), columns=vectorizer.get_feature_names())\n",
        "train_df.reset_index(inplace=True, drop=True)\n",
        "train_df = pd.concat([train_df, tempd], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X0FYt2YtwKpD"
      },
      "source": [
        "## MILESTONE3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PI_KPbB8coQ",
        "colab_type": "text"
      },
      "source": [
        "### Importing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi7rE6rU5n57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import linear_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjYq5hsL8Sos",
        "colab_type": "text"
      },
      "source": [
        "### Data processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SLJ9pGLYwKpG",
        "outputId": "e551037f-5d76-4d39-b830-c87341f3822d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        }
      },
      "source": [
        "train_df['interest_level'] = train_df['interest_level'].astype('category')\n",
        "train_df['target'] = train_df['interest_level'].cat.codes\n",
        "train_df.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>building_id</th>\n",
              "      <th>created</th>\n",
              "      <th>description</th>\n",
              "      <th>display_address</th>\n",
              "      <th>features</th>\n",
              "      <th>latitude</th>\n",
              "      <th>listing_id</th>\n",
              "      <th>longitude</th>\n",
              "      <th>manager_id</th>\n",
              "      <th>photos</th>\n",
              "      <th>price</th>\n",
              "      <th>street_address</th>\n",
              "      <th>interest_level</th>\n",
              "      <th>absolute</th>\n",
              "      <th>actual</th>\n",
              "      <th>air</th>\n",
              "      <th>assigned</th>\n",
              "      <th>available</th>\n",
              "      <th>balcony</th>\n",
              "      <th>bike</th>\n",
              "      <th>brand</th>\n",
              "      <th>building</th>\n",
              "      <th>cable</th>\n",
              "      <th>cats</th>\n",
              "      <th>childrens</th>\n",
              "      <th>close</th>\n",
              "      <th>common</th>\n",
              "      <th>courtyard</th>\n",
              "      <th>deck</th>\n",
              "      <th>deco</th>\n",
              "      <th>decorative</th>\n",
              "      <th>dining</th>\n",
              "      <th>dishwasher</th>\n",
              "      <th>dogs</th>\n",
              "      <th>doorman</th>\n",
              "      <th>duplex</th>\n",
              "      <th>dw</th>\n",
              "      <th>eat</th>\n",
              "      <th>...</th>\n",
              "      <th>laundry</th>\n",
              "      <th>live</th>\n",
              "      <th>loft</th>\n",
              "      <th>low</th>\n",
              "      <th>multi</th>\n",
              "      <th>new</th>\n",
              "      <th>newly</th>\n",
              "      <th>nyu</th>\n",
              "      <th>offering</th>\n",
              "      <th>outdoor</th>\n",
              "      <th>parking</th>\n",
              "      <th>patio</th>\n",
              "      <th>pets</th>\n",
              "      <th>pool</th>\n",
              "      <th>postwar</th>\n",
              "      <th>pre</th>\n",
              "      <th>prewar</th>\n",
              "      <th>private</th>\n",
              "      <th>queen</th>\n",
              "      <th>recreational</th>\n",
              "      <th>reduced</th>\n",
              "      <th>renovated</th>\n",
              "      <th>residents</th>\n",
              "      <th>roof</th>\n",
              "      <th>short</th>\n",
              "      <th>stainless</th>\n",
              "      <th>storage</th>\n",
              "      <th>swimming</th>\n",
              "      <th>terrace</th>\n",
              "      <th>townhouse</th>\n",
              "      <th>triplex</th>\n",
              "      <th>view</th>\n",
              "      <th>views</th>\n",
              "      <th>walk</th>\n",
              "      <th>washer</th>\n",
              "      <th>wifi</th>\n",
              "      <th>wonderful</th>\n",
              "      <th>wood</th>\n",
              "      <th>yard</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>8579a0b0d54db803821a35a4a615e97a</td>\n",
              "      <td>2016-06-16 05:55:27</td>\n",
              "      <td>Spacious 1 Bedroom 1 Bathroom in Williamsburg!...</td>\n",
              "      <td>145 Borinquen Place</td>\n",
              "      <td>Dining Room Pre-War Laundry in Building Dishwa...</td>\n",
              "      <td>40.7108</td>\n",
              "      <td>7170325</td>\n",
              "      <td>-73.9539</td>\n",
              "      <td>a10db4590843d78c784171a107bdacb4</td>\n",
              "      <td>[https://photos.renthop.com/2/7170325_3bb5ac84...</td>\n",
              "      <td>2400.0</td>\n",
              "      <td>145 Borinquen Place</td>\n",
              "      <td>medium</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>b8e75fc949a6cd8225b455648a951712</td>\n",
              "      <td>2016-06-01 05:44:33</td>\n",
              "      <td>BRAND NEW GUT RENOVATED TRUE 2 BEDROOMFind you...</td>\n",
              "      <td>East 44th</td>\n",
              "      <td>Doorman Elevator Laundry in Building Dishwashe...</td>\n",
              "      <td>40.7513</td>\n",
              "      <td>7092344</td>\n",
              "      <td>-73.9722</td>\n",
              "      <td>955db33477af4f40004820b4aed804a0</td>\n",
              "      <td>[https://photos.renthop.com/2/7092344_7663c19a...</td>\n",
              "      <td>3800.0</td>\n",
              "      <td>230 East 44th</td>\n",
              "      <td>low</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>cd759a988b8f23924b5a2058d5ab2b49</td>\n",
              "      <td>2016-06-14 15:19:59</td>\n",
              "      <td>**FLEX 2 BEDROOM WITH FULL PRESSURIZED WALL**L...</td>\n",
              "      <td>East 56th Street</td>\n",
              "      <td>Doorman Elevator Laundry in Building Laundry i...</td>\n",
              "      <td>40.7575</td>\n",
              "      <td>7158677</td>\n",
              "      <td>-73.9625</td>\n",
              "      <td>c8b10a317b766204f08e613cef4ce7a0</td>\n",
              "      <td>[https://photos.renthop.com/2/7158677_c897a134...</td>\n",
              "      <td>3495.0</td>\n",
              "      <td>405 East 56th Street</td>\n",
              "      <td>medium</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.5</td>\n",
              "      <td>3</td>\n",
              "      <td>53a5b119ba8f7b61d4e010512e0dfc85</td>\n",
              "      <td>2016-06-24 07:54:24</td>\n",
              "      <td>A Brand New 3 Bedroom 1.5 bath ApartmentEnjoy ...</td>\n",
              "      <td>Metropolitan Avenue</td>\n",
              "      <td></td>\n",
              "      <td>40.7145</td>\n",
              "      <td>7211212</td>\n",
              "      <td>-73.9425</td>\n",
              "      <td>5ba989232d0489da1b5f2c45f6688adc</td>\n",
              "      <td>[https://photos.renthop.com/2/7211212_1ed4542e...</td>\n",
              "      <td>3000.0</td>\n",
              "      <td>792 Metropolitan Avenue</td>\n",
              "      <td>medium</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>bfb9405149bfff42a92980b594c28234</td>\n",
              "      <td>2016-06-28 03:50:23</td>\n",
              "      <td>Over-sized Studio w abundant closets. Availabl...</td>\n",
              "      <td>East 34th Street</td>\n",
              "      <td>Doorman Elevator Fitness Center Laundry in Bui...</td>\n",
              "      <td>40.7439</td>\n",
              "      <td>7225292</td>\n",
              "      <td>-73.9743</td>\n",
              "      <td>2c3b41f588fbb5234d8a1e885a436cfa</td>\n",
              "      <td>[https://photos.renthop.com/2/7225292_901f1984...</td>\n",
              "      <td>2795.0</td>\n",
              "      <td>340 East 34th Street</td>\n",
              "      <td>low</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  96 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   bathrooms  bedrooms                       building_id  ... wood yard target\n",
              "0        1.0         1  8579a0b0d54db803821a35a4a615e97a  ...  0.0  0.0      2\n",
              "1        1.0         2  b8e75fc949a6cd8225b455648a951712  ...  0.0  0.0      1\n",
              "2        1.0         2  cd759a988b8f23924b5a2058d5ab2b49  ...  0.0  0.0      2\n",
              "3        1.5         3  53a5b119ba8f7b61d4e010512e0dfc85  ...  0.0  0.0      2\n",
              "4        1.0         0  bfb9405149bfff42a92980b594c28234  ...  0.0  0.0      1\n",
              "\n",
              "[5 rows x 96 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJo-7tlZ5n5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feat1 = ['interest_level','building_id','created', 'description','display_address','features', 'listing_id','manager_id','photos', 'street_address', 'target' ]\n",
        "x = train_df.drop(feat1,axis=1)\n",
        "y = train_df['target']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AekkuKjb5n6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df = pd.read_json(\"test.json\")\n",
        "test_df['features'] = test_df['features'].apply(lambda x: ' '.join(x))\n",
        "w = vectorizer.transform(test_df['features'])\n",
        "tempd = pd.DataFrame(w.toarray(), columns=vectorizer.get_feature_names())\n",
        "test_df.reset_index(inplace=True, drop=True)\n",
        "test_df = pd.concat([test_df, tempd], axis=1)\n",
        "feat2 = ['building_id','created', 'description','display_address','features', 'listing_id','manager_id','photos', 'street_address']\n",
        "testing = test_df.drop(feat2, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Masddw9X8nb6",
        "colab_type": "text"
      },
      "source": [
        "### MLP classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSX4tobGF3Km",
        "colab_type": "text"
      },
      "source": [
        "#### Starting Version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_gL4MtzcZWA",
        "colab_type": "code",
        "outputId": "67c3929e-3bb9-4ce5-9449-be0513e2852c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "\n",
        "\n",
        "mlp = MLPClassifier(max_iter=100)\n",
        "parameter_space = {\n",
        "    'hidden_layer_sizes': [(50,50,50), (50,100,50)],\n",
        "    'activation': ['logistic', 'relu'],\n",
        "    'alpha': [0.0001,0.001,0.01,0.05,0.1,1],\n",
        "}\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "search = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
        "search.fit(np.array(x), np.array(y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=3, error_score=nan,\n",
              "             estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
              "                                     batch_size='auto', beta_1=0.9,\n",
              "                                     beta_2=0.999, early_stopping=False,\n",
              "                                     epsilon=1e-08, hidden_layer_sizes=(100,),\n",
              "                                     learning_rate='constant',\n",
              "                                     learning_rate_init=0.001, max_fun=15000,\n",
              "                                     max_iter=100, momentum=0.9,\n",
              "                                     n_iter_no_change=10,\n",
              "                                     nesterovs_momentum=True, power_t=0.5,\n",
              "                                     random_state=None, shuffle=True,\n",
              "                                     solver='adam', tol=0.0001,\n",
              "                                     validation_fraction=0.1, verbose=False,\n",
              "                                     warm_start=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'activation': ['logistic', 'relu'],\n",
              "                         'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, 1],\n",
              "                         'hidden_layer_sizes': [(50, 50, 50), (50, 100, 50)]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQpJXPLkcZd4",
        "colab_type": "code",
        "outputId": "5f7bcb9d-b83d-4f0f-df1d-179efe9e3350",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "print('Best parameters found:\\n', search.best_params_)\n",
        "means = search.cv_results_['mean_test_score']\n",
        "stds = search.cv_results_['std_test_score']\n",
        "for mean, std, params in zip(means, stds, search.cv_results_['params']):\n",
        "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best parameters found:\n",
            " {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (50, 100, 50)}\n",
            "0.701 (+/-0.005) for {'activation': 'logistic', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50)}\n",
            "0.699 (+/-0.005) for {'activation': 'logistic', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50)}\n",
            "0.698 (+/-0.004) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (50, 50, 50)}\n",
            "0.692 (+/-0.029) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (50, 100, 50)}\n",
            "0.697 (+/-0.002) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 50)}\n",
            "0.698 (+/-0.001) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (50, 100, 50)}\n",
            "0.697 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50)}\n",
            "0.699 (+/-0.004) for {'activation': 'logistic', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50)}\n",
            "0.695 (+/-0.000) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50)}\n",
            "0.696 (+/-0.004) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (50, 100, 50)}\n",
            "0.695 (+/-0.000) for {'activation': 'logistic', 'alpha': 1, 'hidden_layer_sizes': (50, 50, 50)}\n",
            "0.695 (+/-0.000) for {'activation': 'logistic', 'alpha': 1, 'hidden_layer_sizes': (50, 100, 50)}\n",
            "0.664 (+/-0.072) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50)}\n",
            "0.697 (+/-0.002) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50)}\n",
            "0.701 (+/-0.010) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (50, 50, 50)}\n",
            "0.703 (+/-0.003) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (50, 100, 50)}\n",
            "0.693 (+/-0.028) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 50)}\n",
            "0.683 (+/-0.053) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (50, 100, 50)}\n",
            "0.699 (+/-0.007) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50)}\n",
            "0.699 (+/-0.007) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50)}\n",
            "0.699 (+/-0.005) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50)}\n",
            "0.701 (+/-0.001) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 100, 50)}\n",
            "0.697 (+/-0.001) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (50, 50, 50)}\n",
            "0.698 (+/-0.000) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (50, 100, 50)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kc4Cv6PIHfd6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5835c0f3-b4f2-40d9-fbb6-9974d32c2a5f"
      },
      "source": [
        "## cross validation\n",
        "\n",
        "train_df_cv = shuffle(train_df)\n",
        "train_df_cv.reset_index(inplace=True, drop=True)\n",
        "x_cv = train_df_cv.drop(feat1, axis=1)\n",
        "y_cv = train_df_cv['target']\n",
        "\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "i=1\n",
        "tsum = 0\n",
        "for train_index, test_index in kf.split(x_cv):\n",
        "    x_train, x_test = x_cv.loc[train_index[0]:train_index[-1]], x_cv.loc[test_index[0]:test_index[-1]]\n",
        "    y_train, y_test = y_cv.loc[train_index[0]:train_index[-1]], y_cv.loc[test_index[0]:test_index[-1]]\n",
        "    clf = MLPClassifier(alpha = 0.001, hidden_layer_sizes = (50,100,50), max_iter=100)\n",
        "    clf.fit(np.array(x_train),np.array(y_train))\n",
        "    y_pred = clf.predict(x_test)\n",
        "    y_pred2 = clf.predict_proba(x_test)\n",
        "    \n",
        "    print('log_loss for ',i, ' : ', log_loss(y_test,y_pred2),'\\n')\n",
        "    print(classification_report(y_test,y_pred))\n",
        "    print(\"SVM Accuracy Score -> \",accuracy_score(y_pred, y_test)*100, 'for ', i, '\\n')\n",
        "    i=i+1\n",
        "    tsum = tsum + log_loss(y_test,y_pred2)\n",
        "    \n",
        "print('log_loss average evaluation results of 5 classifiers: ', tsum/5 , \"\\n\")"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_loss for  1  :  0.7089644728123278 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.18      0.27       761\n",
            "           1       0.72      0.99      0.83      6849\n",
            "           2       0.45      0.03      0.06      2261\n",
            "\n",
            "    accuracy                           0.71      9871\n",
            "   macro avg       0.57      0.40      0.39      9871\n",
            "weighted avg       0.64      0.71      0.61      9871\n",
            "\n",
            "SVM Accuracy Score ->  70.67166447168474 for  1 \n",
            "\n",
            "log_loss for  2  :  0.7127078534402234 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.17      0.26       775\n",
            "           1       0.72      0.96      0.82      6793\n",
            "           2       0.39      0.09      0.15      2303\n",
            "\n",
            "    accuracy                           0.70      9871\n",
            "   macro avg       0.55      0.41      0.41      9871\n",
            "weighted avg       0.63      0.70      0.62      9871\n",
            "\n",
            "SVM Accuracy Score ->  69.72951068787356 for  2 \n",
            "\n",
            "log_loss for  3  :  0.7028847996587603 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.17      0.25       785\n",
            "           1       0.73      0.98      0.83      6911\n",
            "           2       0.43      0.06      0.11      2174\n",
            "\n",
            "    accuracy                           0.71      9870\n",
            "   macro avg       0.56      0.40      0.40      9870\n",
            "weighted avg       0.65      0.71      0.63      9870\n",
            "\n",
            "SVM Accuracy Score ->  71.26646403242148 for  3 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "log_loss for  4  :  0.7261224522657062 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.06      0.11       768\n",
            "           1       0.70      1.00      0.82      6844\n",
            "           2       0.44      0.02      0.03      2258\n",
            "\n",
            "    accuracy                           0.70      9870\n",
            "   macro avg       0.58      0.36      0.32      9870\n",
            "weighted avg       0.63      0.70      0.59      9870\n",
            "\n",
            "SVM Accuracy Score ->  69.95947315096251 for  4 \n",
            "\n",
            "log_loss for  5  :  0.7338421538375419 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.19      0.25       750\n",
            "           1       0.74      0.92      0.82      6887\n",
            "           2       0.36      0.14      0.20      2233\n",
            "\n",
            "    accuracy                           0.69      9870\n",
            "   macro avg       0.49      0.42      0.42      9870\n",
            "weighted avg       0.62      0.69      0.64      9870\n",
            "\n",
            "SVM Accuracy Score ->  68.9564336372847 for  5 \n",
            "\n",
            "log_loss average evaluation results of 5 classifiers:  0.7169043464029119 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8VNbl2Tfkzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = search.predict_proba(testing)\n",
        "result = pd.DataFrame({'listing_id': test_df['listing_id'], 'high': prediction[:,0], 'medium': prediction[:,2], 'low': prediction[:,1] })\n",
        "result.to_csv('m3_search.csv', index=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IExHCZdaHWD1",
        "colab_type": "text"
      },
      "source": [
        "#### increase iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOf81hDPr3fw",
        "colab_type": "code",
        "outputId": "52f59c8e-72db-42c9-f6d6-fb2df7dfabbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "mlp_s1 = MLPClassifier(alpha = 0.001, hidden_layer_sizes = (50,100,50), max_iter=1000)\n",
        "mlp_s1.fit(np.array(x),np.array(y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(50, 100, 50), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A59x2qK-r37z",
        "colab_type": "code",
        "outputId": "dc672806-241a-4c12-bc62-8ee2e789b554",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## cross validation\n",
        "\n",
        "train_df_cv = shuffle(train_df)\n",
        "train_df_cv.reset_index(inplace=True, drop=True)\n",
        "x_cv = train_df_cv.drop(feat1, axis=1)\n",
        "y_cv = train_df_cv['target']\n",
        "\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "i=1\n",
        "tsum = 0\n",
        "for train_index, test_index in kf.split(x_cv):\n",
        "    x_train, x_test = x_cv.loc[train_index[0]:train_index[-1]], x_cv.loc[test_index[0]:test_index[-1]]\n",
        "    y_train, y_test = y_cv.loc[train_index[0]:train_index[-1]], y_cv.loc[test_index[0]:test_index[-1]]\n",
        "    clf = MLPClassifier(alpha = 0.001, hidden_layer_sizes = (50,100,50), max_iter=1000)\n",
        "    clf.fit(np.array(x_train),np.array(y_train))\n",
        "    y_pred = clf.predict(x_test)\n",
        "    y_pred2 = clf.predict_proba(x_test)\n",
        "    \n",
        "    print('log_loss for ',i, ' : ', log_loss(y_test,y_pred2),'\\n')\n",
        "    print(classification_report(y_test,y_pred))\n",
        "    print(\"SVM Accuracy Score -> \",accuracy_score(y_pred, y_test)*100, 'for ', i, '\\n')\n",
        "    i=i+1\n",
        "    tsum = tsum + log_loss(y_test,y_pred2)\n",
        "    \n",
        "print('log_loss average evaluation results of 5 classifiers: ', tsum/5 , \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_loss for  1  :  0.7139833833373197 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.10      0.17       790\n",
            "           1       0.72      0.97      0.83      6816\n",
            "           2       0.42      0.11      0.17      2265\n",
            "\n",
            "    accuracy                           0.70      9871\n",
            "   macro avg       0.59      0.39      0.39      9871\n",
            "weighted avg       0.65      0.70      0.62      9871\n",
            "\n",
            "SVM Accuracy Score ->  70.41839732549894 for  1 \n",
            "\n",
            "log_loss for  2  :  0.7455266377583426 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.23      0.29       732\n",
            "           1       0.75      0.91      0.82      6922\n",
            "           2       0.37      0.17      0.24      2217\n",
            "\n",
            "    accuracy                           0.69      9871\n",
            "   macro avg       0.50      0.44      0.45      9871\n",
            "weighted avg       0.64      0.69      0.65      9871\n",
            "\n",
            "SVM Accuracy Score ->  69.48637422753521 for  2 \n",
            "\n",
            "log_loss for  3  :  0.7320912405528235 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.15      0.23       795\n",
            "           1       0.71      0.98      0.83      6830\n",
            "           2       0.40      0.04      0.07      2245\n",
            "\n",
            "    accuracy                           0.70      9870\n",
            "   macro avg       0.53      0.39      0.37      9870\n",
            "weighted avg       0.62      0.70      0.61      9870\n",
            "\n",
            "SVM Accuracy Score ->  70.1823708206687 for  3 \n",
            "\n",
            "log_loss for  4  :  0.7048970477957199 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.18      0.27       775\n",
            "           1       0.72      0.98      0.83      6873\n",
            "           2       0.48      0.04      0.08      2222\n",
            "\n",
            "    accuracy                           0.71      9870\n",
            "   macro avg       0.57      0.40      0.39      9870\n",
            "weighted avg       0.65      0.71      0.62      9870\n",
            "\n",
            "SVM Accuracy Score ->  70.81053698074975 for  4 \n",
            "\n",
            "log_loss for  5  :  0.7437994605653657 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.31      0.32       747\n",
            "           1       0.73      0.96      0.83      6843\n",
            "           2       0.34      0.02      0.04      2280\n",
            "\n",
            "    accuracy                           0.69      9870\n",
            "   macro avg       0.47      0.43      0.40      9870\n",
            "weighted avg       0.61      0.69      0.61      9870\n",
            "\n",
            "SVM Accuracy Score ->  69.4224924012158 for  5 \n",
            "\n",
            "log_loss average evaluation results of 5 classifiers:  0.7280595540019142 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KfTEz05r4Az",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = mlp_s1.predict_proba(testing)\n",
        "result = pd.DataFrame({'listing_id': test_df['listing_id'], 'high': prediction[:,0], 'medium': prediction[:,2], 'low': prediction[:,1] })\n",
        "result.to_csv('m3_search2.csv', index=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWPHTfkN8wyC",
        "colab_type": "text"
      },
      "source": [
        "### Stacking "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Z3nDFk94jt",
        "colab_type": "text"
      },
      "source": [
        "#### Randomforest classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEK-PeXCrCQS",
        "colab_type": "code",
        "outputId": "2f382d62-cd69-4219-fd06-b50050905e42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "rf = RandomForestClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=20,n_estimators=50)\n",
        "rf.fit(np.array(x),np.array(y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=20, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=50,\n",
              "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-o-KdJIorCe6",
        "colab_type": "code",
        "outputId": "e013a39f-6e87-45b4-dc69-e0a13b7fd241",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## cross validation\n",
        "\n",
        "train_df_cv = shuffle(train_df)\n",
        "train_df_cv.reset_index(inplace=True, drop=True)\n",
        "x_cv = train_df_cv.drop(feat1, axis=1)\n",
        "y_cv = train_df_cv['target']\n",
        "\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "i=1\n",
        "tsum = 0\n",
        "for train_index, test_index in kf.split(x_cv):\n",
        "    x_train, x_test = x_cv.loc[train_index[0]:train_index[-1]], x_cv.loc[test_index[0]:test_index[-1]]\n",
        "    y_train, y_test = y_cv.loc[train_index[0]:train_index[-1]], y_cv.loc[test_index[0]:test_index[-1]]\n",
        "    clf = RandomForestClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=20,n_estimators=50)\n",
        "    clf.fit(np.array(x_train),np.array(y_train))\n",
        "    y_pred = clf.predict(x_test)\n",
        "    y_pred2 = clf.predict_proba(x_test)\n",
        "    \n",
        "    print('log_loss for ',i, ' : ', log_loss(y_test,y_pred2),'\\n')\n",
        "    print(classification_report(y_test,y_pred))\n",
        "    print(\"SVM Accuracy Score -> \",accuracy_score(y_pred, y_test)*100, 'for ', i, '\\n')\n",
        "    i=i+1\n",
        "    tsum = tsum + log_loss(y_test,y_pred2)\n",
        "    \n",
        "print('log_loss average evaluation results of 5 classifiers: ', tsum/5 , \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_loss for  1  :  0.694029277107931 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.02      0.04       796\n",
            "           1       0.70      1.00      0.82      6841\n",
            "           2       0.41      0.01      0.01      2234\n",
            "\n",
            "    accuracy                           0.70      9871\n",
            "   macro avg       0.61      0.34      0.29      9871\n",
            "weighted avg       0.63      0.70      0.58      9871\n",
            "\n",
            "SVM Accuracy Score ->  69.52689697092494 for  1 \n",
            "\n",
            "log_loss for  2  :  0.6617035987967547 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.00      0.01       740\n",
            "           1       0.70      1.00      0.82      6906\n",
            "           2       0.43      0.00      0.01      2225\n",
            "\n",
            "    accuracy                           0.70      9871\n",
            "   macro avg       0.58      0.34      0.28      9871\n",
            "weighted avg       0.63      0.70      0.58      9871\n",
            "\n",
            "SVM Accuracy Score ->  70.04356194914396 for  2 \n",
            "\n",
            "log_loss for  3  :  0.6761706375041623 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.00      0.01       784\n",
            "           1       0.70      1.00      0.82      6846\n",
            "           2       0.76      0.01      0.01      2240\n",
            "\n",
            "    accuracy                           0.70      9870\n",
            "   macro avg       0.71      0.34      0.28      9870\n",
            "weighted avg       0.71      0.70      0.57      9870\n",
            "\n",
            "SVM Accuracy Score ->  69.54407294832828 for  3 \n",
            "\n",
            "log_loss for  4  :  0.6696073956074365 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.01      0.02       749\n",
            "           1       0.70      1.00      0.82      6864\n",
            "           2       0.50      0.01      0.01      2257\n",
            "\n",
            "    accuracy                           0.70      9870\n",
            "   macro avg       0.68      0.34      0.28      9870\n",
            "weighted avg       0.66      0.70      0.57      9870\n",
            "\n",
            "SVM Accuracy Score ->  69.65552178318136 for  4 \n",
            "\n",
            "log_loss for  5  :  0.7029528311387206 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       770\n",
            "           1       0.69      1.00      0.82      6827\n",
            "           2       0.44      0.00      0.00      2273\n",
            "\n",
            "    accuracy                           0.69      9870\n",
            "   macro avg       0.38      0.33      0.27      9870\n",
            "weighted avg       0.58      0.69      0.57      9870\n",
            "\n",
            "SVM Accuracy Score ->  69.16919959473151 for  5 \n",
            "\n",
            "log_loss average evaluation results of 5 classifiers:  0.6808927480310011 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEQLERW6rCum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = rf.predict_proba(testing)\n",
        "result = pd.DataFrame({'listing_id': test_df['listing_id'], 'high': prediction[:,0], 'medium': prediction[:,2], 'low': prediction[:,1] })\n",
        "result.to_csv('m3_rf.csv', index=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPlh1rJP-96z",
        "colab_type": "text"
      },
      "source": [
        "#### added randomforest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILiNeZQkwHgY",
        "colab_type": "code",
        "outputId": "98823577-de2a-4276-dc38-4e0541926018",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "est = [\n",
        "       ('mlp3', MLPClassifier(alpha = 0.001, hidden_layer_sizes = (50,100,50), max_iter=1000)),\n",
        "       ('rf', RandomForestClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=20,n_estimators=50))\n",
        "]\n",
        "\n",
        "mlp_rf1 = StackingClassifier(estimators=est, final_estimator=RandomForestClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=20,n_estimators=50))\n",
        "mlp_rf1.fit(np.array(x),np.array(y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StackingClassifier(cv=None,\n",
              "                   estimators=[('mlp3',\n",
              "                                MLPClassifier(activation='relu', alpha=0.001,\n",
              "                                              batch_size='auto', beta_1=0.9,\n",
              "                                              beta_2=0.999,\n",
              "                                              early_stopping=False,\n",
              "                                              epsilon=1e-08,\n",
              "                                              hidden_layer_sizes=(50, 100, 50),\n",
              "                                              learning_rate='constant',\n",
              "                                              learning_rate_init=0.001,\n",
              "                                              max_fun=15000, max_iter=1000,\n",
              "                                              momentum=0.9, n_iter_no_change=10,\n",
              "                                              nesterovs_momentum=True,\n",
              "                                              power_t=0.5, rand...\n",
              "                                                          criterion='gini',\n",
              "                                                          max_depth=None,\n",
              "                                                          max_features='auto',\n",
              "                                                          max_leaf_nodes=None,\n",
              "                                                          max_samples=None,\n",
              "                                                          min_impurity_decrease=0.0,\n",
              "                                                          min_impurity_split=None,\n",
              "                                                          min_samples_leaf=20,\n",
              "                                                          min_samples_split=2,\n",
              "                                                          min_weight_fraction_leaf=0.0,\n",
              "                                                          n_estimators=50,\n",
              "                                                          n_jobs=None,\n",
              "                                                          oob_score=False,\n",
              "                                                          random_state=0,\n",
              "                                                          verbose=0,\n",
              "                                                          warm_start=False),\n",
              "                   n_jobs=None, passthrough=False, stack_method='auto',\n",
              "                   verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqlwJgeC7rtN",
        "colab_type": "code",
        "outputId": "22bec056-5dd1-4469-dd0b-1acade24e685",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## cross validation\n",
        "\n",
        "train_df_cv = shuffle(train_df)\n",
        "train_df_cv.reset_index(inplace=True, drop=True)\n",
        "x_cv = train_df_cv.drop(feat1, axis=1)\n",
        "y_cv = train_df_cv['target']\n",
        "\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "i=1\n",
        "tsum = 0\n",
        "for train_index, test_index in kf.split(x_cv):\n",
        "    x_train, x_test = x_cv.loc[train_index[0]:train_index[-1]], x_cv.loc[test_index[0]:test_index[-1]]\n",
        "    y_train, y_test = y_cv.loc[train_index[0]:train_index[-1]], y_cv.loc[test_index[0]:test_index[-1]]\n",
        "    clf = StackingClassifier(estimators=est, final_estimator=RandomForestClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=20,n_estimators=50))\n",
        "    clf.fit(np.array(x_train),np.array(y_train))\n",
        "    y_pred = clf.predict(x_test)\n",
        "    y_pred2 = clf.predict_proba(x_test)\n",
        "    \n",
        "    print('log_loss for ',i, ' : ', log_loss(y_test,y_pred2),'\\n')\n",
        "    print(classification_report(y_test,y_pred))\n",
        "    print(\"SVM Accuracy Score -> \",accuracy_score(y_pred, y_test)*100, 'for ', i, '\\n')\n",
        "    i=i+1\n",
        "    tsum = tsum + log_loss(y_test,y_pred2)\n",
        "    \n",
        "print('log_loss average evaluation results of 5 classifiers: ', tsum/5 , \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_loss for  1  :  0.671299841260537 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.18      0.27       730\n",
            "           1       0.75      0.93      0.84      6912\n",
            "           2       0.44      0.21      0.29      2229\n",
            "\n",
            "    accuracy                           0.72      9871\n",
            "   macro avg       0.57      0.44      0.46      9871\n",
            "weighted avg       0.67      0.72      0.67      9871\n",
            "\n",
            "SVM Accuracy Score ->  71.53277276871644 for  1 \n",
            "\n",
            "log_loss for  2  :  0.6381761094052887 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.16      0.24       788\n",
            "           1       0.77      0.93      0.84      6871\n",
            "           2       0.46      0.28      0.35      2212\n",
            "\n",
            "    accuracy                           0.72      9871\n",
            "   macro avg       0.58      0.46      0.48      9871\n",
            "weighted avg       0.68      0.72      0.68      9871\n",
            "\n",
            "SVM Accuracy Score ->  72.10009117617263 for  2 \n",
            "\n",
            "log_loss for  3  :  0.6543854768662569 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.19      0.27       815\n",
            "           1       0.76      0.93      0.84      6788\n",
            "           2       0.47      0.25      0.32      2267\n",
            "\n",
            "    accuracy                           0.71      9870\n",
            "   macro avg       0.58      0.46      0.48      9870\n",
            "weighted avg       0.67      0.71      0.67      9870\n",
            "\n",
            "SVM Accuracy Score ->  71.47922998986829 for  3 \n",
            "\n",
            "log_loss for  4  :  0.6408011460466398 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.12      0.19       747\n",
            "           1       0.77      0.92      0.84      6824\n",
            "           2       0.48      0.31      0.38      2299\n",
            "\n",
            "    accuracy                           0.72      9870\n",
            "   macro avg       0.60      0.45      0.47      9870\n",
            "weighted avg       0.68      0.72      0.68      9870\n",
            "\n",
            "SVM Accuracy Score ->  71.96555217831813 for  4 \n",
            "\n",
            "log_loss for  5  :  0.6742492278608139 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.15      0.23       759\n",
            "           1       0.75      0.95      0.83      6889\n",
            "           2       0.43      0.18      0.25      2222\n",
            "\n",
            "    accuracy                           0.71      9870\n",
            "   macro avg       0.57      0.42      0.44      9870\n",
            "weighted avg       0.66      0.71      0.66      9870\n",
            "\n",
            "SVM Accuracy Score ->  71.26646403242148 for  5 \n",
            "\n",
            "log_loss average evaluation results of 5 classifiers:  0.6557823602879073 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw9eE1C17r3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = mlp_rf1.predict_proba(testing)\n",
        "result = pd.DataFrame({'listing_id': test_df['listing_id'], 'high': prediction[:,0], 'medium': prediction[:,2], 'low': prediction[:,1] })\n",
        "result.to_csv('m3_mlp+rf1.csv', index=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZDC5h3-8Yat",
        "colab_type": "code",
        "outputId": "06dea025-e5b7-447d-d702-33ae381a2d50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "est = [\n",
        "       ('mlp3', MLPClassifier(alpha = 0.001, hidden_layer_sizes = (50,100,50), max_iter=1000)),\n",
        "       ('rf', RandomForestClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=20,n_estimators=50))\n",
        "]\n",
        "\n",
        "mlp_rf2 = StackingClassifier(estimators=est, final_estimator=MLPClassifier(alpha = 0.001, hidden_layer_sizes = (50,100,50), max_iter=1000))\n",
        "mlp_rf2.fit(np.array(x),np.array(y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StackingClassifier(cv=None,\n",
              "                   estimators=[('mlp3',\n",
              "                                MLPClassifier(activation='relu', alpha=0.001,\n",
              "                                              batch_size='auto', beta_1=0.9,\n",
              "                                              beta_2=0.999,\n",
              "                                              early_stopping=False,\n",
              "                                              epsilon=1e-08,\n",
              "                                              hidden_layer_sizes=(50, 100, 50),\n",
              "                                              learning_rate='constant',\n",
              "                                              learning_rate_init=0.001,\n",
              "                                              max_fun=15000, max_iter=1000,\n",
              "                                              momentum=0.9, n_iter_no_change=10,\n",
              "                                              nesterovs_momentum=True,\n",
              "                                              power_t=0.5, rand...\n",
              "                                                 hidden_layer_sizes=(50, 100,\n",
              "                                                                     50),\n",
              "                                                 learning_rate='constant',\n",
              "                                                 learning_rate_init=0.001,\n",
              "                                                 max_fun=15000, max_iter=1000,\n",
              "                                                 momentum=0.9,\n",
              "                                                 n_iter_no_change=10,\n",
              "                                                 nesterovs_momentum=True,\n",
              "                                                 power_t=0.5, random_state=None,\n",
              "                                                 shuffle=True, solver='adam',\n",
              "                                                 tol=0.0001,\n",
              "                                                 validation_fraction=0.1,\n",
              "                                                 verbose=False,\n",
              "                                                 warm_start=False),\n",
              "                   n_jobs=None, passthrough=False, stack_method='auto',\n",
              "                   verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDBvwu8I8Yik",
        "colab_type": "code",
        "outputId": "7f89e0da-ad93-4717-eb84-9d8585cc8385",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## cross validation\n",
        "\n",
        "train_df_cv = shuffle(train_df)\n",
        "train_df_cv.reset_index(inplace=True, drop=True)\n",
        "x_cv = train_df_cv.drop(feat1, axis=1)\n",
        "y_cv = train_df_cv['target']\n",
        "\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "i=1\n",
        "tsum = 0\n",
        "for train_index, test_index in kf.split(x_cv):\n",
        "    x_train, x_test = x_cv.loc[train_index[0]:train_index[-1]], x_cv.loc[test_index[0]:test_index[-1]]\n",
        "    y_train, y_test = y_cv.loc[train_index[0]:train_index[-1]], y_cv.loc[test_index[0]:test_index[-1]]\n",
        "    clf = StackingClassifier(estimators=est, final_estimator=MLPClassifier(alpha = 0.001, hidden_layer_sizes = (50,100,50), max_iter=1000))\n",
        "    clf.fit(np.array(x_train),np.array(y_train))\n",
        "    y_pred = clf.predict(x_test)\n",
        "    y_pred2 = clf.predict_proba(x_test)\n",
        "    \n",
        "    print('log_loss for ',i, ' : ', log_loss(y_test,y_pred2),'\\n')\n",
        "    print(classification_report(y_test,y_pred))\n",
        "    print(\"SVM Accuracy Score -> \",accuracy_score(y_pred, y_test)*100, 'for ', i, '\\n')\n",
        "    i=i+1\n",
        "    tsum = tsum + log_loss(y_test,y_pred2)\n",
        "    \n",
        "print('log_loss average evaluation results of 5 classifiers: ', tsum/5 , \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_loss for  1  :  0.6763492189847757 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.21      0.29       776\n",
            "           1       0.76      0.93      0.83      6840\n",
            "           2       0.44      0.21      0.29      2255\n",
            "\n",
            "    accuracy                           0.71      9871\n",
            "   macro avg       0.55      0.45      0.47      9871\n",
            "weighted avg       0.66      0.71      0.67      9871\n",
            "\n",
            "SVM Accuracy Score ->  70.98571573295513 for  1 \n",
            "\n",
            "log_loss for  2  :  0.6367575361451367 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.13      0.21       733\n",
            "           1       0.75      0.96      0.84      6863\n",
            "           2       0.51      0.20      0.29      2275\n",
            "\n",
            "    accuracy                           0.72      9871\n",
            "   macro avg       0.62      0.43      0.45      9871\n",
            "weighted avg       0.68      0.72      0.67      9871\n",
            "\n",
            "SVM Accuracy Score ->  72.35335832235842 for  2 \n",
            "\n",
            "log_loss for  3  :  0.6369267771079636 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.16      0.26       777\n",
            "           1       0.77      0.93      0.84      6832\n",
            "           2       0.45      0.29      0.36      2261\n",
            "\n",
            "    accuracy                           0.72      9870\n",
            "   macro avg       0.61      0.46      0.48      9870\n",
            "weighted avg       0.69      0.72      0.68      9870\n",
            "\n",
            "SVM Accuracy Score ->  72.05673758865248 for  3 \n",
            "\n",
            "log_loss for  4  :  0.6302258477694195 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.15      0.23       772\n",
            "           1       0.76      0.95      0.84      6859\n",
            "           2       0.49      0.22      0.30      2239\n",
            "\n",
            "    accuracy                           0.72      9870\n",
            "   macro avg       0.59      0.44      0.46      9870\n",
            "weighted avg       0.68      0.72      0.67      9870\n",
            "\n",
            "SVM Accuracy Score ->  72.33029381965552 for  4 \n",
            "\n",
            "log_loss for  5  :  0.6622110563034668 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.47      0.18      0.26       781\n",
            "           1       0.75      0.95      0.84      6890\n",
            "           2       0.46      0.19      0.26      2199\n",
            "\n",
            "    accuracy                           0.72      9870\n",
            "   macro avg       0.56      0.44      0.45      9870\n",
            "weighted avg       0.66      0.72      0.66      9870\n",
            "\n",
            "SVM Accuracy Score ->  71.6919959473151 for  5 \n",
            "\n",
            "log_loss average evaluation results of 5 classifiers:  0.6484940872621525 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2ZZLFcV8YrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = mlp_rf2.predict_proba(testing)\n",
        "result = pd.DataFrame({'listing_id': test_df['listing_id'], 'high': prediction[:,0], 'medium': prediction[:,2], 'low': prediction[:,1] })\n",
        "result.to_csv('m3_mlp+rf2.csv', index=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rMgaBse_UJS",
        "colab_type": "text"
      },
      "source": [
        "#### KNeighbors classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiLYosnff_X-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d87e7e74-05ba-40e6-e0f8-82b72cf6dd12"
      },
      "source": [
        "neigh = KNeighborsClassifier(n_neighbors= 150, leaf_size = 100, algorithm = 'ball_tree')\n",
        "neigh.fit(np.array(x), np.array(y))\n",
        "\n",
        "# neigh_base = KNeighborsClassifier()\n",
        "\n",
        "# parameter_space = {\n",
        "#     'n_neighbors': [20, 40, 60],\n",
        "#     # 'weights': ['uniform', 'distance'],\n",
        "#     'algorithm': ['ball_tree', 'kd_tree'],\n",
        "#     # 'leaf_size': [20,30, 40]\n",
        "#     'leaf_size': [50, 100, 150, 200]\n",
        "# }\n",
        "\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# neigh = GridSearchCV(neigh_base, parameter_space, n_jobs=-1, cv=3)\n",
        "# neigh.fit(np.array(x), np.array(y))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='ball_tree', leaf_size=100, metric='minkowski',\n",
              "                     metric_params=None, n_jobs=None, n_neighbors=150, p=2,\n",
              "                     weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HajxCIrNk4Pp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "d19b3a99-e97c-43f1-869e-19a9d04dc7c1"
      },
      "source": [
        "# print('Best parameters found:\\n', neigh.best_params_)\n",
        "# means = neigh.cv_results_['mean_test_score']\n",
        "# stds = neigh.cv_results_['std_test_score']\n",
        "# for mean, std, params in zip(means, stds, neigh.cv_results_['params']):\n",
        "#     print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best parameters found:\n",
            " {'algorithm': 'ball_tree', 'leaf_size': 100, 'n_neighbors': 40}\n",
            "0.698 (+/-0.002) for {'algorithm': 'ball_tree', 'leaf_size': 50, 'n_neighbors': 20}\n",
            "0.701 (+/-0.003) for {'algorithm': 'ball_tree', 'leaf_size': 50, 'n_neighbors': 40}\n",
            "0.700 (+/-0.001) for {'algorithm': 'ball_tree', 'leaf_size': 50, 'n_neighbors': 60}\n",
            "0.698 (+/-0.002) for {'algorithm': 'ball_tree', 'leaf_size': 100, 'n_neighbors': 20}\n",
            "0.701 (+/-0.003) for {'algorithm': 'ball_tree', 'leaf_size': 100, 'n_neighbors': 40}\n",
            "0.700 (+/-0.001) for {'algorithm': 'ball_tree', 'leaf_size': 100, 'n_neighbors': 60}\n",
            "0.698 (+/-0.002) for {'algorithm': 'ball_tree', 'leaf_size': 150, 'n_neighbors': 20}\n",
            "0.700 (+/-0.003) for {'algorithm': 'ball_tree', 'leaf_size': 150, 'n_neighbors': 40}\n",
            "0.700 (+/-0.001) for {'algorithm': 'ball_tree', 'leaf_size': 150, 'n_neighbors': 60}\n",
            "0.698 (+/-0.002) for {'algorithm': 'ball_tree', 'leaf_size': 200, 'n_neighbors': 20}\n",
            "0.700 (+/-0.003) for {'algorithm': 'ball_tree', 'leaf_size': 200, 'n_neighbors': 40}\n",
            "0.700 (+/-0.001) for {'algorithm': 'ball_tree', 'leaf_size': 200, 'n_neighbors': 60}\n",
            "0.698 (+/-0.003) for {'algorithm': 'kd_tree', 'leaf_size': 50, 'n_neighbors': 20}\n",
            "0.700 (+/-0.002) for {'algorithm': 'kd_tree', 'leaf_size': 50, 'n_neighbors': 40}\n",
            "0.700 (+/-0.001) for {'algorithm': 'kd_tree', 'leaf_size': 50, 'n_neighbors': 60}\n",
            "0.698 (+/-0.002) for {'algorithm': 'kd_tree', 'leaf_size': 100, 'n_neighbors': 20}\n",
            "0.701 (+/-0.003) for {'algorithm': 'kd_tree', 'leaf_size': 100, 'n_neighbors': 40}\n",
            "0.700 (+/-0.001) for {'algorithm': 'kd_tree', 'leaf_size': 100, 'n_neighbors': 60}\n",
            "0.698 (+/-0.002) for {'algorithm': 'kd_tree', 'leaf_size': 150, 'n_neighbors': 20}\n",
            "0.700 (+/-0.002) for {'algorithm': 'kd_tree', 'leaf_size': 150, 'n_neighbors': 40}\n",
            "0.700 (+/-0.001) for {'algorithm': 'kd_tree', 'leaf_size': 150, 'n_neighbors': 60}\n",
            "0.698 (+/-0.002) for {'algorithm': 'kd_tree', 'leaf_size': 200, 'n_neighbors': 20}\n",
            "0.700 (+/-0.002) for {'algorithm': 'kd_tree', 'leaf_size': 200, 'n_neighbors': 40}\n",
            "0.700 (+/-0.001) for {'algorithm': 'kd_tree', 'leaf_size': 200, 'n_neighbors': 60}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFvEqzZOf_qr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "761f3e7d-40dc-477e-a5c9-85d185b29bb5"
      },
      "source": [
        "## cross validation\n",
        "\n",
        "train_df_cv = shuffle(train_df)\n",
        "train_df_cv.reset_index(inplace=True, drop=True)\n",
        "x_cv = train_df_cv.drop(feat1, axis=1)\n",
        "y_cv = train_df_cv['target']\n",
        "\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "i=1\n",
        "tsum = 0\n",
        "for train_index, test_index in kf.split(x_cv):\n",
        "    x_train, x_test = x_cv.loc[train_index[0]:train_index[-1]], x_cv.loc[test_index[0]:test_index[-1]]\n",
        "    y_train, y_test = y_cv.loc[train_index[0]:train_index[-1]], y_cv.loc[test_index[0]:test_index[-1]]\n",
        "    clf = KNeighborsClassifier(n_neighbors= 150, leaf_size = 100, algorithm = 'ball_tree')\n",
        "    clf.fit(np.array(x_train),np.array(y_train))\n",
        "    y_pred = clf.predict(x_test)\n",
        "    y_pred2 = clf.predict_proba(x_test)\n",
        "    \n",
        "    print('log_loss for ',i, ' : ', log_loss(y_test,y_pred2),'\\n')\n",
        "    print(classification_report(y_test,y_pred))\n",
        "    print(\"SVM Accuracy Score -> \",accuracy_score(y_pred, y_test)*100, 'for ', i, '\\n')\n",
        "    i=i+1\n",
        "    tsum = tsum + log_loss(y_test,y_pred2)\n",
        "    \n",
        "print('log_loss average evaluation results of 5 classifiers: ', tsum/5 , \"\\n\")"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_loss for  1  :  0.7268914942734905 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.10      0.16       790\n",
            "           1       0.70      0.99      0.82      6828\n",
            "           2       0.41      0.02      0.03      2253\n",
            "\n",
            "    accuracy                           0.70      9871\n",
            "   macro avg       0.54      0.37      0.34      9871\n",
            "weighted avg       0.62      0.70      0.59      9871\n",
            "\n",
            "SVM Accuracy Score ->  69.56741971431467 for  1 \n",
            "\n",
            "log_loss for  2  :  0.7143553646699005 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.11      0.19       765\n",
            "           1       0.71      0.99      0.82      6847\n",
            "           2       0.41      0.02      0.05      2259\n",
            "\n",
            "    accuracy                           0.70      9871\n",
            "   macro avg       0.56      0.37      0.35      9871\n",
            "weighted avg       0.63      0.70      0.60      9871\n",
            "\n",
            "SVM Accuracy Score ->  69.88147097558505 for  2 \n",
            "\n",
            "log_loss for  3  :  0.6979623749219386 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.10      0.17       729\n",
            "           1       0.71      0.99      0.83      6931\n",
            "           2       0.48      0.02      0.04      2210\n",
            "\n",
            "    accuracy                           0.71      9870\n",
            "   macro avg       0.55      0.37      0.34      9870\n",
            "weighted avg       0.64      0.71      0.60      9870\n",
            "\n",
            "SVM Accuracy Score ->  70.69908814589667 for  3 \n",
            "\n",
            "log_loss for  4  :  0.7089394246077185 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.12      0.20       767\n",
            "           1       0.70      0.99      0.82      6797\n",
            "           2       0.34      0.01      0.03      2306\n",
            "\n",
            "    accuracy                           0.69      9870\n",
            "   macro avg       0.52      0.38      0.35      9870\n",
            "weighted avg       0.60      0.69      0.59      9870\n",
            "\n",
            "SVM Accuracy Score ->  69.34143870314084 for  4 \n",
            "\n",
            "log_loss for  5  :  0.7206522073915704 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.11      0.18       788\n",
            "           1       0.71      0.99      0.83      6881\n",
            "           2       0.41      0.02      0.04      2201\n",
            "\n",
            "    accuracy                           0.70      9870\n",
            "   macro avg       0.55      0.37      0.35      9870\n",
            "weighted avg       0.63      0.70      0.60      9870\n",
            "\n",
            "SVM Accuracy Score ->  70.2127659574468 for  5 \n",
            "\n",
            "log_loss average evaluation results of 5 classifiers:  0.7137601731729238 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47dlMjrPf_25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = neigh.predict_proba(testing)\n",
        "result = pd.DataFrame({'listing_id': test_df['listing_id'], 'high': prediction[:,0], 'medium': prediction[:,2], 'low': prediction[:,1] })\n",
        "result.to_csv('m3_neigh.csv', index=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzEM1ffw_f9y",
        "colab_type": "text"
      },
      "source": [
        "#### added KNeighbors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpUinK3J6XQd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "205b0955-6861-4d49-b405-46e460337028"
      },
      "source": [
        "est = [\n",
        "       ('mlp3', MLPClassifier(alpha = 0.001, hidden_layer_sizes = (50,100,50), max_iter=1000)),\n",
        "       ('rf', RandomForestClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=20,n_estimators=50)),\n",
        "       ('knn', KNeighborsClassifier(n_neighbors= 150, leaf_size = 100, algorithm = 'ball_tree'))\n",
        "]\n",
        "\n",
        "mlp_rf2_knn = StackingClassifier(estimators=est, final_estimator=MLPClassifier(alpha = 0.001, hidden_layer_sizes = (50,100,50), max_iter=1000))\n",
        "mlp_rf2_knn.fit(np.array(x),np.array(y))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StackingClassifier(cv=None,\n",
              "                   estimators=[('mlp3',\n",
              "                                MLPClassifier(activation='relu', alpha=0.001,\n",
              "                                              batch_size='auto', beta_1=0.9,\n",
              "                                              beta_2=0.999,\n",
              "                                              early_stopping=False,\n",
              "                                              epsilon=1e-08,\n",
              "                                              hidden_layer_sizes=(50, 100, 50),\n",
              "                                              learning_rate='constant',\n",
              "                                              learning_rate_init=0.001,\n",
              "                                              max_fun=15000, max_iter=1000,\n",
              "                                              momentum=0.9, n_iter_no_change=10,\n",
              "                                              nesterovs_momentum=True,\n",
              "                                              power_t=0.5, rand...\n",
              "                                                 hidden_layer_sizes=(50, 100,\n",
              "                                                                     50),\n",
              "                                                 learning_rate='constant',\n",
              "                                                 learning_rate_init=0.001,\n",
              "                                                 max_fun=15000, max_iter=1000,\n",
              "                                                 momentum=0.9,\n",
              "                                                 n_iter_no_change=10,\n",
              "                                                 nesterovs_momentum=True,\n",
              "                                                 power_t=0.5, random_state=None,\n",
              "                                                 shuffle=True, solver='adam',\n",
              "                                                 tol=0.0001,\n",
              "                                                 validation_fraction=0.1,\n",
              "                                                 verbose=False,\n",
              "                                                 warm_start=False),\n",
              "                   n_jobs=None, passthrough=False, stack_method='auto',\n",
              "                   verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk2c2t4I6Xtc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "86ae55f0-8f9f-4315-be34-fee566b21ca3"
      },
      "source": [
        "## cross validation\n",
        "\n",
        "train_df_cv = shuffle(train_df)\n",
        "train_df_cv.reset_index(inplace=True, drop=True)\n",
        "x_cv = train_df_cv.drop(feat1, axis=1)\n",
        "y_cv = train_df_cv['target']\n",
        "\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "i=1\n",
        "tsum = 0\n",
        "for train_index, test_index in kf.split(x_cv):\n",
        "    x_train, x_test = x_cv.loc[train_index[0]:train_index[-1]], x_cv.loc[test_index[0]:test_index[-1]]\n",
        "    y_train, y_test = y_cv.loc[train_index[0]:train_index[-1]], y_cv.loc[test_index[0]:test_index[-1]]\n",
        "    clf = StackingClassifier(estimators=est, final_estimator=MLPClassifier(alpha = 0.001, hidden_layer_sizes = (50,100,50), max_iter=1000))\n",
        "    clf.fit(np.array(x_train),np.array(y_train))\n",
        "    y_pred = clf.predict(x_test)\n",
        "    y_pred2 = clf.predict_proba(x_test)\n",
        "    \n",
        "    print('log_loss for ',i, ' : ', log_loss(y_test,y_pred2),'\\n')\n",
        "    print(classification_report(y_test,y_pred))\n",
        "    print(\"SVM Accuracy Score -> \",accuracy_score(y_pred, y_test)*100, 'for ', i, '\\n')\n",
        "    i=i+1\n",
        "    tsum = tsum + log_loss(y_test,y_pred2)\n",
        "    \n",
        "print('log_loss average evaluation results of 5 classifiers: ', tsum/5 , \"\\n\")"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_loss for  1  :  0.6715647450204595 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.12      0.19       767\n",
            "           1       0.74      0.95      0.83      6854\n",
            "           2       0.42      0.16      0.24      2250\n",
            "\n",
            "    accuracy                           0.71      9871\n",
            "   macro avg       0.55      0.41      0.42      9871\n",
            "weighted avg       0.65      0.71      0.65      9871\n",
            "\n",
            "SVM Accuracy Score ->  70.81349407354878 for  1 \n",
            "\n",
            "log_loss for  2  :  0.6381141093903572 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.14      0.22       778\n",
            "           1       0.76      0.94      0.84      6851\n",
            "           2       0.45      0.25      0.32      2242\n",
            "\n",
            "    accuracy                           0.72      9871\n",
            "   macro avg       0.59      0.44      0.46      9871\n",
            "weighted avg       0.67      0.72      0.67      9871\n",
            "\n",
            "SVM Accuracy Score ->  71.78603991490225 for  2 \n",
            "\n",
            "log_loss for  3  :  0.6234063440264569 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.18      0.27       765\n",
            "           1       0.78      0.93      0.85      6868\n",
            "           2       0.47      0.30      0.37      2237\n",
            "\n",
            "    accuracy                           0.73      9870\n",
            "   macro avg       0.60      0.47      0.49      9870\n",
            "weighted avg       0.69      0.73      0.69      9870\n",
            "\n",
            "SVM Accuracy Score ->  72.82674772036474 for  3 \n",
            "\n",
            "log_loss for  4  :  0.6313150997260969 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.19      0.28       749\n",
            "           1       0.77      0.93      0.85      6866\n",
            "           2       0.48      0.28      0.35      2255\n",
            "\n",
            "    accuracy                           0.73      9870\n",
            "   macro avg       0.59      0.47      0.49      9870\n",
            "weighted avg       0.69      0.73      0.69      9870\n",
            "\n",
            "SVM Accuracy Score ->  72.78622087132726 for  4 \n",
            "\n",
            "log_loss for  5  :  0.6735100066142915 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.08      0.14       780\n",
            "           1       0.74      0.95      0.83      6845\n",
            "           2       0.42      0.19      0.26      2245\n",
            "\n",
            "    accuracy                           0.71      9870\n",
            "   macro avg       0.60      0.41      0.41      9870\n",
            "weighted avg       0.66      0.71      0.65      9870\n",
            "\n",
            "SVM Accuracy Score ->  70.7193515704154 for  5 \n",
            "\n",
            "log_loss average evaluation results of 5 classifiers:  0.6475820609555324 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4aqbd9s6Xya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = mlp_rf2_knn.predict_proba(testing)\n",
        "result = pd.DataFrame({'listing_id': test_df['listing_id'], 'high': prediction[:,0], 'medium': prediction[:,2], 'low': prediction[:,1] })\n",
        "result.to_csv('m3_mlp+rf2+knn.csv', index=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB9TIlvDhMX2",
        "colab_type": "text"
      },
      "source": [
        "### reduce overfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMX_SQFghSW1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "4eff1eb5-c182-4d6d-a804-560158cbae56"
      },
      "source": [
        "est = [\n",
        "       ('mlp3', MLPClassifier(alpha = 0.001, hidden_layer_sizes = (50,100,50), max_iter=500)),\n",
        "       ('rf', RandomForestClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=20,n_estimators=50)),\n",
        "       ('knn', KNeighborsClassifier(n_neighbors= 150, leaf_size = 100, algorithm = 'ball_tree'))\n",
        "]\n",
        "\n",
        "mlp_rf2_knn2 = StackingClassifier(estimators=est, final_estimator=MLPClassifier(alpha = 0.001, hidden_layer_sizes = (50,100,50), max_iter=500))\n",
        "mlp_rf2_knn2.fit(np.array(x),np.array(y))"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StackingClassifier(cv=None,\n",
              "                   estimators=[('mlp3',\n",
              "                                MLPClassifier(activation='relu', alpha=0.001,\n",
              "                                              batch_size='auto', beta_1=0.9,\n",
              "                                              beta_2=0.999,\n",
              "                                              early_stopping=False,\n",
              "                                              epsilon=1e-08,\n",
              "                                              hidden_layer_sizes=(50, 100, 50),\n",
              "                                              learning_rate='constant',\n",
              "                                              learning_rate_init=0.001,\n",
              "                                              max_fun=15000, max_iter=500,\n",
              "                                              momentum=0.9, n_iter_no_change=10,\n",
              "                                              nesterovs_momentum=True,\n",
              "                                              power_t=0.5, rando...\n",
              "                                                 epsilon=1e-08,\n",
              "                                                 hidden_layer_sizes=(50, 100,\n",
              "                                                                     50),\n",
              "                                                 learning_rate='constant',\n",
              "                                                 learning_rate_init=0.001,\n",
              "                                                 max_fun=15000, max_iter=500,\n",
              "                                                 momentum=0.9,\n",
              "                                                 n_iter_no_change=10,\n",
              "                                                 nesterovs_momentum=True,\n",
              "                                                 power_t=0.5, random_state=None,\n",
              "                                                 shuffle=True, solver='adam',\n",
              "                                                 tol=0.0001,\n",
              "                                                 validation_fraction=0.1,\n",
              "                                                 verbose=False,\n",
              "                                                 warm_start=False),\n",
              "                   n_jobs=None, passthrough=False, stack_method='auto',\n",
              "                   verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iWSBda-hTyW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6e266322-e88f-4d82-b878-b5362986accf"
      },
      "source": [
        "## cross validation\n",
        "\n",
        "train_df_cv = shuffle(train_df)\n",
        "train_df_cv.reset_index(inplace=True, drop=True)\n",
        "x_cv = train_df_cv.drop(feat1, axis=1)\n",
        "y_cv = train_df_cv['target']\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "i=1\n",
        "tsum = 0\n",
        "for train_index, test_index in kf.split(x_cv):\n",
        "    x_train, x_test = x_cv.loc[train_index[0]:train_index[-1]], x_cv.loc[test_index[0]:test_index[-1]]\n",
        "    y_train, y_test = y_cv.loc[train_index[0]:train_index[-1]], y_cv.loc[test_index[0]:test_index[-1]]\n",
        "    clf = StackingClassifier(estimators=est, final_estimator=MLPClassifier(alpha = 0.001, hidden_layer_sizes = (50,100,50), max_iter=500))\n",
        "    clf.fit(np.array(x_train),np.array(y_train))\n",
        "    y_pred = clf.predict(x_test)\n",
        "    y_pred2 = clf.predict_proba(x_test)\n",
        "    \n",
        "    print('log_loss for ',i, ' : ', log_loss(y_test,y_pred2),'\\n')\n",
        "    print(classification_report(y_test,y_pred))\n",
        "    print(\"SVM Accuracy Score -> \",accuracy_score(y_pred, y_test)*100, 'for ', i, '\\n')\n",
        "    i=i+1\n",
        "    tsum = tsum + log_loss(y_test,y_pred2)\n",
        "    \n",
        "print('log_loss average evaluation results of 5 classifiers: ', tsum/5 , \"\\n\")"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_loss for  1  :  0.6868556265995636 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.14      0.23       799\n",
            "           1       0.72      0.96      0.83      6750\n",
            "           2       0.41      0.12      0.18      2322\n",
            "\n",
            "    accuracy                           0.70      9871\n",
            "   macro avg       0.57      0.41      0.41      9871\n",
            "weighted avg       0.64      0.70      0.63      9871\n",
            "\n",
            "SVM Accuracy Score ->  69.78016411711073 for  1 \n",
            "\n",
            "log_loss for  2  :  0.6272705909148935 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.16      0.25       732\n",
            "           1       0.75      0.96      0.84      6840\n",
            "           2       0.50      0.19      0.27      2299\n",
            "\n",
            "    accuracy                           0.72      9871\n",
            "   macro avg       0.59      0.44      0.45      9871\n",
            "weighted avg       0.67      0.72      0.67      9871\n",
            "\n",
            "SVM Accuracy Score ->  72.19126734879951 for  2 \n",
            "\n",
            "log_loss for  3  :  0.6275296627460556 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.17      0.25       782\n",
            "           1       0.77      0.95      0.85      6896\n",
            "           2       0.48      0.24      0.32      2192\n",
            "\n",
            "    accuracy                           0.73      9870\n",
            "   macro avg       0.58      0.45      0.47      9870\n",
            "weighted avg       0.68      0.73      0.68      9870\n",
            "\n",
            "SVM Accuracy Score ->  72.78622087132726 for  3 \n",
            "\n",
            "log_loss for  4  :  0.6184560008526158 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.19      0.28       753\n",
            "           1       0.76      0.96      0.85      6863\n",
            "           2       0.52      0.23      0.32      2254\n",
            "\n",
            "    accuracy                           0.73      9870\n",
            "   macro avg       0.63      0.46      0.48      9870\n",
            "weighted avg       0.69      0.73      0.68      9870\n",
            "\n",
            "SVM Accuracy Score ->  73.09017223910841 for  4 \n",
            "\n",
            "log_loss for  5  :  0.6626339086854693 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.46      0.16      0.24       773\n",
            "           1       0.74      0.97      0.84      6935\n",
            "           2       0.40      0.08      0.14      2162\n",
            "\n",
            "    accuracy                           0.71      9870\n",
            "   macro avg       0.53      0.40      0.40      9870\n",
            "weighted avg       0.64      0.71      0.64      9870\n",
            "\n",
            "SVM Accuracy Score ->  71.35764944275581 for  5 \n",
            "\n",
            "log_loss average evaluation results of 5 classifiers:  0.6445491579597196 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6onkMH4XhUId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = mlp_rf2_knn2.predict_proba(testing)\n",
        "result = pd.DataFrame({'listing_id': test_df['listing_id'], 'high': prediction[:,0], 'medium': prediction[:,2], 'low': prediction[:,1] })\n",
        "result.to_csv('m3_mlp+rf2+knn2.csv', index=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3F__MnY-McT",
        "colab_type": "text"
      },
      "source": [
        "## Two layer stacking (no improvement)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1INs2lH3Yqn",
        "colab_type": "code",
        "outputId": "cc7a4c26-c5a0-48bc-f32c-03fd136ecfe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "layer1_est = [\n",
        "              ('mlp3', MLPClassifier(alpha = 0.001, hidden_layer_sizes = (50,100,50), max_iter=1000)),\n",
        "              ('rf', RandomForestClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=20,n_estimators=50))\n",
        "]\n",
        "\n",
        "layer2_est = [\n",
        "              ('mlp3', MLPClassifier(alpha = 0.001, hidden_layer_sizes = (50,100,50), max_iter=1000)),\n",
        "              ('rf', RandomForestClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=20,n_estimators=50))\n",
        "]\n",
        "\n",
        "layer2 = StackingClassifier(estimators=layer2_est, final_estimator=MLPClassifier(alpha = 0.001, hidden_layer_sizes = (50,100,50), max_iter=1000))\n",
        "\n",
        "twolayer = StackingClassifier(estimators=layer1_est, final_estimator=layer2)\n",
        "twolayer.fit(np.array(x), np.array(y))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StackingClassifier(cv=None,\n",
              "                   estimators=[('mlp3',\n",
              "                                MLPClassifier(activation='relu', alpha=0.001,\n",
              "                                              batch_size='auto', beta_1=0.9,\n",
              "                                              beta_2=0.999,\n",
              "                                              early_stopping=False,\n",
              "                                              epsilon=1e-08,\n",
              "                                              hidden_layer_sizes=(50, 100, 50),\n",
              "                                              learning_rate='constant',\n",
              "                                              learning_rate_init=0.001,\n",
              "                                              max_fun=15000, max_iter=1000,\n",
              "                                              momentum=0.9, n_iter_no_change=10,\n",
              "                                              nesterovs_momentum=True,\n",
              "                                              power_t=0.5, rand...\n",
              "                                                                                    learning_rate_init=0.001,\n",
              "                                                                                    max_fun=15000,\n",
              "                                                                                    max_iter=1000,\n",
              "                                                                                    momentum=0.9,\n",
              "                                                                                    n_iter_no_change=10,\n",
              "                                                                                    nesterovs_momentum=True,\n",
              "                                                                                    power_t=0.5,\n",
              "                                                                                    random_state=None,\n",
              "                                                                                    shuffle=True,\n",
              "                                                                                    solver='adam',\n",
              "                                                                                    tol=0.0001,\n",
              "                                                                                    validation_fraction=0.1,\n",
              "                                                                                    verbose=False,\n",
              "                                                                                    warm_start=False),\n",
              "                                                      n_jobs=None,\n",
              "                                                      passthrough=False,\n",
              "                                                      stack_method='auto',\n",
              "                                                      verbose=0),\n",
              "                   n_jobs=None, passthrough=False, stack_method='auto',\n",
              "                   verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFDoKQMX3ZCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = twolayer.predict_proba(testing)\n",
        "result = pd.DataFrame({'listing_id': test_df['listing_id'], 'high': prediction[:,0], 'medium': prediction[:,2], 'low': prediction[:,1] })\n",
        "result.to_csv('m3_twolayer.csv', index=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSxyaW3pI0HT",
        "colab_type": "code",
        "outputId": "35ee5f98-d970-4d84-fa14-8040b4606c42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "layer1_est = [\n",
        "              ('rf1', RandomForestClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=20,n_estimators=50)),\n",
        "              ('rf2', RandomForestClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=25,n_estimators=55)),\n",
        "              ('rf3', RandomForestClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=30,n_estimators=60))\n",
        "]\n",
        "\n",
        "layer2_est = [\n",
        "              ('mlp3', MLPClassifier(alpha = 0.001, hidden_layer_sizes = (50,100,50), max_iter=1000)),\n",
        "              ('rf', RandomForestClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=20,n_estimators=50))\n",
        "]\n",
        "\n",
        "layer2 = StackingClassifier(estimators=layer2_est, final_estimator=MLPClassifier(alpha = 0.001, hidden_layer_sizes = (50,100,50), max_iter=1000))\n",
        "\n",
        "twolayer2 = StackingClassifier(estimators=layer1_est, final_estimator=layer2)\n",
        "twolayer2.fit(np.array(x), np.array(y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StackingClassifier(cv=None,\n",
              "                   estimators=[('rf1',\n",
              "                                RandomForestClassifier(bootstrap=True,\n",
              "                                                       ccp_alpha=0.0,\n",
              "                                                       class_weight=None,\n",
              "                                                       criterion='gini',\n",
              "                                                       max_depth=None,\n",
              "                                                       max_features='auto',\n",
              "                                                       max_leaf_nodes=None,\n",
              "                                                       max_samples=None,\n",
              "                                                       min_impurity_decrease=0.0,\n",
              "                                                       min_impurity_split=None,\n",
              "                                                       min_samples_leaf=20,\n",
              "                                                       min_samples_split=2,\n",
              "                                                       min_weight_fraction_leaf=0.0,\n",
              "                                                       n_estimators=50,\n",
              "                                                       n_jobs=Non...\n",
              "                                                                                    learning_rate_init=0.001,\n",
              "                                                                                    max_fun=15000,\n",
              "                                                                                    max_iter=1000,\n",
              "                                                                                    momentum=0.9,\n",
              "                                                                                    n_iter_no_change=10,\n",
              "                                                                                    nesterovs_momentum=True,\n",
              "                                                                                    power_t=0.5,\n",
              "                                                                                    random_state=None,\n",
              "                                                                                    shuffle=True,\n",
              "                                                                                    solver='adam',\n",
              "                                                                                    tol=0.0001,\n",
              "                                                                                    validation_fraction=0.1,\n",
              "                                                                                    verbose=False,\n",
              "                                                                                    warm_start=False),\n",
              "                                                      n_jobs=None,\n",
              "                                                      passthrough=False,\n",
              "                                                      stack_method='auto',\n",
              "                                                      verbose=0),\n",
              "                   n_jobs=None, passthrough=False, stack_method='auto',\n",
              "                   verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yq90XxQaI0Xr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = twolayer2.predict_proba(testing)\n",
        "result = pd.DataFrame({'listing_id': test_df['listing_id'], 'high': prediction[:,0], 'medium': prediction[:,2], 'low': prediction[:,1] })\n",
        "result.to_csv('m3_twolayer2.csv', index=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0akyfqw29pA",
        "colab_type": "text"
      },
      "source": [
        "## Other attempts\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hWUOPGQGnAT",
        "colab_type": "code",
        "outputId": "f5726d86-2fb8-4d7c-d540-0864b9b9230c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "mlp1 = MLPClassifier(alpha = 0.1, hidden_layer_sizes = (50,50,50), max_iter=1000)\n",
        "mlp1.fit(np.array(x),np.array(y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.1, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(50, 50, 50), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wuXmVOqHHuN",
        "colab_type": "code",
        "outputId": "385e52af-66f8-47bb-bbf1-ecfd766655a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## cross validation\n",
        "\n",
        "train_df_cv = shuffle(train_df)\n",
        "train_df_cv.reset_index(inplace=True, drop=True)\n",
        "x_cv = train_df_cv.drop(feat1, axis=1)\n",
        "y_cv = train_df_cv['target']\n",
        "\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "i=1\n",
        "tsum = 0\n",
        "for train_index, test_index in kf.split(x):\n",
        "    x_train, x_test = x_cv.loc[train_index[0]:train_index[-1]], x_cv.loc[test_index[0]:test_index[-1]]\n",
        "    y_train, y_test = y_cv.loc[train_index[0]:train_index[-1]], y_cv.loc[test_index[0]:test_index[-1]]\n",
        "    clf = MLPClassifier(alpha = 0.1, hidden_layer_sizes = (50,50,50), max_iter=1000)\n",
        "    clf.fit(np.array(x_train),np.array(y_train))\n",
        "    y_pred = clf.predict(x_test)\n",
        "    y_pred2 = clf.predict_proba(x_test)\n",
        "    \n",
        "    print('log_loss for ',i, ' : ', log_loss(y_test,y_pred2),'\\n')\n",
        "    print(classification_report(y_test,y_pred))\n",
        "    print(\"SVM Accuracy Score -> \",accuracy_score(y_pred, y_test)*100, 'for ', i, '\\n')\n",
        "    i=i+1\n",
        "    tsum = tsum + log_loss(y_test,y_pred2)\n",
        "    \n",
        "print('log_loss average evaluation results of 5 classifiers: ', tsum/5 , \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_loss for  1  :  0.7444403180702548 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      0.19      0.26       785\n",
            "           1       0.71      0.99      0.82      6816\n",
            "           2       0.55      0.01      0.01      2270\n",
            "\n",
            "    accuracy                           0.70      9871\n",
            "   macro avg       0.56      0.39      0.37      9871\n",
            "weighted avg       0.65      0.70      0.59      9871\n",
            "\n",
            "SVM Accuracy Score ->  69.7092493161787 for  1 \n",
            "\n",
            "log_loss for  2  :  0.724957984903575 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.11      0.18       769\n",
            "           1       0.70      1.00      0.82      6847\n",
            "           2       0.53      0.01      0.03      2255\n",
            "\n",
            "    accuracy                           0.70      9871\n",
            "   macro avg       0.61      0.37      0.34      9871\n",
            "weighted avg       0.66      0.70      0.59      9871\n",
            "\n",
            "SVM Accuracy Score ->  70.17526086516057 for  2 \n",
            "\n",
            "log_loss for  3  :  0.7618571647416151 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.08      0.15       757\n",
            "           1       0.70      1.00      0.82      6868\n",
            "           2       0.33      0.00      0.00      2245\n",
            "\n",
            "    accuracy                           0.70      9870\n",
            "   macro avg       0.58      0.36      0.32      9870\n",
            "weighted avg       0.62      0.70      0.58      9870\n",
            "\n",
            "SVM Accuracy Score ->  70.1418439716312 for  3 \n",
            "\n",
            "log_loss for  4  :  0.729218772052432 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.11      0.19       718\n",
            "           1       0.71      0.99      0.83      6857\n",
            "           2       0.38      0.01      0.03      2295\n",
            "\n",
            "    accuracy                           0.70      9870\n",
            "   macro avg       0.55      0.37      0.35      9870\n",
            "weighted avg       0.62      0.70      0.59      9870\n",
            "\n",
            "SVM Accuracy Score ->  70.15197568389058 for  4 \n",
            "\n",
            "log_loss for  5  :  0.7468509780966461 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.20      0.28       810\n",
            "           1       0.72      0.98      0.83      6896\n",
            "           2       0.34      0.02      0.04      2164\n",
            "\n",
            "    accuracy                           0.70      9870\n",
            "   macro avg       0.50      0.40      0.38      9870\n",
            "weighted avg       0.61      0.70      0.61      9870\n",
            "\n",
            "SVM Accuracy Score ->  70.34447821681864 for  5 \n",
            "\n",
            "log_loss average evaluation results of 5 classifiers:  0.7414650435729047 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIx1WRSXGVXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testing = test_df.drop(feat2, axis=1)\n",
        "# prediction = svc_class.predict_proba(testing)\n",
        "# result = pd.DataFrame({'listing_id': test_df['listing_id'], 'high': prediction[:,0], 'medium': prediction[:,2], 'low': prediction[:,1] })\n",
        "# result.to_csv('m3_1.csv', index=False) \n",
        "\n",
        "prediction = mlp1.predict_proba(testing)\n",
        "result = pd.DataFrame({'listing_id': test_df['listing_id'], 'high': prediction[:,0], 'medium': prediction[:,2], 'low': prediction[:,1] })\n",
        "result.to_csv('m3_1.csv', index=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0LhAMPMKnZ6",
        "colab_type": "code",
        "outputId": "436b86b0-2817-4822-efb2-5fc026df6e28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "mlp2 = MLPClassifier(alpha = 0.1, hidden_layer_sizes = (50,50,50), max_iter=2000)\n",
        "mlp2.fit(np.array(x),np.array(y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.1, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(50, 50, 50), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=2000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXQ4TJXQKnci",
        "colab_type": "code",
        "outputId": "bebb29e3-1b70-49c5-ba8d-1b497c23a687",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## cross validation\n",
        "\n",
        "train_df_cv = shuffle(train_df)\n",
        "train_df_cv.reset_index(inplace=True, drop=True)\n",
        "x_cv = train_df_cv.drop(feat1, axis=1)\n",
        "y_cv = train_df_cv['target']\n",
        "\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "i=1\n",
        "tsum = 0\n",
        "for train_index, test_index in kf.split(x):\n",
        "    x_train, x_test = x_cv.loc[train_index[0]:train_index[-1]], x_cv.loc[test_index[0]:test_index[-1]]\n",
        "    y_train, y_test = y_cv.loc[train_index[0]:train_index[-1]], y_cv.loc[test_index[0]:test_index[-1]]\n",
        "    clf = MLPClassifier(alpha = 0.1, hidden_layer_sizes = (50,50,50), max_iter=2000)\n",
        "    clf.fit(np.array(x_train),np.array(y_train))\n",
        "    y_pred = clf.predict(x_test)\n",
        "    y_pred2 = clf.predict_proba(x_test)\n",
        "    \n",
        "    print('log_loss for ',i, ' : ', log_loss(y_test,y_pred2),'\\n')\n",
        "    print(classification_report(y_test,y_pred))\n",
        "    print(\"SVM Accuracy Score -> \",accuracy_score(y_pred, y_test)*100, 'for ', i, '\\n')\n",
        "    i=i+1\n",
        "    tsum = tsum + log_loss(y_test,y_pred2)\n",
        "    \n",
        "print('log_loss average evaluation results of 5 classifiers: ', tsum/5 , \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_loss for  1  :  0.7428073507641397 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.09      0.16       758\n",
            "           1       0.70      1.00      0.82      6826\n",
            "           2       0.47      0.01      0.02      2287\n",
            "\n",
            "    accuracy                           0.70      9871\n",
            "   macro avg       0.60      0.37      0.34      9871\n",
            "weighted avg       0.64      0.70      0.59      9871\n",
            "\n",
            "SVM Accuracy Score ->  69.85107891804276 for  1 \n",
            "\n",
            "log_loss for  2  :  0.7396339186924779 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.09      0.16       802\n",
            "           1       0.71      1.00      0.83      6917\n",
            "           2       0.00      0.00      0.00      2152\n",
            "\n",
            "    accuracy                           0.71      9871\n",
            "   macro avg       0.46      0.36      0.33      9871\n",
            "weighted avg       0.55      0.71      0.59      9871\n",
            "\n",
            "SVM Accuracy Score ->  70.631141728295 for  2 \n",
            "\n",
            "log_loss for  3  :  0.7630944597011607 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.04      0.08       774\n",
            "           1       0.69      1.00      0.82      6778\n",
            "           2       0.16      0.00      0.00      2318\n",
            "\n",
            "    accuracy                           0.69      9870\n",
            "   macro avg       0.51      0.35      0.30      9870\n",
            "weighted avg       0.56      0.69      0.57      9870\n",
            "\n",
            "SVM Accuracy Score ->  68.96656534954407 for  3 \n",
            "\n",
            "log_loss for  4  :  0.7435761884467517 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.24      0.28       716\n",
            "           1       0.73      0.96      0.83      6935\n",
            "           2       0.34      0.03      0.05      2219\n",
            "\n",
            "    accuracy                           0.70      9870\n",
            "   macro avg       0.47      0.41      0.39      9870\n",
            "weighted avg       0.61      0.70      0.61      9870\n",
            "\n",
            "SVM Accuracy Score ->  70.08105369807498 for  4 \n",
            "\n",
            "log_loss for  5  :  0.7545886789764744 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.00      0.01       789\n",
            "           1       0.69      1.00      0.82      6828\n",
            "           2       0.22      0.00      0.01      2253\n",
            "\n",
            "    accuracy                           0.69      9870\n",
            "   macro avg       0.53      0.34      0.28      9870\n",
            "weighted avg       0.58      0.69      0.57      9870\n",
            "\n",
            "SVM Accuracy Score ->  69.26038500506586 for  5 \n",
            "\n",
            "log_loss average evaluation results of 5 classifiers:  0.7487401193162009 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3Qwem7sKnjB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = mlp2.predict_proba(testing)\n",
        "result = pd.DataFrame({'listing_id': test_df['listing_id'], 'high': prediction[:,0], 'medium': prediction[:,2], 'low': prediction[:,1] })\n",
        "result.to_csv('m3_2.csv', index=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDC-1nbWMHDB",
        "colab_type": "code",
        "outputId": "6df18dbc-d693-48ce-bd5d-887f52580cea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "mlp3 = MLPClassifier(alpha = 0.01, hidden_layer_sizes = (50,50,50), max_iter=2000)\n",
        "mlp3.fit(np.array(x),np.array(y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.01, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(50, 50, 50), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=2000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8OOj9EgMHFZ",
        "colab_type": "code",
        "outputId": "05777b09-248f-4f55-aabd-d124f64dd749",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## cross validation\n",
        "\n",
        "train_df_cv = shuffle(train_df)\n",
        "train_df_cv.reset_index(inplace=True, drop=True)\n",
        "x_cv = train_df_cv.drop(feat1, axis=1)\n",
        "y_cv = train_df_cv['target']\n",
        "\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "i=1\n",
        "tsum = 0\n",
        "for train_index, test_index in kf.split(x):\n",
        "    x_train, x_test = x_cv.loc[train_index[0]:train_index[-1]], x_cv.loc[test_index[0]:test_index[-1]]\n",
        "    y_train, y_test = y_cv.loc[train_index[0]:train_index[-1]], y_cv.loc[test_index[0]:test_index[-1]]\n",
        "    clf = MLPClassifier(alpha = 0.01, hidden_layer_sizes = (50,50,50), max_iter=2000)\n",
        "    clf.fit(np.array(x_train),np.array(y_train))\n",
        "    y_pred = clf.predict(x_test)\n",
        "    y_pred2 = clf.predict_proba(x_test)\n",
        "    \n",
        "    print('log_loss for ',i, ' : ', log_loss(y_test,y_pred2),'\\n')\n",
        "    print(classification_report(y_test,y_pred))\n",
        "    print(\"SVM Accuracy Score -> \",accuracy_score(y_pred, y_test)*100, 'for ', i, '\\n')\n",
        "    i=i+1\n",
        "    tsum = tsum + log_loss(y_test,y_pred2)\n",
        "    \n",
        "print('log_loss average evaluation results of 5 classifiers: ', tsum/5 , \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_loss for  1  :  0.7173075994558318 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.11      0.19       811\n",
            "           1       0.72      0.99      0.83      6919\n",
            "           2       0.30      0.02      0.03      2141\n",
            "\n",
            "    accuracy                           0.71      9871\n",
            "   macro avg       0.53      0.37      0.35      9871\n",
            "weighted avg       0.61      0.71      0.61      9871\n",
            "\n",
            "SVM Accuracy Score ->  70.81349407354878 for  1 \n",
            "\n",
            "log_loss for  2  :  0.72739583797117 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.16      0.24       764\n",
            "           1       0.71      0.98      0.83      6815\n",
            "           2       0.39      0.04      0.08      2292\n",
            "\n",
            "    accuracy                           0.70      9871\n",
            "   macro avg       0.53      0.39      0.38      9871\n",
            "weighted avg       0.62      0.70      0.61      9871\n",
            "\n",
            "SVM Accuracy Score ->  70.01316989160166 for  2 \n",
            "\n",
            "log_loss for  3  :  0.7707235332404987 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.00      0.00       756\n",
            "           1       0.69      1.00      0.82      6852\n",
            "           2       0.00      0.00      0.00      2262\n",
            "\n",
            "    accuracy                           0.69      9870\n",
            "   macro avg       0.56      0.33      0.27      9870\n",
            "weighted avg       0.56      0.69      0.57      9870\n",
            "\n",
            "SVM Accuracy Score ->  69.43262411347517 for  3 \n",
            "\n",
            "log_loss for  4  :  0.7712302426646193 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.05      0.10       751\n",
            "           1       0.70      1.00      0.82      6856\n",
            "           2       0.23      0.00      0.01      2263\n",
            "\n",
            "    accuracy                           0.70      9870\n",
            "   macro avg       0.55      0.35      0.31      9870\n",
            "weighted avg       0.59      0.70      0.58      9870\n",
            "\n",
            "SVM Accuracy Score ->  69.8176291793313 for  4 \n",
            "\n",
            "log_loss for  5  :  0.7624413251519918 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.09      0.16       757\n",
            "           1       0.70      0.99      0.82      6842\n",
            "           2       0.33      0.01      0.03      2271\n",
            "\n",
            "    accuracy                           0.70      9870\n",
            "   macro avg       0.55      0.37      0.34      9870\n",
            "weighted avg       0.61      0.70      0.59      9870\n",
            "\n",
            "SVM Accuracy Score ->  69.97973657548125 for  5 \n",
            "\n",
            "log_loss average evaluation results of 5 classifiers:  0.7498197076968223 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhlyZNBjMHID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = mlp3.predict_proba(testing)\n",
        "result = pd.DataFrame({'listing_id': test_df['listing_id'], 'high': prediction[:,0], 'medium': prediction[:,2], 'low': prediction[:,1] })\n",
        "result.to_csv('m3_3.csv', index=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13yNqfsMRvT3",
        "colab_type": "code",
        "outputId": "2ffa5631-9762-464d-afc1-06a3ff2cbf1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "est = [\n",
        "       ('mlp3', MLPClassifier(alpha = 0.01, hidden_layer_sizes = (50,50,50), max_iter=2000)),\n",
        "       ('rf', RandomForestClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=20,n_estimators=50))\n",
        "]\n",
        "\n",
        "mlp_stack = StackingClassifier(estimators=est, final_estimator=RandomForestClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=20,n_estimators=50))\n",
        "mlp_stack.fit(np.array(x),np.array(y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StackingClassifier(cv=None,\n",
              "                   estimators=[('mlp3',\n",
              "                                MLPClassifier(activation='relu', alpha=0.01,\n",
              "                                              batch_size='auto', beta_1=0.9,\n",
              "                                              beta_2=0.999,\n",
              "                                              early_stopping=False,\n",
              "                                              epsilon=1e-08,\n",
              "                                              hidden_layer_sizes=(50, 50, 50),\n",
              "                                              learning_rate='constant',\n",
              "                                              learning_rate_init=0.001,\n",
              "                                              max_fun=15000, max_iter=2000,\n",
              "                                              momentum=0.9, n_iter_no_change=10,\n",
              "                                              nesterovs_momentum=True,\n",
              "                                              power_t=0.5, random...\n",
              "                                                          criterion='gini',\n",
              "                                                          max_depth=None,\n",
              "                                                          max_features='auto',\n",
              "                                                          max_leaf_nodes=None,\n",
              "                                                          max_samples=None,\n",
              "                                                          min_impurity_decrease=0.0,\n",
              "                                                          min_impurity_split=None,\n",
              "                                                          min_samples_leaf=20,\n",
              "                                                          min_samples_split=2,\n",
              "                                                          min_weight_fraction_leaf=0.0,\n",
              "                                                          n_estimators=50,\n",
              "                                                          n_jobs=None,\n",
              "                                                          oob_score=False,\n",
              "                                                          random_state=0,\n",
              "                                                          verbose=0,\n",
              "                                                          warm_start=False),\n",
              "                   n_jobs=None, passthrough=False, stack_method='auto',\n",
              "                   verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X6xuWe7RvXY",
        "colab_type": "code",
        "outputId": "dae89c9e-5646-4163-a744-58f601eb0408",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## cross validation\n",
        "\n",
        "train_df_cv = shuffle(train_df)\n",
        "train_df_cv.reset_index(inplace=True, drop=True)\n",
        "x_cv = train_df_cv.drop(feat1, axis=1)\n",
        "y_cv = train_df_cv['target']\n",
        "\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "i=1\n",
        "tsum = 0\n",
        "for train_index, test_index in kf.split(x):\n",
        "    x_train, x_test = x_cv.loc[train_index[0]:train_index[-1]], x_cv.loc[test_index[0]:test_index[-1]]\n",
        "    y_train, y_test = y_cv.loc[train_index[0]:train_index[-1]], y_cv.loc[test_index[0]:test_index[-1]]\n",
        "    clf = StackingClassifier(estimators=est, final_estimator=RandomForestClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=20,n_estimators=50))\n",
        "    clf.fit(np.array(x),np.array(y))\n",
        "    y_pred = clf.predict(x_test)\n",
        "    y_pred2 = clf.predict_proba(x_test)\n",
        "    \n",
        "    print('log_loss for ',i, ' : ', log_loss(y_test,y_pred2),'\\n')\n",
        "    print(classification_report(y_test,y_pred))\n",
        "    print(\"SVM Accuracy Score -> \",accuracy_score(y_pred, y_test)*100, 'for ', i, '\\n')\n",
        "    i=i+1\n",
        "    tsum = tsum + log_loss(y_test,y_pred2)\n",
        "    \n",
        "print('log_loss average evaluation results of 5 classifiers: ', tsum/5 , \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_loss for  1  :  0.6297635974951408 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.13      0.21       749\n",
            "           1       0.76      0.94      0.84      6903\n",
            "           2       0.48      0.25      0.33      2219\n",
            "\n",
            "    accuracy                           0.73      9871\n",
            "   macro avg       0.60      0.44      0.46      9871\n",
            "weighted avg       0.68      0.73      0.68      9871\n",
            "\n",
            "SVM Accuracy Score ->  72.5053186100699 for  1 \n",
            "\n",
            "log_loss for  2  :  0.6387197582863193 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.15      0.25       794\n",
            "           1       0.77      0.93      0.84      6859\n",
            "           2       0.45      0.29      0.35      2218\n",
            "\n",
            "    accuracy                           0.72      9871\n",
            "   macro avg       0.62      0.46      0.48      9871\n",
            "weighted avg       0.69      0.72      0.69      9871\n",
            "\n",
            "SVM Accuracy Score ->  72.23179009218924 for  2 \n",
            "\n",
            "log_loss for  3  :  0.6395917490662623 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.47      0.22      0.30       744\n",
            "           1       0.76      0.93      0.84      6827\n",
            "           2       0.47      0.24      0.31      2299\n",
            "\n",
            "    accuracy                           0.72      9870\n",
            "   macro avg       0.56      0.46      0.48      9870\n",
            "weighted avg       0.67      0.72      0.67      9870\n",
            "\n",
            "SVM Accuracy Score ->  71.56028368794325 for  3 \n",
            "\n",
            "log_loss for  4  :  0.6422057200187937 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.20      0.28       787\n",
            "           1       0.77      0.92      0.84      6862\n",
            "           2       0.45      0.27      0.34      2221\n",
            "\n",
            "    accuracy                           0.72      9870\n",
            "   macro avg       0.57      0.46      0.49      9870\n",
            "weighted avg       0.67      0.72      0.68      9870\n",
            "\n",
            "SVM Accuracy Score ->  71.62107396149949 for  4 \n",
            "\n",
            "log_loss for  5  :  0.6446954966878636 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.19      0.27       765\n",
            "           1       0.76      0.93      0.84      6833\n",
            "           2       0.48      0.26      0.33      2272\n",
            "\n",
            "    accuracy                           0.72      9870\n",
            "   macro avg       0.58      0.46      0.48      9870\n",
            "weighted avg       0.68      0.72      0.68      9870\n",
            "\n",
            "SVM Accuracy Score ->  71.95542046605877 for  5 \n",
            "\n",
            "log_loss average evaluation results of 5 classifiers:  0.6389952643108759 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2EKw9hlRviH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = mlp_stack.predict_proba(testing)\n",
        "result = pd.DataFrame({'listing_id': test_df['listing_id'], 'high': prediction[:,0], 'medium': prediction[:,2], 'low': prediction[:,1] })\n",
        "result.to_csv('m3_stack.csv', index=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ojY__c0pNXK",
        "colab_type": "code",
        "outputId": "465e9e02-de7f-4811-bb85-7c8fd22e8145",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "est = [\n",
        "       ('mlp3', MLPClassifier(alpha = 0.01, hidden_layer_sizes = (50,50,50), max_iter=2000)),\n",
        "       ('lr', RandomForestClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=20,n_estimators=50))\n",
        "]\n",
        "\n",
        "mlp_stack2 = StackingClassifier(estimators=est, final_estimator=MLPClassifier(alpha = 0.01, hidden_layer_sizes = (50,50,50), max_iter=2000))\n",
        "mlp_stack2.fit(np.array(x),np.array(y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StackingClassifier(cv=None,\n",
              "                   estimators=[('mlp3',\n",
              "                                MLPClassifier(activation='relu', alpha=0.01,\n",
              "                                              batch_size='auto', beta_1=0.9,\n",
              "                                              beta_2=0.999,\n",
              "                                              early_stopping=False,\n",
              "                                              epsilon=1e-08,\n",
              "                                              hidden_layer_sizes=(50, 50, 50),\n",
              "                                              learning_rate='constant',\n",
              "                                              learning_rate_init=0.001,\n",
              "                                              max_fun=15000, max_iter=2000,\n",
              "                                              momentum=0.9, n_iter_no_change=10,\n",
              "                                              nesterovs_momentum=True,\n",
              "                                              power_t=0.5, random...\n",
              "                                                 epsilon=1e-08,\n",
              "                                                 hidden_layer_sizes=(50, 50,\n",
              "                                                                     50),\n",
              "                                                 learning_rate='constant',\n",
              "                                                 learning_rate_init=0.001,\n",
              "                                                 max_fun=15000, max_iter=2000,\n",
              "                                                 momentum=0.9,\n",
              "                                                 n_iter_no_change=10,\n",
              "                                                 nesterovs_momentum=True,\n",
              "                                                 power_t=0.5, random_state=None,\n",
              "                                                 shuffle=True, solver='adam',\n",
              "                                                 tol=0.0001,\n",
              "                                                 validation_fraction=0.1,\n",
              "                                                 verbose=False,\n",
              "                                                 warm_start=False),\n",
              "                   n_jobs=None, passthrough=False, stack_method='auto',\n",
              "                   verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3dwfNo6pNbp",
        "colab_type": "code",
        "outputId": "da74238d-6175-4307-8440-40bf38b3c96f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## cross validation\n",
        "\n",
        "train_df_cv = shuffle(train_df)\n",
        "train_df_cv.reset_index(inplace=True, drop=True)\n",
        "x_cv = train_df_cv.drop(feat1, axis=1)\n",
        "y_cv = train_df_cv['target']\n",
        "\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "i=1\n",
        "tsum = 0\n",
        "for train_index, test_index in kf.split(x):\n",
        "    x_train, x_test = x_cv.loc[train_index[0]:train_index[-1]], x_cv.loc[test_index[0]:test_index[-1]]\n",
        "    y_train, y_test = y_cv.loc[train_index[0]:train_index[-1]], y_cv.loc[test_index[0]:test_index[-1]]\n",
        "    clf = StackingClassifier(estimators=est, final_estimator=MLPClassifier(alpha = 0.01, hidden_layer_sizes = (50,50,50), max_iter=2000))\n",
        "    clf.fit(np.array(x),np.array(y))\n",
        "    y_pred = clf.predict(x_test)\n",
        "    y_pred2 = clf.predict_proba(x_test)\n",
        "    \n",
        "    print('log_loss for ',i, ' : ', log_loss(y_test,y_pred2),'\\n')\n",
        "    print(classification_report(y_test,y_pred))\n",
        "    print(\"SVM Accuracy Score -> \",accuracy_score(y_pred, y_test)*100, 'for ', i, '\\n')\n",
        "    i=i+1\n",
        "    tsum = tsum + log_loss(y_test,y_pred2)\n",
        "    \n",
        "print('log_loss average evaluation results of 5 classifiers: ', tsum/5 , \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_loss for  1  :  0.6298123595318583 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.20      0.30       791\n",
            "           1       0.78      0.92      0.85      6856\n",
            "           2       0.48      0.33      0.39      2224\n",
            "\n",
            "    accuracy                           0.73      9871\n",
            "   macro avg       0.63      0.48      0.51      9871\n",
            "weighted avg       0.70      0.73      0.70      9871\n",
            "\n",
            "SVM Accuracy Score ->  73.06250633167866 for  1 \n",
            "\n",
            "log_loss for  2  :  0.6255790120317299 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.19      0.28       755\n",
            "           1       0.77      0.94      0.85      6885\n",
            "           2       0.51      0.28      0.36      2231\n",
            "\n",
            "    accuracy                           0.73      9871\n",
            "   macro avg       0.59      0.47      0.50      9871\n",
            "weighted avg       0.69      0.73      0.69      9871\n",
            "\n",
            "SVM Accuracy Score ->  73.22459730523757 for  2 \n",
            "\n",
            "log_loss for  3  :  0.6247915215189076 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.15      0.24       748\n",
            "           1       0.76      0.95      0.85      6881\n",
            "           2       0.48      0.24      0.32      2241\n",
            "\n",
            "    accuracy                           0.73      9870\n",
            "   macro avg       0.59      0.45      0.47      9870\n",
            "weighted avg       0.68      0.73      0.68      9870\n",
            "\n",
            "SVM Accuracy Score ->  72.65450861195542 for  3 \n",
            "\n",
            "log_loss for  4  :  0.6449473222105153 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.13      0.21       789\n",
            "           1       0.76      0.94      0.84      6811\n",
            "           2       0.49      0.28      0.36      2270\n",
            "\n",
            "    accuracy                           0.72      9870\n",
            "   macro avg       0.59      0.45      0.47      9870\n",
            "weighted avg       0.68      0.72      0.68      9870\n",
            "\n",
            "SVM Accuracy Score ->  72.1580547112462 for  4 \n",
            "\n",
            "log_loss for  5  :  0.6342478404689841 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.15      0.24       756\n",
            "           1       0.78      0.92      0.84      6851\n",
            "           2       0.48      0.35      0.41      2263\n",
            "\n",
            "    accuracy                           0.73      9870\n",
            "   macro avg       0.63      0.47      0.50      9870\n",
            "weighted avg       0.70      0.73      0.70      9870\n",
            "\n",
            "SVM Accuracy Score ->  72.86727456940223 for  5 \n",
            "\n",
            "log_loss average evaluation results of 5 classifiers:  0.6318756111523991 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9YemgucpNZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = mlp_stack2.predict_proba(testing)\n",
        "result = pd.DataFrame({'listing_id': test_df['listing_id'], 'high': prediction[:,0], 'medium': prediction[:,2], 'low': prediction[:,1] })\n",
        "result.to_csv('m3_stack2.csv', index=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GP9weGz-KWRp",
        "colab_type": "code",
        "outputId": "937e42ea-c27a-4746-fee9-571dc353a76c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "est = [\n",
        "       ('rf1', RandomForestClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=20,n_estimators=50)),\n",
        "       ('rf2', RandomForestClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=25,n_estimators=55)),\n",
        "       ('rf3', RandomForestClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=30,n_estimators=60))\n",
        "]\n",
        "\n",
        "mlp_stack3 = StackingClassifier(estimators=est, final_estimator=RandomForestClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=20,n_estimators=50))\n",
        "mlp_stack3.fit(np.array(x),np.array(y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StackingClassifier(cv=None,\n",
              "                   estimators=[('rf1',\n",
              "                                RandomForestClassifier(bootstrap=True,\n",
              "                                                       ccp_alpha=0.0,\n",
              "                                                       class_weight=None,\n",
              "                                                       criterion='gini',\n",
              "                                                       max_depth=None,\n",
              "                                                       max_features='auto',\n",
              "                                                       max_leaf_nodes=None,\n",
              "                                                       max_samples=None,\n",
              "                                                       min_impurity_decrease=0.0,\n",
              "                                                       min_impurity_split=None,\n",
              "                                                       min_samples_leaf=20,\n",
              "                                                       min_samples_split=2,\n",
              "                                                       min_weight_fraction_leaf=0.0,\n",
              "                                                       n_estimators=50,\n",
              "                                                       n_jobs=Non...\n",
              "                                                          criterion='gini',\n",
              "                                                          max_depth=None,\n",
              "                                                          max_features='auto',\n",
              "                                                          max_leaf_nodes=None,\n",
              "                                                          max_samples=None,\n",
              "                                                          min_impurity_decrease=0.0,\n",
              "                                                          min_impurity_split=None,\n",
              "                                                          min_samples_leaf=20,\n",
              "                                                          min_samples_split=2,\n",
              "                                                          min_weight_fraction_leaf=0.0,\n",
              "                                                          n_estimators=50,\n",
              "                                                          n_jobs=None,\n",
              "                                                          oob_score=False,\n",
              "                                                          random_state=0,\n",
              "                                                          verbose=0,\n",
              "                                                          warm_start=False),\n",
              "                   n_jobs=None, passthrough=False, stack_method='auto',\n",
              "                   verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c87K4EgKWc2",
        "colab_type": "code",
        "outputId": "7cf15ed3-e3a5-48d9-f1c1-b0092880435c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## cross validation\n",
        "\n",
        "train_df_cv = shuffle(train_df)\n",
        "train_df_cv.reset_index(inplace=True, drop=True)\n",
        "x_cv = train_df_cv.drop(feat1, axis=1)\n",
        "y_cv = train_df_cv['target']\n",
        "\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "i=1\n",
        "tsum = 0\n",
        "for train_index, test_index in kf.split(x):\n",
        "    x_train, x_test = x_cv.loc[train_index[0]:train_index[-1]], x_cv.loc[test_index[0]:test_index[-1]]\n",
        "    y_train, y_test = y_cv.loc[train_index[0]:train_index[-1]], y_cv.loc[test_index[0]:test_index[-1]]\n",
        "    clf = StackingClassifier(estimators=est, final_estimator=RandomForestClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=20,n_estimators=50))\n",
        "    clf.fit(np.array(x),np.array(y))\n",
        "    y_pred = clf.predict(x_test)\n",
        "    y_pred2 = clf.predict_proba(x_test)\n",
        "    \n",
        "    print('log_loss for ',i, ' : ', log_loss(y_test,y_pred2),'\\n')\n",
        "    print(classification_report(y_test,y_pred))\n",
        "    print(\"SVM Accuracy Score -> \",accuracy_score(y_pred, y_test)*100, 'for ', i, '\\n')\n",
        "    i=i+1\n",
        "    tsum = tsum + log_loss(y_test,y_pred2)\n",
        "    \n",
        "print('log_loss average evaluation results of 5 classifiers: ', tsum/5 , \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_loss for  1  :  0.6364936949776013 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.19      0.28       756\n",
            "           1       0.77      0.93      0.84      6857\n",
            "           2       0.48      0.28      0.35      2258\n",
            "\n",
            "    accuracy                           0.72      9871\n",
            "   macro avg       0.59      0.47      0.49      9871\n",
            "weighted avg       0.68      0.72      0.69      9871\n",
            "\n",
            "SVM Accuracy Score ->  72.36348900820585 for  1 \n",
            "\n",
            "log_loss for  2  :  0.6376405123908137 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.17      0.26       788\n",
            "           1       0.77      0.93      0.84      6915\n",
            "           2       0.45      0.26      0.33      2168\n",
            "\n",
            "    accuracy                           0.72      9871\n",
            "   macro avg       0.58      0.46      0.48      9871\n",
            "weighted avg       0.68      0.72      0.69      9871\n",
            "\n",
            "SVM Accuracy Score ->  72.42427312329045 for  2 \n",
            "\n",
            "log_loss for  3  :  0.6397480117700887 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.18      0.28       775\n",
            "           1       0.76      0.93      0.84      6798\n",
            "           2       0.48      0.27      0.35      2297\n",
            "\n",
            "    accuracy                           0.72      9870\n",
            "   macro avg       0.61      0.46      0.49      9870\n",
            "weighted avg       0.68      0.72      0.68      9870\n",
            "\n",
            "SVM Accuracy Score ->  72.01621073961499 for  3 \n",
            "\n",
            "log_loss for  4  :  0.6387266421084652 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.16      0.25       779\n",
            "           1       0.76      0.93      0.84      6853\n",
            "           2       0.46      0.26      0.33      2238\n",
            "\n",
            "    accuracy                           0.72      9870\n",
            "   macro avg       0.58      0.45      0.47      9870\n",
            "weighted avg       0.68      0.72      0.68      9870\n",
            "\n",
            "SVM Accuracy Score ->  71.83383991894631 for  4 \n",
            "\n",
            "log_loss for  5  :  0.6361789910624097 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.47      0.16      0.24       741\n",
            "           1       0.76      0.93      0.84      6861\n",
            "           2       0.47      0.26      0.33      2268\n",
            "\n",
            "    accuracy                           0.72      9870\n",
            "   macro avg       0.57      0.45      0.47      9870\n",
            "weighted avg       0.67      0.72      0.68      9870\n",
            "\n",
            "SVM Accuracy Score ->  71.82370820668693 for  5 \n",
            "\n",
            "log_loss average evaluation results of 5 classifiers:  0.6377575704618758 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG5n92kaKWi9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = mlp_stack3.predict_proba(testing)\n",
        "result = pd.DataFrame({'listing_id': test_df['listing_id'], 'high': prediction[:,0], 'medium': prediction[:,2], 'low': prediction[:,1] })\n",
        "result.to_csv('m3_stack3.csv', index=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7GNdWKw5n6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# svc_class = SVC(kernel='linear', probability = True, cache_size=1000, )\n",
        "# svc_class.fit(np.array(x), np.array(y))\n",
        "\n",
        "# sclf = SVC(kernel='linear', probability = True, cache_size=1000)\n",
        "# sclf.fit(x,y)\n",
        "# rclf = RandomForestClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=20,n_estimators=50,min_samples_split=100)\n",
        "# rclf.fit(x,y)\n",
        "# mclf = MLPClassifier(alpha=1, max_iter=1000)\n",
        "# mclf.fit(x,y)\n",
        "\n",
        "# est = [\n",
        "#        ('svm1', make_pipeline(MaxAbsScaler(),\n",
        "#                              LinearSVC())),\n",
        "#        ('svm2', make_pipeline(MaxAbsScaler(),\n",
        "#                              linear_model.SGDClassifier()))  \n",
        "# ]\n",
        "\n",
        "# svc_class = StackingClassifier(estimators=est, final_estimator=SVC(kernel = 'linear', probability=True))\n",
        "# svc_class.fit(np.array(x),np.array(y))\n",
        "\n",
        "# svc_class = LinearSVC(random_state=0, tol=1e-5)\n",
        "# svc_class.fit(x,y)\n",
        "# svc_class = MLPClassifier(alpha = 0.1, hidden_layer_sizes = (50,50,50), max_iter=1000)\n",
        "# svc_class.fit(np.array(x), np.array(y))\n",
        "# svc_class = RandomForestClassifier(criterion=\"gini\", random_state=0,min_samples_leaf=20,n_estimators=50,min_samples_split=100)\n",
        "# svc_class.fit(np.array(x), np.array(y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOE7hB8p5n6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ## Cross Validation\n",
        "\n",
        "# train_df_cv = shuffle(train_df)\n",
        "# train_df_cv.reset_index(inplace=True, drop=True)\n",
        "# x_cv = train_df_cv.drop(feat1, axis=1)\n",
        "# y_cv = train_df_cv['target']\n",
        "\n",
        "\n",
        "# kf = KFold(n_splits=5)\n",
        "# i=1\n",
        "# tsum = 0\n",
        "# for train_index, test_index in kf.split(x):\n",
        "#     x_train, x_test = x_cv.loc[train_index[0]:train_index[-1]], x_cv.loc[test_index[0]:test_index[-1]]\n",
        "#     y_train, y_test = y_cv.loc[train_index[0]:train_index[-1]], y_cv.loc[test_index[0]:test_index[-1]]\n",
        "#     est = [\n",
        "#        ('svm1', make_pipeline(MaxAbsScaler(),\n",
        "#                              LinearSVC())),\n",
        "#        ('svm2', make_pipeline(MaxAbsScaler(),\n",
        "#                              linear_model.SGDClassifier()))  \n",
        "#     ]\n",
        "#     clf = StackingClassifier(estimators=est, final_estimator=SVC(kernel = 'linear', probability=True))\n",
        "#     clf.fit(np.array(x_train),np.array(y_train))\n",
        "#     y_pred = clf.predict(x_test)\n",
        "#     y_pred2 = clf.predict_proba(x_test)\n",
        "    \n",
        "#     print('log_loss for ',i, ' : ', log_loss(y_test,y_pred2),'\\n')\n",
        "#     print(classification_report(y_test,y_pred))\n",
        "#     print(\"SVM Accuracy Score -> \",accuracy_score(y_pred, y_test)*100, 'for ', i, '\\n')\n",
        "#     i=i+1\n",
        "#     tsum = tsum + log_loss(y_test,y_pred2)\n",
        "    \n",
        "# print('log_loss average evaluation results of 5 classifiers: ', tsum/5 , \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lKZGzlq5n6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}