---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.3.3
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

# Phase 2: Decision Tree Classifier

```{python}
from Preprocessing import *;
from sklearn.model_selection import StratifiedKFold,KFold,train_test_split;
from sklearn.preprocessing import LabelEncoder, OneHotEncoder;
from sklearn.tree import DecisionTreeClassifier;
from sklearn.ensemble import RandomForestClassifier;
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report;
from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer;
from pandas.api.types import CategoricalDtype
plt.style.use('ggplot')
```

```{python}
data = train_df.reset_index(drop=True)
```

## Processing test dataset

```{python}
test = pd.read_json("test.json")
test = test.reset_index(drop=True)
```

```{python}
# convert column to datetime
test["created"] = pd.to_datetime(test["created"])
# add hour created column
test["hour_created"] = test["created"].dt.hour
```

```{python}
# label encode
test["address"] = test["display_address"].astype('category')
test["address"] = test["address"].cat.codes
test["manager_id"] = test["manager_id"].astype('category')
test["manager"] = test["manager_id"].cat.codes
test.drop(['manager_id'], axis=1, inplace=True)
test["building_id"] = test["building_id"].astype('category')
test["building"] = test["building_id"].cat.codes
test.drop(['building_id'], axis=1, inplace=True)
```

```{python}
test.drop(['photos','street_address','features','description','created','display_address'], axis=1, inplace=True)
```

# 1. Feature Selection

Since the listing_id is unique to each lising, it really isn't relevant for our model and will be removed. We will also remove the photos column because we won't be using this in our model either. We will also remove street_address because we can use the display_address as a categorical value since this is just the general street that the listing is on. This helps identify the general neighbourhood the listing is in for the buyer. The description, created, and features columns are removed because these are represented by newly extracted feature columns from the previous milestone.

```{python}
data.drop(['photos','listing_id','street_address','features','description','created'], axis=1, inplace=True)
```

## Binarize data by label encoding

```{python}
cat_type = CategoricalDtype(categories=["high", "medium", "low"],ordered=True)
cat_type
```

```{python}
# high-0, medium-1, low-2
data["interest_level"] = data["interest_level"].astype(cat_type)
data["target"] = data["interest_level"].cat.codes
data.drop(['interest_level'], axis=1, inplace=True)
```

```{python}
data["address"] = data["display_address"].astype('category')
data["address"] = data["address"].cat.codes
data.drop(['display_address'], axis=1, inplace=True)
```

```{python}
data["building_id"] = data["building_id"].astype('category')
data["building"] = data["building_id"].cat.codes
data.drop(['building_id'], axis=1, inplace=True)
```

```{python}
data["manager_id"] = data["manager_id"].astype('category')
data["manager"] = data["manager_id"].cat.codes
data.drop(['manager_id'], axis=1, inplace=True)
```

## Split the data

I will split our input and target variables into X and y respectively. We will also split into training and test data so we can see if our models are overfitting the data.

```{python}
X = data.drop(['target'], axis=1)
y = data['target']
X_tr, X_test, y_tr, y_test = train_test_split(X,y)
```

```{python}
X_test.reset_index(inplace=True,drop=True)
X_tr.reset_index(inplace=True,drop=True)
```

## Fisher score

Since this is only valid for numerica features, we must first take a subset of our data. We will separate the data based on interest level to calculate the mean and standard deviation of each class. These values will be used to calculate the fisher score for each feature. We can then use a filtering technique to find the most relevant features based on the scores. 

```{python}
# take subset
numeric = ['bathrooms','bedrooms','latitude','longitude','price','hour_created','address','manager','building']
num_data = data[['bathrooms','bedrooms','latitude','longitude','price','hour_created','address','manager','building','target']]
```

```{python}
# separate data by interest level
high = num_data[num_data['target'] == 2].drop(['target'], axis=1)
med = num_data[num_data['target'] == 1].drop(['target'], axis=1)
low = num_data[num_data['target'] == 0].drop(['target'], axis=1)
```

```{python}
# calculate mean of each feature in each class
avg_if = np.array([high.mean(),
      med.mean(),
      low.mean()])
#calculate mean of each feature
avg_f = np.array(num_data.drop(['target'], axis=1).mean())
```

```{python}
# calculate variance for each feature
var = np.square(np.array([high.std(),
      med.std(),
      low.std()]))

# calculate probability of each class
prob = np.array([high.shape[0]/float(data.shape[0]),
             med.shape[0]/float(data.shape[0]),
             low.shape[0]/float(data.shape[0])]
)
```

```{python}
fisher = []
numerator = []
denomerator = []

for feat in np.arange(0,avg_if.shape[1]):
    for class_i in np.arange(0,avg_if.shape[0]):
        numerator.append(np.array(prob[class_i]*np.square(avg_if[class_i][feat]-avg_f[feat])))
        denomerator.append(np.array(prob[class_i]*var[class_i]))      
    fisher.append(np.array(numerator).sum()/np.array(denomerator).sum())
fisher
```

```{python}
top3 = ['building', 'manager', 'address']
top5 = ['building', 'manager', 'address','price', 'hour_created']
```

From the Fisher scores, it looks like the building, address, and manager have the top 3 fisher scores, in that order.  Price and hour the listing was created also have decent fisher scores. The worst was longitude and the rest weren't that far off either with 10 significant digits. 


# 2. Train Classifiers


## First Version

The classifier doesn't accept the extracted text features from the previous milestone so for the first classifier they were removed and the classifier was trained for the numerical data only. The 3 features with the highest fisher scores are selected for the first classifier. These are building, address, and manager. The default Gini index was used to determine the best split for each node. The classifier predicted with average 62.73% accuracy. You can see the classifier is biased and has higher precision, recall, and f1 scores for the low interest class. More than triple the high interest. It also overfits to the training data because the accuracy for the test dataset is almost 20% higher.

```{python}
# perform cross-validation for first set of classifiers
kf = KFold(n_splits=5)
acc_scores = []
best_acc = 0
for train_index, valid_index in kf.split(X_tr):
    # split data
    X_train, X_valid = X_tr.iloc[train_index], X_tr.iloc[valid_index]
    y_train, y_valid = y_tr.iloc[train_index], y_tr.iloc[valid_index]
    # Create classifier object
    clf = DecisionTreeClassifier(criterion="gini", random_state=0)
    # Train Decision Tree Classifer
    clf = clf.fit(np.array(X_train[top3]),np.array(y_train),check_input=False)
    #Predict the response for test dataset
    y_pred = clf.predict(X_valid[top3])
    acc = accuracy_score(y_valid, y_pred)
    acc_scores.append(acc)
    if acc > best_acc:
        best_acc = acc
        best_clf = clf
acc_scores
```

```{python}
np.array(acc_scores).mean()
```

```{python}
print(classification_report(y_valid, y_pred, target_names=['high', 'medium','low']))
```

```{python}
# Check overfitting
y_pred = clf.predict(X_test[top3])
acc = accuracy_score(y_test, y_pred)
acc
```

```{python}
# use best classifier on test dataset
y_pred = best_clf.predict_proba(test[top3])
```

```{python}
listing_id = test['listing_id'].values
listing_id = listing_id.reshape(listing_id.shape[0],1)
```

```{python}
# Test output for log loss
test_prob = np.concatenate((listing_id,y_pred),axis=1)
np.savetxt('test_v1.csv',test_prob, delimiter=',',header='listing_id,high,medium,low')
```

## Second Version

Top 5 features according to Fisher scores were used for this classifier. Gini index was used to determine the best split for each node. The classifier predicted with the same average accuracy (63.32%) as the first version.

```{python}
# perform cross-validation for first set of classifiers
kf = KFold(n_splits=5)
acc_scores = []
for train_index, valid_index in kf.split(X_tr):
    # split data
    X_train, X_valid = X_tr.iloc[train_index], X_tr.iloc[valid_index]
    y_train, y_valid = y_tr.iloc[train_index], y_tr.iloc[valid_index]
    # Create classifier object
    clf = DecisionTreeClassifier(criterion="gini", random_state=0)
    # Train Decision Tree Classifer
    clf = clf.fit(np.array(X_train[top5]),np.array(y_train),check_input=False)
    #Predict the response for test dataset
    y_pred = clf.predict(X_valid[top5])
    acc = accuracy_score(y_valid, y_pred)
    acc_scores.append(acc)
    if acc > best_acc:
        best_acc = acc
        best_clf = clf
acc_scores
```

```{python}
np.array(acc_scores).mean()
```

```{python}
print(classification_report(y_valid, y_pred, target_names=['high', 'medium','low']))
```

```{python}
# Check overfitting
y_pred = clf.predict(X_test[top5])
acc = accuracy_score(y_test, y_pred)
acc
```

## Third Version

All numeric features were used for this classifier. The default Gini index was used to determine the best split for each node like before. The classifier predicted with average 65.66% accuracy. It looks like the classifier has slightly better accuracy with all features.

```{python}
# perform cross-validation for first set of classifiers
kf = KFold(n_splits=5)
acc_scores = []
for train_index, valid_index in kf.split(X_tr):
    # split data
    X_train, X_valid = X_tr.iloc[train_index], X_tr.iloc[valid_index]
    y_train, y_valid = y_tr.iloc[train_index], y_tr.iloc[valid_index]
    # Create classifier object
    clf = DecisionTreeClassifier(criterion="gini", random_state=0)
    # Train Decision Tree Classifer
    clf = clf.fit(np.array(X_train[numeric]),np.array(y_train),check_input=False)
    #Predict the response for test dataset
    y_pred = clf.predict(X_valid[numeric])
    acc = accuracy_score(y_valid, y_pred)
    acc_scores.append(acc)
    if acc > best_acc:
        best_acc = acc
        best_clf = clf
acc_scores
```

```{python}
np.array(acc_scores).mean()
```

```{python}
print(classification_report(y_valid, y_pred, target_names=['high', 'medium','low']))
```

```{python}
# Check overfitting
y_pred = clf.predict(X_test[numeric])
acc = accuracy_score(y_test, y_pred)
acc
```

## Fourth Version

We can check the distribution of the target to see if stratifying the samples will help us identify any under-represented classes.

```{python}
int_level = train_df['interest_level'].value_counts()
plt.figure(figsize=(15,6))
sns.barplot(int_level.index, int_level.values, alpha=1, order=['low','medium','high'],color=color[0])
plt.ylabel('Number of Occurrences', fontsize=14, fontweight='bold')
plt.xlabel('Interest level', fontsize=14, fontweight='bold')
plt.title('Target distribution', fontsize=16, fontweight='bold')
plt.show()
```

```{python}
int_all = int_level.sum()
prob_h = int_level[0]/float(int_all)
prob_m = int_level[1]/float(int_all)
prob_l = int_level[2]/float(int_all)
print(prob_h, prob_m, prob_l)
```

Since the distribution of our target is skewed, we should stratify our data when spliting so it represents our data well. All numeric features were used for this classifier. Gini index was used to determine the best split for each node. A stratified k-fold cross validation method was used. The classifier predicted with average 65.74% accuracy, which is ever so slightly better than regular k-fold cross validation.

```{python}
# perform cross-validation for first set of classifiers
kf = StratifiedKFold(n_splits=5)
acc_scores = []
for train_index, valid_index in kf.split(X_tr, y_tr):
    # split data
    X_train, X_valid = X_tr.iloc[train_index], X_tr.iloc[valid_index]
    y_train, y_valid = y_tr.iloc[train_index], y_tr.iloc[valid_index]
    # Create classifier object
    clf = DecisionTreeClassifier(criterion="gini", random_state=0)
    # Train Decision Tree Classifer
    clf = clf.fit(np.array(X_train[numeric]),np.array(y_train),check_input=False)
    #Predict the response for test dataset
    y_pred = clf.predict(X_valid[numeric])
    acc = accuracy_score(y_valid, y_pred)
    acc_scores.append(acc)
    if acc > best_acc:
        best_acc = acc
        best_clf = clf
acc_scores
```

```{python}
np.array(acc_scores).mean()
```

```{python}
print(classification_report(y_valid, y_pred, target_names=['high', 'medium','low']))
```

```{python}
# Check overfitting
y_pred = clf.predict(X_test[numeric])
acc = accuracy_score(y_test, y_pred)
acc
```

## Fifth Version

All numeric features were used for this classifier. Entropy was used to determine the best split for each node. A stratified k-fold cross validation method was used. The classifier predicted with average 65.62% accuracy, which is ever so slightly worse than using gini index to find the best split.

```{python}
# perform cross-validation for first set of classifiers
kf = StratifiedKFold(n_splits=5)
acc_scores = []
best_acc = 0
for train_index, valid_index in kf.split(X_tr, y_tr):
    # split data
    X_train, X_valid = X_tr.iloc[train_index], X_tr.iloc[valid_index]
    y_train, y_valid = y_tr.iloc[train_index], y_tr.iloc[valid_index]
    # Create classifier object
    clf = DecisionTreeClassifier(criterion="entropy", random_state=0)
    # Train Decision Tree Classifer
    clf = clf.fit(np.array(X_train[numeric]),np.array(y_train),check_input=False)
    #Predict the response for test dataset
    y_pred = clf.predict(X_valid[numeric])
    acc = accuracy_score(y_valid, y_pred)
    acc_scores.append(acc)
    if acc > best_acc:
        best_acc = acc
        best_clf = clf
acc_scores
```

```{python}
np.array(acc_scores).mean()
```

```{python}
print(classification_report(y_valid, y_pred, target_names=['high', 'medium','low']))
```

```{python}
# Check overfitting
y_pred = clf.predict(X_test[numeric])
acc = accuracy_score(y_test, y_pred)
acc
```

## Sixth Version

All numeric features were used for this classifier. Gini index was used to determine the best split for each node since it performed slightly better. A stratified k-fold cross validation method was used. Minimum support was changed to 3 to combat overfitting. The classifier predicted with average 66.46% accuracy.

```{python}
# perform cross-validation for first set of classifiers
kf = StratifiedKFold(n_splits=5)
acc_scores = []
for train_index, valid_index in kf.split(X_tr, y_tr):
    # split data
    X_train, X_valid = X_tr.iloc[train_index], X_tr.iloc[valid_index]
    y_train, y_valid = y_tr.iloc[train_index], y_tr.iloc[valid_index]
    # Create classifier object
    clf = DecisionTreeClassifier(criterion="gini", random_state=0,min_samples_leaf=3)
    # Train Decision Tree Classifer
    clf = clf.fit(np.array(X_train[numeric]),np.array(y_train),check_input=False)
    #Predict the response for test dataset
    y_pred = clf.predict(X_valid[numeric])
    acc = accuracy_score(y_valid, y_pred)
    acc_scores.append(acc)
    if acc > best_acc:
        best_acc = acc
        best_clf = clf
acc_scores
```

```{python}
np.array(acc_scores).mean()
```

```{python}
print(classification_report(y_valid, y_pred, target_names=['high', 'medium','low']))
```

```{python}
# Check overfitting
y_pred = clf.predict(X_test[numeric])
acc = accuracy_score(y_test, y_pred)
acc
```

## Seventh Version

We can introduce our extracted text features for this version.


### Hot encode features vector

The Decision Tree classifier in the sklearn sorts each feature to split the data and doesn't support features that are vectors. So to use any text features we must hot encode them. However, the description vectors are 1119 long so we will only do the features. Looking at the two text features, it seems like description might not be that helpful because there are a lot of irrelevant words/strings in most of them like 'XXX'. 

```{python}
feat = X_tr['feat_vect'].apply(lambda x: x.toarray().tolist())
feat_list = []
for i in feat:
    feat_list.append(i)
```

```{python}
X2 = pd.DataFrame(np.array(feat_list).reshape(np.array(feat_list).shape[0],np.array(feat_list).shape[2]))
X2 = X_tr[numeric].join(X2)
```

```{python}
feat = X_test['feat_vect'].apply(lambda x: x.toarray().tolist())
feat_list = []
for i in feat:
    feat_list.append(i)
```

```{python}
X_test2 = pd.DataFrame(np.array(feat_list).reshape(np.array(feat_list).shape[0],np.array(feat_list).shape[2]))
X_test = X_test[numeric].join(X_test2)
```

All numeric features and the hot encoded features were used for this classifier. Gini was used to determine the best split for each node. A stratified k-fold cross validation method was used. Minimum support was changed to 3 to combat overfitting seen from the previous versions. The classifier predicted with average 53.55% accuracy, which is significantly worse than the previous. Looking at the tfidf vectors of the features column, it looks like each one only has one value out of the 79 word dictionary. The features of most listing must be very similar or have very rare words. The classifier is no longer overfitting but the hot encoded data is throwing off the classifier. It must be splitting the data using these columns closer to the root node, which isn't very helpful.

```{python}
# perform cross-validation for first set of classifiers
kf = StratifiedKFold(n_splits=5)
acc_scores = []
for train_index, valid_index in kf.split(X2, y_tr):
    # split data
    X_train, X_valid = X2.iloc[train_index], X2.iloc[valid_index]
    y_train, y_valid = y_tr.iloc[train_index], y_tr.iloc[valid_index]
    # Create classifier object
    clf = DecisionTreeClassifier(criterion="gini", random_state=0,min_samples_leaf=3)
    # Train Decision Tree Classifer
    clf = clf.fit(np.array(X_train),np.array(y_train),check_input=False)
    #Predict the response for test dataset
    y_pred = clf.predict(X_valid)
    acc = accuracy_score(y_valid, y_pred)
    acc_scores.append(acc)
    if acc > best_acc:
        best_acc = acc
        best_clf = clf
acc_scores
```

```{python}
np.array(acc_scores).mean()
```

```{python}
print(classification_report(y_valid, y_pred, target_names=['high', 'medium','low']))
```

```{python}
# Check overfitting
y_pred = clf.predict(X_test)
acc = accuracy_score(y_test, y_pred)
acc
```

## Eighth Version

All numeric features and the features were used for this classifier. Gini index seemed to do slightly better so it was used to determine the best split for each node. A stratified k-fold cross validation method was used. Minimum support was changed to 100 and the classifier predicted with average 69.55% accuracy, which is a large improvement. Recall the low interest class was at an all time high with 1.00. The classifier doesn't seem to be predicting the medium or high interest classes corectly at all. There seems to be no overfitting since the performance on the test dataset is only off by 0.01%.

```{python}
# perform cross-validation for first set of classifiers
kf = StratifiedKFold(n_splits=5)
acc_scores = []
for train_index, valid_index in kf.split(X2, y_tr):
    # split data
    X_train, X_valid = X2.iloc[train_index], X2.iloc[valid_index]
    y_train, y_valid = y_tr.iloc[train_index], y_tr.iloc[valid_index]
    # Create classifier object
    clf = DecisionTreeClassifier(criterion="gini", random_state=0,min_samples_leaf=100)
    # Train Decision Tree Classifer
    clf = clf.fit(np.array(X_train),np.array(y_train),check_input=False)
    #Predict the response for test dataset
    y_pred = clf.predict(X_valid)
    acc = accuracy_score(y_valid, y_pred)
    acc_scores.append(acc)
    if acc > best_acc:
        best_acc = acc
        best_clf = clf
acc_scores
```

```{python}
np.array(acc_scores).mean()
```

```{python}
print(classification_report(y_valid, y_pred, target_names=['high', 'medium','low']))
```

```{python}
# Check overfitting
y_pred = clf.predict(X_test)
acc = accuracy_score(y_test, y_pred)
acc
```

## Ninth Version

Here we're trying a Random Forest to try and combat inconsistent data creating very different trees. Playing with the parameters, number of folds, and features also gets the same results, on average, as before. Although we do get one classifier that performs the best with an accuracy of 70.81%. This classifier will be used for testing.

```{python}
# perform cross-validation for first set of classifiers
kf = StratifiedKFold(n_splits=10)
acc_scores = []
for train_index, valid_index in kf.split(X2, y_tr):
    # split data
    X_train, X_valid = X2.iloc[train_index], X2.iloc[valid_index]
    y_train, y_valid = y_tr.iloc[train_index], y_tr.iloc[valid_index]
    # Create classifier object
    clf = RandomForestClassifier(criterion="gini", random_state=0,min_samples_leaf=20,n_estimators=50,min_samples_split=100)
    # Train Decision Tree Classifer
    clf = clf.fit(np.array(X_train),np.array(y_train))
    #Predict the response for test dataset
    y_pred = clf.predict(X_valid)
    acc = accuracy_score(y_valid, y_pred)
    acc_scores.append(acc)
    if acc > best_acc:
        best_clf = clf
acc_scores
```

```{python}
np.array(acc_scores).mean()
```

```{python}
print(classification_report(y_valid, y_pred, target_names=['high', 'medium','low']))
```

```{python}
# Check overfitting
y_pred = clf.predict(X_test)
acc = accuracy_score(y_test, y_pred)
acc
```

## Tenth Version

Here we're trying a Random Forest to try and combat inconsistent data creating very different trees. Playing with the parameters, number of folds, and features also gets the same results, on average, as before. Although we do get one classifier that performs the best with an accuracy of 70.81%. This classifier will be used for testing.

```{python}
# perform cross-validation for first set of classifiers
kf = StratifiedKFold(n_splits=10)
acc_scores = []
for train_index, valid_index in kf.split(X2, y_tr):
    # split data
    X_train, X_valid = X2.iloc[train_index], X2.iloc[valid_index]
    y_train, y_valid = y_tr.iloc[train_index], y_tr.iloc[valid_index]
    # Create classifier object
    clf = RandomForestClassifier(criterion="gini", random_state=0,min_samples_leaf=20,n_estimators=50,min_samples_split=100)
    # Train Decision Tree Classifer
    clf = clf.fit(np.array(X_train[numeric]),np.array(y_train))
    #Predict the response for test dataset
    y_pred = clf.predict(X_valid[numeric])
    acc = accuracy_score(y_valid, y_pred)
    acc_scores.append(acc)
    if acc > best_acc:
        best_clf = clf
acc_scores
```

```{python}
np.array(acc_scores).mean()
```

```{python}
print(classification_report(y_valid, y_pred, target_names=['high', 'medium','low']))
```

```{python}
# Check overfitting
y_pred = clf.predict(X_test[numeric])
acc = accuracy_score(y_test, y_pred)
acc
```

# 4. Predictions on  test dataset

Score (multiclass log-loss) = 0.79164 using a Random Forest classifier, the eighth version of classifiers. The benchmark of sample submission is 0.78598 so our classifier isn’t that far off. 

- Decision-tree learners can create over-complex trees that do not generalise the data well. This is called overfitting. Mechanisms such as pruning (not currently supported), setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.

- Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.

## Add hot encoded text feature to test

```{python}
# convert features list to string
test['features'] = test['features'].apply(lambda x: ' '.join(x))
```

```{python}
# extract features using tfidf
vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', token_pattern=r'^[a-zA-Z][a-zA-Z]+',max_features=79)
vectorizer.fit(test['features'].values);
test['feat_vect'] = test['features'].apply(lambda x: vectorizer.transform([x]))
```

```{python}
# convert features vectors to list
feat = test['feat_vect'].apply(lambda x: x.toarray().tolist())
feat_list = []
for i in feat:
    feat_list.append(i)
```

```{python}
# create dataframe of hot encoded feature vector
test2 = pd.DataFrame(np.array(feat_list).reshape(np.array(feat_list).shape[0],np.array(feat_list).shape[2]))
```

```{python}
# join hot encoded features to test dataset
test = test.join(test2)
```

```{python}
test.drop(['feat_vect'], axis=1, inplace=True)
```

```{python}
# use best classifier on test dataset
y_pred = best_clf.predict_proba(test.drop(['listing_id'],axis=1))
```

```{python}
# Test output for log loss
test_prob = np.concatenate((listing_id,y_pred),axis=1)
np.savetxt('test_best.csv',test_prob, delimiter=',',header='listing_id,high,medium,low')
```
