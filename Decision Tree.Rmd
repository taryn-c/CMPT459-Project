---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.3.3
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{python}
from Preprocessing import *;
from sklearn.model_selection import StratifiedKFold,KFold,train_test_split;
from sklearn.preprocessing import LabelEncoder, OneHotEncoder;
from sklearn.tree import DecisionTreeClassifier;
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report;
from sklearn.feature_extraction.text import TfidfVectorizer
#from skfeature.function.similarity_based import fisher_score;
#from skfeature.function.statistical_based import gini_index;
import seaborn as sns
plt.style.use('ggplot')
```

```{python}
data = train_df.reset_index(drop=True)
test = pd.read_json("test.json")
```

# 1. Feature Selection

Since the building_id, listing_id, and manager_id are neither categorical or numeric data, it really isn't relevant for our model. We will remove these attributes. We will also remove the photos column because we won't be using this in our model either. We will also remove street_address because we can use the display_address as a categorical value since this is just the genral street that the listing is on. This helps identify the general neighbourhood the listing is in for the buyer. The description, created, and features columns are removed because these are represented by newly extracted feature columns from the previous milestone.

```{python}
data.drop(['photos','listing_id','building_id','manager_id','street_address','features','description','created'], axis=1, inplace=True)
```

## Label encode

```{python}
data["interest_level"] = data["interest_level"].astype('category')
data["target"] = data["interest_level"].cat.codes
data.drop(['interest_level'], axis=1, inplace=True)
```

```{python}
data["address"] = data["display_address"].astype('category')
data["address"] = data["address"].cat.codes
data.drop(['display_address'], axis=1, inplace=True)
```

## Split our data

I will split our input and target variables into X and y respectively. I do this before feature selection to reduce bias by keeping the test data untouched. In this way, the test data doesn't affect our selection methods. Next, we will check the distribution of our target variable to see if we must stratify our training and testing data samples.

```{python}
X = data.drop(['target'], axis=1)
y = data['target']
```

```{python}
int_level = train_df['interest_level'].value_counts()
plt.figure(figsize=(15,6))
sns.barplot(int_level.index, int_level.values, alpha=1, order=['low','medium','high'],color=color[0])
plt.ylabel('Number of Occurrences', fontsize=14, fontweight='bold')
plt.xlabel('Interest level', fontsize=14, fontweight='bold')
plt.title('Target distribution', fontsize=16, fontweight='bold')
plt.show()
```

Since the distribution is skewed, we should stratify our data when spliting so it represents our data well.

```{python}
X_train, X_test, y_train, y_test = train_test_split(X,y,stratify=y)
X_train.reset_index(inplace=True);
X_test.reset_index(inplace=True);
y_train.reset_index();
y_test.reset_index();
```

```{python}
kf = KFold(n_splits=5)
```

## Fisher score

Since this is only valid for numerica features, we must first take a subset of our data. We will separate the data based on interest level to calculate the mean and standard deviation of each class. These values will be used to calculate the fisher score for each feature. We can then use a filtering technique to find the most relevant features based on the scores. 

```{python}
# take subset
numeric = ['bathrooms','bedrooms','latitude','longitude','price','hour_created','address']
num_data = data[['bathrooms','bedrooms','latitude','longitude','price','hour_created','address','target']]
```

```{python}
# separate data by interest level
high = num_data[num_data['target'] == 2].drop(['target'], axis=1)
med = num_data[num_data['target'] == 1].drop(['target'], axis=1)
low = num_data[num_data['target'] == 0].drop(['target'], axis=1)
```

```{python}
# calculate mean of each feature in each class
avg_if = np.array([high.mean(),
      med.mean(),
      low.mean()])
#calculate mean of each feature
avg_f = np.array(num_data.drop(['target'], axis=1).mean())
```

```{python}
# calculate variance for each feature
var = np.square(np.array([high.std(),
      med.std(),
      low.std()]))

# calculate probability of each class
prob = np.array([high.shape[0]/float(data.shape[0]),
             med.shape[0]/float(data.shape[0]),
             low.shape[0]/float(data.shape[0])]
)
```

```{python}
fisher = []
numerator = []
denomerator = []

for feat in np.arange(0,avg_if.shape[1]):
    for class_i in np.arange(0,avg_if.shape[0]):
        numerator.append(np.array(prob[class_i]*np.square(avg_if[class_i][feat]-avg_f[feat])))
        denomerator.append(np.array(prob[class_i]*var[class_i]))      
    fisher.append(np.array(numerator).sum()/np.array(denomerator).sum())
fisher
```

From the fisher scores, it looks like the address, price, and hour the listing was created have the best fisher score, in that order. The worst was longitude and the rest weren't that far off either with 10 significant digits. 


# 2. Train Classifiers


## First Version

The classifier doesn't accept the extracted text features from the previous milestone so for the first classifier they were removed and the classifier was trained for the numerical data only. The 3 features with the highest fisher scores are selected for the first classifier. These are price, hour created, and bedrooms. The default Gini index was used to determine the best split for each node. The classifier predicted with average 62.52% accuracy.

```{python}
# perform cross-validation for first set of classifiers
kf = KFold(n_splits=5)
acc_scores = []
top3 = ['price', 'hour_created', 'address']
for train_index, valid_index in kf.split(X):
    # split data
    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
    y_train, y_valid = y[train_index], y[valid_index]
    # Create classifier object
    clf = DecisionTreeClassifier(criterion="gini", random_state=0)
    # Train Decision Tree Classifer
    clf = clf.fit(np.array(X_train[top3]),np.array(y_train),check_input=False)
    #Predict the response for test dataset
    y_pred = clf.predict(X_valid[top3])
    acc_scores.append(accuracy_score(y_valid, y_pred))
acc_scores
```

```{python}
np.array(acc_scores).mean()
```

## Second Version

All numeric features were used for this classifier. Gini index was used to determine the best split for each node. The classifier predicted with average 64.92% accuracy. It looks like the classifier has slightly better accuracy with all features.

```{python}
# perform cross-validation for first set of classifiers
acc_scores = []
for train_index, valid_index in kf.split(X_train):
    # split data
    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
    y_train, y_valid = y[train_index], y[valid_index]
    # Create classifier object
    clf = DecisionTreeClassifier(criterion="gini", random_state=0)
    # Train Decision Tree Classifer
    clf = clf.fit(np.array(X_train[numeric]),np.array(y_train),check_input=False)
    #Predict the response for test dataset
    y_pred = clf.predict(X_valid[numeric])
    acc_scores.append(accuracy_score(y_valid, y_pred))
acc_scores
```

```{python}
np.array(acc_scores).mean()
```

## Third Version

All numeric features were used for this classifier. Gini index was used to determine the best split for each node. A stratified k-fold cross validation method was used. The classifier predicted with average 65.24% accuracy, which is ever so slightly better than regular k-fold cross validation.

```{python}
# perform cross-validation for first set of classifiers
kf = StratifiedKFold(n_splits=5)
acc_scores = []
for train_index, valid_index in kf.split(X, y):
    # split data
    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
    y_train, y_valid = y[train_index], y[valid_index]
    # Create classifier object
    clf = DecisionTreeClassifier(criterion="gini", random_state=0)
    # Train Decision Tree Classifer
    clf = clf.fit(np.array(X_train[numeric]),np.array(y_train),check_input=False)
    #Predict the response for test dataset
    y_pred = clf.predict(X_valid[numeric])
    acc_scores.append(accuracy_score(y_valid, y_pred))
acc_scores
```

```{python}
np.array(acc_scores).mean()
```

## Fourth

All numeric features were used for this classifier. Entropy was used to determine the best split for each node. A stratified k-fold cross validation method was used. The classifier predicted with average 65.58% accuracy, which is ever so slightly worse than regular k-fold cross validation.

```{python}
# perform cross-validation for first set of classifiers
kf = StratifiedKFold(n_splits=5)
acc_scores = []
for train_index, valid_index in kf.split(X, y):
    # split data
    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
    y_train, y_valid = y[train_index], y[valid_index]
    # Create classifier object
    clf = DecisionTreeClassifier(criterion="entropy", random_state=0)
    # Train Decision Tree Classifer
    clf = clf.fit(np.array(X_train[numeric]),np.array(y_train),check_input=False)
    #Predict the response for test dataset
    y_pred = clf.predict(X_valid[numeric])
    acc_scores.append(accuracy_score(y_valid, y_pred))
acc_scores
```

```{python}
np.array(acc_scores).mean()
```

## Fifth Version

All numeric features were used for this classifier. Gini was used to determine the best split for each node. A stratified k-fold cross validation method was used. Minimum support was changed to 3 to combat overfitting. The classifier predicted with average 66.11% accuracy, which is ever so slightly worse than regular k-fold cross validation.

```{python}
# perform cross-validation for first set of classifiers
kf = StratifiedKFold(n_splits=5)
acc_scores = []
for train_index, valid_index in kf.split(X, y):
    # split data
    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
    y_train, y_valid = y[train_index], y[valid_index]
    # Create classifier object
    clf = DecisionTreeClassifier(criterion="entropy", random_state=0,min_samples_leaf=3)
    # Train Decision Tree Classifer
    clf = clf.fit(np.array(X_train[numeric]),np.array(y_train),check_input=False)
    #Predict the response for test dataset
    y_pred = clf.predict(X_valid[numeric])
    acc_scores.append(accuracy_score(y_valid, y_pred))
acc_scores
```

```{python}
np.array(acc_scores).mean()
```

## Hot encode features vector

The Decision Tree classifier in the sklearn sorts each feature to split the data and doesn't support features that are vectors. So to use any text features we must hot encode them. However, the description vectors are 1119 long so we will only do the features. Looking at the two text features, it seems like description might not be that helpful because there are a lot of irrelevant words/strings in most of them like 'XXX'.  

```{python}
feat = data['feat_vect'].apply(lambda x: x.toarray().tolist())
feat_list = []
for i in feat:
    feat_list.append(i)
```

```{python}
X2 = pd.DataFrame(np.array(feat_list).reshape(np.array(feat_list).shape[0],np.array(feat_list).shape[2]))
X2 = X[numeric].join(X2)
```

## Process test input

```{python}
test = pd.read_json("test.json")
test["created"] = pd.to_datetime(test["created"])
# add hour created column
test["hour_created"] = test["created"].dt.hour
```

## Label encode

```{python}
test["address"] = test["display_address"].astype('category')
test["address"] = test["address"].cat.codes
```

```{python}
# convert features list to string
test['features'] = test['features'].apply(lambda x: ' '.join(x))
```

```{python}
# extract features using tfidf
vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', token_pattern=r'^[a-zA-Z][a-zA-Z]+',max_features=79)
vectorizer.fit(test['features'].values);
test['feat_vect'] = test['features'].apply(lambda x: vectorizer.transform([x]))
```

```{python}
feat = test['feat_vect'].apply(lambda x: x.toarray().tolist())
feat_list = []
for i in feat:
    feat_list.append(i)
```

```{python}
#X2 = X[numeric].join(pd.DataFrame(np.array(feat_list).reshape(49197,79)))
test2 = pd.DataFrame(np.array(feat_list).reshape(np.array(feat_list).shape[0],np.array(feat_list).shape[2]))
```

```{python}
test = test.join(test2)
```

```{python}
test.drop(['photos','listing_id','building_id','manager_id','street_address','features','description','created','display_address','feat_vect'], axis=1, inplace=True)
test
```

## Sixth Version

All numeric features were used for this classifier. Gini was used to determine the best split for each node. A stratified k-fold cross validation method was used. Minimum support was changed to 3 to combat overfitting. The classifier predicted with average 66.11% accuracy, which is ever so slightly worse than regular k-fold cross validation.

```{python}
X2
```

```{python}
clf = DecisionTreeClassifier(criterion="entropy", random_state=0,min_samples_leaf=3)
clf = clf.fit(np.array(X2),np.array(y),check_input=False)
```

```{python}
# perform cross-validation for first set of classifiers
kf = StratifiedKFold(n_splits=5)
acc_scores = []
for train_index, valid_index in kf.split(X2, y):
    # split data
    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
    y_train, y_valid = y[train_index], y[valid_index]
    print(X_train)
    # Create classifier object
    clf = DecisionTreeClassifier(criterion="entropy", random_state=0,min_samples_leaf=3)
    # Train Decision Tree Classifer
    clf = clf.fit(np.array(X_train),np.array(y_train),check_input=False)
    #Predict the response for test dataset
    y_pred = clf.predict(X_valid)
    acc_scores.append(accuracy_score(y_valid, y_pred))
acc_scores
```

```{python}
np.array(acc_scores).mean()
```

## Confusion Matrix

```{python}
conf_matrix = confusion_matrix(y_valid, y_pred)
conf_matrix
```

```{python}
print(classification_report(y_valid, y_pred, target_names=['low', 'medium','high']))
```

## Multi-class log loss output

```{python}
# Test output for log loss
y_prob = clf.predict_proba(X_test[numeric])
```

```{python}

```
