{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Gradient Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocessing import *;\n",
    "from sklearn.model_selection import StratifiedKFold,KFold,train_test_split,cross_val_score;\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder;\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC;\n",
    "from sklearn.metrics import log_loss, confusion_matrix, classification_report;\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer;\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from xgboost import XGBClassifier\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = train_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json(\"test.json\")\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert column to datetime\n",
    "test[\"created\"] = pd.to_datetime(test[\"created\"])\n",
    "# add hour created column\n",
    "test[\"hour_created\"] = test[\"created\"].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode\n",
    "test[\"address\"] = test[\"display_address\"].astype('category')\n",
    "test[\"address\"] = test[\"address\"].cat.codes\n",
    "test[\"manager_id\"] = test[\"manager_id\"].astype('category')\n",
    "test[\"manager\"] = test[\"manager_id\"].cat.codes\n",
    "test.drop(['manager_id'], axis=1, inplace=True)\n",
    "test[\"building_id\"] = test[\"building_id\"].astype('category')\n",
    "test[\"building\"] = test[\"building_id\"].cat.codes\n",
    "test.drop(['building_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert features list to string\n",
    "test['features'] = test['features'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', token_pattern=r'^[a-zA-Z][a-zA-Z][a-zA-Z]+', max_features=79)\n",
    "vectorizer.fit(test['features'].values);\n",
    "test['feat_vect'] = test['features'].apply(lambda x: vectorizer.transform([x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', token_pattern=r'^[a-zA-Z][a-zA-Z][a-zA-Z]+', max_features=1119)\n",
    "vectorizer.fit(test['description'].values);\n",
    "test['desc_vect'] = test['description'].apply(lambda x: vectorizer.transform([x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.drop(['photos','street_address','features','description','created','display_address'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature Selection\n",
    "\n",
    "Since the listing_id is unique to each lising, it really isn't relevant for our model and will be removed. We will also remove the photos column because we won't be using this in our model either. We will also remove street_address because we can use the display_address as a categorical value since this is just the general street that the listing is on. This helps identify the general neighbourhood the listing is in for the buyer. The description, created, and features columns are removed because these are represented by newly extracted feature columns from the previous milestone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['photos','listing_id','street_address','features','description','created'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarize data by label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalDtype(categories=[u'high', u'medium', u'low'], ordered=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_type = CategoricalDtype(categories=[\"high\", \"medium\", \"low\"],ordered=True)\n",
    "cat_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high-0, medium-1, low-2\n",
    "data[\"interest_level\"] = data[\"interest_level\"].astype(cat_type)\n",
    "data[\"target\"] = data[\"interest_level\"].cat.codes\n",
    "data.drop(['interest_level'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"address\"] = data[\"display_address\"].astype('category')\n",
    "data[\"address\"] = data[\"address\"].cat.codes\n",
    "data.drop(['display_address'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"building_id\"] = data[\"building_id\"].astype('category')\n",
    "data[\"building\"] = data[\"building_id\"].cat.codes\n",
    "data.drop(['building_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"manager_id\"] = data[\"manager_id\"].astype('category')\n",
    "data[\"manager\"] = data[\"manager_id\"].cat.codes\n",
    "data.drop(['manager_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data\n",
    "\n",
    "I will split our input and target variables into X and y respectively. We will also split into training and test data so we can see if our models are overfitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['target'], axis=1)\n",
    "y = data['target']\n",
    "X_tr, X_test, y_tr, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.reset_index(inplace=True,drop=True)\n",
    "y.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher score\n",
    "\n",
    "Since this is only valid for numerica features, we must first take a subset of our data. We will separate the data based on interest level to calculate the mean and standard deviation of each class. These values will be used to calculate the fisher score for each feature. We can then use a filtering technique to find the most relevant features based on the scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take subset\n",
    "numeric = ['bathrooms','bedrooms','latitude','longitude','price','hour_created','address','manager','building']\n",
    "all_feats = ['bathrooms','bedrooms','latitude','longitude','price','hour_created','address','manager','building','desc_vect','feat_vect']\n",
    "num_data = data[['bathrooms','bedrooms','latitude','longitude','price','hour_created','address','manager','building','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# separate data by interest level\n",
    "high = num_data[num_data['target'] == 2].drop(['target'], axis=1)\n",
    "med = num_data[num_data['target'] == 1].drop(['target'], axis=1)\n",
    "low = num_data[num_data['target'] == 0].drop(['target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean of each feature in each class\n",
    "avg_if = np.array([high.mean(),\n",
    "      med.mean(),\n",
    "      low.mean()])\n",
    "#calculate mean of each feature\n",
    "avg_f = np.array(num_data.drop(['target'], axis=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate variance for each feature\n",
    "var = np.square(np.array([high.std(),\n",
    "      med.std(),\n",
    "      low.std()]))\n",
    "\n",
    "# calculate probability of each class\n",
    "prob = np.array([high.shape[0]/float(data.shape[0]),\n",
    "             med.shape[0]/float(data.shape[0]),\n",
    "             low.shape[0]/float(data.shape[0])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.4391986856950884e-10,\n",
       " 1.5495308224893388e-10,\n",
       " 1.0336764419360136e-10,\n",
       " 7.75380886324851e-11,\n",
       " 0.0008348517025325086,\n",
       " 0.000695716318490876,\n",
       " 0.0011911021471335808,\n",
       " 0.0010447074295786081,\n",
       " 0.0019208781511415576]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fisher = []\n",
    "numerator = []\n",
    "denomerator = []\n",
    "\n",
    "for feat in np.arange(0,avg_if.shape[1]):\n",
    "    for class_i in np.arange(0,avg_if.shape[0]):\n",
    "        numerator.append(np.array(prob[class_i]*np.square(avg_if[class_i][feat]-avg_f[feat])))\n",
    "        denomerator.append(np.array(prob[class_i]*var[class_i]))      \n",
    "    fisher.append(np.array(numerator).sum()/np.array(denomerator).sum())\n",
    "fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3 = ['building', 'manager', 'address']\n",
    "top5 = ['building', 'manager', 'address','price', 'hour_created']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Fisher scores, it looks like the building, address, and manager have the top 3 fisher scores, in that order.  Price and hour the listing was created also have decent fisher scores. The worst was longitude and the rest weren't that far off either with 10 significant digits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train Classifiers\n",
    "\n",
    "Used as a guideline for improvements: https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/\n",
    "\n",
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set default hyperparameters\n",
    "N_EST = 100\n",
    "DEPTH = 3\n",
    "RATE = 0.1\n",
    "MIN_SAMP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "global listing_id;\n",
    "listing_id = test['listing_id'].values\n",
    "listing_id = listing_id.reshape(listing_id.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(kf,train_df,test_df,n_est,depth,min_leaf,rate):\n",
    "    model_gbc = GBC(n_estimators=n_est, random_state=0,max_depth=depth,min_samples_leaf=min_leaf, learning_rate=rate)\n",
    "    model_gbc = model_gbc.fit(np.array(train_df),np.array(y_tr))\n",
    "    y_prob = model_gbc.predict_proba(test_df)\n",
    "    y_pred = model_gbc.predict(test_df)\n",
    "    loss = log_loss(y_test, y_prob)\n",
    "    #acc = accuracy_score(y_test, y_pred)\n",
    "    results_gbc = cross_val_score(model_gbc, train_df, y_tr, cv=kf, scoring='neg_log_loss')\n",
    "    print_results(results_gbc,loss)\n",
    "    return y_prob, y_pred, model_gbc\n",
    "\n",
    "def print_results(results_gbc, loss):\n",
    "    print('Cross validation results:\\n'\n",
    "          'Negative log loss of folds: {0}\\n'\n",
    "          'Mean log loss of all folds: {1} (+/-{2})\\n'\n",
    "          'Test log loss: {3}'.format(results_gbc,results_gbc.mean(),results_gbc.std(),loss))\n",
    "\n",
    "def testing(model_gbc, test_df,version):\n",
    "    # use best classifier on test dataset\n",
    "    y_pred = model_gbc.predict_proba(test_df)\n",
    "    # Test output for log loss\n",
    "    test_prob = np.concatenate((listing_id,y_pred),axis=1)\n",
    "    np.savetxt('test_v{}.csv'.format(version),test_prob, delimiter=',',header='listing_id,high,medium,low',encoding='ascii')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Version\n",
    "\n",
    "The first version used all numeric features. Regular cross validation with 5 folds was used. Number of estimators was the default 100. Kaggle score: 0.65793. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation results:\n",
      "Negative log loss of folds: [-0.61973552 -0.6375119  -0.64126953 -0.62879089 -0.64435558]\n",
      "Mean log loss of all folds: -0.634332683827 (+/-0.00897189450249)\n",
      "Test log loss: 0.638684922517\n"
     ]
    }
   ],
   "source": [
    "# second time\n",
    "kf = KFold(n_splits=5, random_state=0)\n",
    "(y_prob, y_pred, model_gbc) = train(kf,X_tr[numeric],X_test[numeric],100,DEPTH,MIN_SAMP,RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.62      0.18      0.28       965\n",
      "      medium       0.46      0.17      0.25      2836\n",
      "         low       0.74      0.96      0.84      8499\n",
      "\n",
      "   micro avg       0.72      0.72      0.72     12300\n",
      "   macro avg       0.61      0.44      0.46     12300\n",
      "weighted avg       0.67      0.72      0.66     12300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(model_gbc,test[numeric],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Version\n",
    "\n",
    "Top 5 features according to Fisher scores were used for this classifier. This version did worse than the first in both training and test data. Precesion for all classes was also lower. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation results:\n",
      "Negative log loss of folds: [-0.66355568 -0.67614683 -0.68068219 -0.66943808 -0.68121573]\n",
      "Mean log loss of all folds: -0.674207701171 (+/-0.00679715445783)\n",
      "Test log loss: 0.681156105594\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, random_state=0)\n",
    "(y_prob, y_pred, model_gbc) = train(kf,X_tr[top5],X_test[top5],100,DEPTH,MIN_SAMP,RATE)\n",
    "#(y_prob, y_pred, model_gbc) = train(kf,100,top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.54      0.15      0.24       965\n",
      "      medium       0.44      0.11      0.18      2836\n",
      "         low       0.73      0.97      0.83      8499\n",
      "\n",
      "   micro avg       0.71      0.71      0.71     12300\n",
      "   macro avg       0.57      0.41      0.41     12300\n",
      "weighted avg       0.65      0.71      0.63     12300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(model_gbc,test[top5],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Version\n",
    "\n",
    "All numeric features were used for this classifier. 10 folds were used. Testing log loss (0.71) was actually higher than validation (mean -0.634304785285 (+/-0.0115601003865))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation results:\n",
      "Negative log loss of folds: [-0.65146836 -0.62566497 -0.60734555 -0.63426111 -0.62017785 -0.64714128\n",
      " -0.64268538 -0.64221236 -0.63948399 -0.64690496]\n",
      "Mean log loss of all folds: -0.635734581048 (+/-0.013260062578)\n",
      "Test log loss: 0.627822368209\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10, random_state=0) #random state was 10 for output\n",
    "(y_prob, y_pred, model_gbc) = train(kf,X_tr[numeric],X_test[numeric],100,DEPTH,MIN_SAMP,RATE)\n",
    "#(y_prob, y_pred, model_gbc) = train(kf,100,numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.61      0.17      0.27       743\n",
      "      medium       0.45      0.17      0.25      2246\n",
      "         low       0.75      0.96      0.84      6851\n",
      "\n",
      "   micro avg       0.72      0.72      0.72      9840\n",
      "   macro avg       0.60      0.43      0.45      9840\n",
      "weighted avg       0.67      0.72      0.66      9840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(model_gbc,test[numeric],3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth Version\n",
    "\n",
    "We can check the distribution of the target to see if stratifying the samples will help us identify any under-represented classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "int_level = train_df['interest_level'].value_counts()\n",
    "plt.figure(figsize=(15,6))\n",
    "sns.barplot(int_level.index, int_level.values, alpha=1, order=['low','medium','high'],color=color[0])\n",
    "plt.ylabel('Number of Occurrences', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Interest level', fontsize=14, fontweight='bold')\n",
    "plt.title('Target distribution', fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6947578104355957, 0.2276358314531374, 0.07760635811126694)\n"
     ]
    }
   ],
   "source": [
    "int_all = int_level.sum()\n",
    "prob_h = int_level[0]/float(int_all)\n",
    "prob_m = int_level[1]/float(int_all)\n",
    "prob_l = int_level[2]/float(int_all)\n",
    "print(prob_h, prob_m, prob_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the distribution of our target is skewed, we should stratify our data when spliting so it represents our data well. All numeric features were used for this classifier. A stratified k-fold cross validation method was used. The classifier predicted with average 71.59, which is the same as regular k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation results:\n",
      "Negative log loss of folds: [-0.6259658  -0.64972545 -0.63965819 -0.63920031 -0.63240227 -0.63434838\n",
      " -0.63813548 -0.63101538 -0.65129083 -0.6428751 ]\n",
      "Mean log loss of all folds: -0.638461718276 (+/-0.00760428659045)\n",
      "Test log loss: 0.624446851318\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=10, random_state=0)\n",
    "(y_prob, y_pred, model_gbc) = train(kf,X_tr[numeric],X_test[numeric],100,DEPTH,MIN_SAMP,RATE)\n",
    "#(y_prob, y_pred, model_gbc) = train(kf,100,numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.60      0.16      0.26       945\n",
      "      medium       0.48      0.19      0.27      2788\n",
      "         low       0.75      0.96      0.85      8567\n",
      "\n",
      "   micro avg       0.73      0.73      0.73     12300\n",
      "   macro avg       0.61      0.44      0.46     12300\n",
      "weighted avg       0.68      0.73      0.67     12300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(model_gbc,test[numeric],4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fifth Version\n",
    "\n",
    "All numeric features were used for this classifier. Number of estimators was increased to 150. A stratified k-fold cross validation method was used. The classifier predicted with average 63.40% validation log loss. OVerfitting is occuring because the test log loss is slightly better at 61.54%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation results:\n",
      "Negative log loss of folds: [-0.61822483 -0.64234056 -0.63176158 -0.63191878 -0.62326404 -0.62596735\n",
      " -0.63093361 -0.62147791 -0.64252866 -0.63553867]\n",
      "Mean log loss of all folds: -0.630395597817 (+/-0.00786728263729)\n",
      "Test log loss: 0.61539739534\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=10, random_state=0)\n",
    "(y_prob, y_pred, model_gbc) = train(kf,X_tr[numeric],X_test[numeric],150,DEPTH,MIN_SAMP,RATE)\n",
    "#(y_prob, y_pred, model_gbc) = train(kf,150,numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.59      0.17      0.26       945\n",
      "      medium       0.48      0.22      0.30      2788\n",
      "         low       0.76      0.96      0.85      8567\n",
      "\n",
      "   micro avg       0.73      0.73      0.73     12300\n",
      "   macro avg       0.61      0.45      0.47     12300\n",
      "weighted avg       0.69      0.73      0.68     12300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(model_gbc,test[numeric],5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sixth Version\n",
    "\n",
    "All numeric features were used for this classifier. Number of estimators was increased to 200. A stratified k-fold cross validation method was used. The classifier predicted with average 62.63% log loss on validation. Since boosting selects the best features for the next stage, its better to start with more features. Overfitting is occurring since our test log loss is higher than validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation results:\n",
      "Negative log loss of folds: [-0.63722464 -0.61450809 -0.61384699 -0.63064995 -0.62008363 -0.63869145\n",
      " -0.6267715  -0.63231959 -0.63207107 -0.62879994]\n",
      "Mean log loss of all folds: -0.627496683717 (+/-0.00828420006427)\n",
      "Test log loss: 0.618917899605\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=10, random_state=0)\n",
    "(y_prob, y_pred, model_gbc) = train(kf,X_tr[numeric],X_test[numeric],150,DEPTH,MIN_SAMP,RATE)\n",
    "#(y_prob, y_pred, model_gbc) = train(kf,200,numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.60      0.18      0.28       743\n",
      "      medium       0.46      0.20      0.28      2246\n",
      "         low       0.76      0.95      0.84      6851\n",
      "\n",
      "   micro avg       0.72      0.72      0.72      9840\n",
      "   macro avg       0.60      0.44      0.47      9840\n",
      "weighted avg       0.68      0.72      0.67      9840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(model_gbc,test[numeric],6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seventh Version\n",
    "\n",
    "Minimum samples per leaf = 20, 200 estimators, numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation results:\n",
      "Negative log loss of folds: [-0.60699157 -0.61433355 -0.62227766 -0.62186212 -0.62982236 -0.62762845\n",
      " -0.62170407 -0.62631502 -0.61222444 -0.61219917]\n",
      "Mean log loss of all folds: -0.619535840986 (+/-0.00725669600167)\n",
      "Test log loss: 0.619775995079\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=10, random_state=0)\n",
    "(y_prob, y_pred, model_gbc) = train(kf,X_tr[numeric],X_test[numeric],200,DEPTH,20,RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.60      0.21      0.31       965\n",
      "      medium       0.48      0.22      0.30      2836\n",
      "         low       0.76      0.95      0.84      8499\n",
      "\n",
      "   micro avg       0.72      0.72      0.72     12300\n",
      "   macro avg       0.61      0.46      0.48     12300\n",
      "weighted avg       0.68      0.72      0.68     12300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(model_gbc,test[numeric],7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigth Version\n",
    "\n",
    "K fold with 5 folds, 50 estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation results:\n",
      "Negative log loss of folds: [-0.64328498 -0.65939972 -0.65768837 -0.65962024 -0.63671908]\n",
      "Mean log loss of all folds: -0.651342478734 (+/-0.00951291501842)\n",
      "Test log loss: 0.619775995079\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, random_state=0)\n",
    "(y_prob, y_pred, model_gbc) = train(kf,X_tr[numeric],X_test[numeric],50,DEPTH,MIN_SAMP,RATE)\n",
    "#(y_prob, y_pred, model_gbc) = train(kf,50,numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.60      0.17      0.26       965\n",
      "      medium       0.44      0.12      0.19      2836\n",
      "         low       0.73      0.97      0.83      8499\n",
      "\n",
      "   micro avg       0.71      0.71      0.71     12300\n",
      "   macro avg       0.59      0.42      0.43     12300\n",
      "weighted avg       0.65      0.71      0.64     12300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(model_gbc,test[numeric],8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ninth Version\n",
    "\n",
    "Minimum samples per leaf = 20, 250 estimators, numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation results:\n",
      "Negative log loss of folds: [-0.61137811 -0.60568171 -0.61866023 -0.62081814 -0.62183886 -0.61919955\n",
      " -0.6100559  -0.61013176 -0.62320911 -0.61524715]\n",
      "Mean log loss of all folds: -0.615622051328 (+/-0.00568619178616)\n",
      "Test log loss: 0.617675695196\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=10, random_state=0)\n",
    "(y_prob, y_pred, model_gbc) = train(kf,X_tr[numeric],X_test[numeric],250,DEPTH,20,RATE)\n",
    "# model_gbc = GBC(n_estimators=250, random_state=0,min_samples_leaf=20)\n",
    "# model_gbc = model_gbc.fit(np.array(X_tr[numeric]),np.array(y_tr))\n",
    "# y_pred = model_gbc.predict(X_test[numeric])\n",
    "# y_prob = model_gbc.predict_proba(X_test[numeric])\n",
    "# loss = log_loss(y_test, y_prob)\n",
    "# results_gbc = cross_val_score(model_gbc, X_tr[numeric], y_tr, cv=kf, scoring='neg_log_loss')\n",
    "# print_results(results_gbc,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.63      0.20      0.31      1032\n",
      "      medium       0.45      0.25      0.32      2770\n",
      "         low       0.77      0.95      0.85      8498\n",
      "\n",
      "   micro avg       0.73      0.73      0.73     12300\n",
      "   macro avg       0.62      0.46      0.49     12300\n",
      "weighted avg       0.69      0.73      0.68     12300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(model_gbc,test[numeric],9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tenth Version\n",
    "\n",
    "Minimum samples per leaf = 40, 250 estimators, numeric features. Kaggle score 0.64874."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation results:\n",
      "Negative log loss of folds: [-0.6091785  -0.60662281 -0.61918619 -0.6201973  -0.62173282 -0.61900322\n",
      " -0.60964536 -0.60848237 -0.6220913  -0.61536997]\n",
      "Mean log loss of all folds: -0.615150983717 (+/-0.00575592699681)\n",
      "Test log loss: 0.617644632047\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=10, random_state=0)\n",
    "(y_prob, y_pred, model_gbc) = train(kf,X_tr[numeric],X_test[numeric],150,DEPTH,MIN_SAMP,RATE)\n",
    "# model_gbc = GBC(n_estimators=250, random_state=0,min_samples_leaf=40)\n",
    "# model_gbc = model_gbc.fit(np.array(X_tr[numeric]),np.array(y_tr))\n",
    "# y_pred = model_gbc.predict(X_test[numeric])\n",
    "# y_prob = model_gbc.predict_proba(X_test[numeric])\n",
    "# loss = log_loss(y_test, y_prob)\n",
    "# results_gbc = cross_val_score(model_gbc, X_tr[numeric], y_tr, cv=kf, scoring='neg_log_loss')\n",
    "# print_results(results_gbc,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.63      0.20      0.31      1032\n",
      "      medium       0.46      0.25      0.32      2770\n",
      "         low       0.77      0.95      0.85      8498\n",
      "\n",
      "   micro avg       0.73      0.73      0.73     12300\n",
      "   macro avg       0.62      0.47      0.49     12300\n",
      "weighted avg       0.69      0.73      0.68     12300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(model_gbc,test[numeric],10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eleventh Version\n",
    "\n",
    "Minimum samples per leaf = 40, 250 estimators, numeric features. large max depth leads to poor performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation results:\n",
      "Negative log loss of folds: [-0.90902446 -0.91916288 -0.96459158 -0.92858905 -0.94383585 -0.95762887\n",
      " -0.90203827 -0.90215749 -0.95564485 -0.96289153]\n",
      "Mean log loss of all folds: -0.934556482742 (+/-0.0240826265889)\n",
      "Test log loss: 0.931281869119\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=10, random_state=0)\n",
    "(y_prob, y_pred, model_gbc) = train(kf,X_tr[numeric],X_test[numeric],250,100,40,RATE)\n",
    "# model_gbc = GBC(n_estimators=250, random_state=0,min_samples_leaf=40, max_depth=100)\n",
    "# model_gbc = model_gbc.fit(np.array(X_tr[numeric]),np.array(y_tr))\n",
    "# y_pred = model_gbc.predict(X_test[numeric])\n",
    "# y_prob = model_gbc.predict_proba(X_test[numeric])\n",
    "# loss = log_loss(y_test, y_prob)\n",
    "# results_gbc = cross_val_score(model_gbc, X_tr[numeric], y_tr, cv=kf, scoring='neg_log_loss')\n",
    "# print_results(results_gbc,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.50      0.30      0.37      1032\n",
      "      medium       0.45      0.39      0.41      2770\n",
      "         low       0.81      0.88      0.84      8498\n",
      "\n",
      "   micro avg       0.72      0.72      0.72     12300\n",
      "   macro avg       0.58      0.52      0.54     12300\n",
      "weighted avg       0.70      0.72      0.71     12300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(model_gbc,test[numeric],11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twelfth Version\n",
    "\n",
    "We can introduce our extracted text features for this version. Number of estimators = 250, Min samples per leaf = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hot encode features vector\n",
    "\n",
    "The Decision Tree classifier in the sklearn sorts each feature to split the data and doesn't support features that are vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csr_to_list(df):\n",
    "    feat = df.apply(lambda x: x.toarray().tolist())\n",
    "    feat_list = []\n",
    "    for i in feat:\n",
    "        feat_list.append(i)\n",
    "    return feat_list\n",
    "\n",
    "def hot_encode(old_df, feature):\n",
    "    feat_list = csr_to_list(old_df[feature])\n",
    "    encoded = pd.DataFrame(np.array(feat_list).reshape(np.array(feat_list).shape[0],np.array(feat_list).shape[2]))\n",
    "    return old_df[numeric].join(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hot encode split from train.json\n",
    "X2 = hot_encode(X_tr,'feat_vect')\n",
    "X3 = hot_encode(X_tr,'desc_vect')\n",
    "# hot encode features in X_test used in cross validation (subset of train.json)\n",
    "X_test2 = hot_encode(X_test, 'feat_vect')\n",
    "# X_test dataset for testing validation (subset)\n",
    "X_test3 = hot_encode(X_test, 'desc_vect')\n",
    "# hot encode features in test.json\n",
    "test2 = hot_encode(test, 'feat_vect')\n",
    "# hot enode description in test.json\n",
    "test3 = hot_encode(test,'desc_vect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine X_tr with hot encoded description and features \n",
    "X3 = pd.merge(X2, X3, on=numeric, right_index=True, left_index=True,suffixes=('_feat', '_desc'))\n",
    "# combine dataframes with hot encoded description and features in test.json\n",
    "test3 = pd.merge(test2, test3, on=numeric, right_index=True, left_index=True, suffixes=('_feat', '_desc'))\n",
    "X_test3 = pd.merge(X_test2, X_test3, on=numeric, right_index=True, left_index=True, suffixes=('_feat', '_desc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill any nan values with 0.0\n",
    "X_test2 = X_test2.fillna(0.0)\n",
    "X_test3 = X_test3.fillna(0.0)\n",
    "X2 = X2.fillna(0.0)\n",
    "X3 = X3.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>price</th>\n",
       "      <th>hour_created</th>\n",
       "      <th>address</th>\n",
       "      <th>manager</th>\n",
       "      <th>building</th>\n",
       "      <th>0</th>\n",
       "      <th>...</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.7185</td>\n",
       "      <td>-73.9865</td>\n",
       "      <td>2950</td>\n",
       "      <td>5</td>\n",
       "      <td>9506</td>\n",
       "      <td>2694</td>\n",
       "      <td>4412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7278</td>\n",
       "      <td>-74.0000</td>\n",
       "      <td>2850</td>\n",
       "      <td>6</td>\n",
       "      <td>9589</td>\n",
       "      <td>3145</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.7306</td>\n",
       "      <td>-73.9890</td>\n",
       "      <td>3758</td>\n",
       "      <td>4</td>\n",
       "      <td>660</td>\n",
       "      <td>2346</td>\n",
       "      <td>2257</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7109</td>\n",
       "      <td>-73.9571</td>\n",
       "      <td>3300</td>\n",
       "      <td>6</td>\n",
       "      <td>318</td>\n",
       "      <td>179</td>\n",
       "      <td>4368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7650</td>\n",
       "      <td>-73.9845</td>\n",
       "      <td>4900</td>\n",
       "      <td>5</td>\n",
       "      <td>8767</td>\n",
       "      <td>2764</td>\n",
       "      <td>3530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>40.7901</td>\n",
       "      <td>-73.9774</td>\n",
       "      <td>9000</td>\n",
       "      <td>6</td>\n",
       "      <td>10826</td>\n",
       "      <td>2259</td>\n",
       "      <td>5678</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7730</td>\n",
       "      <td>-73.9571</td>\n",
       "      <td>2800</td>\n",
       "      <td>3</td>\n",
       "      <td>7117</td>\n",
       "      <td>1205</td>\n",
       "      <td>4676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.6751</td>\n",
       "      <td>-73.9511</td>\n",
       "      <td>1900</td>\n",
       "      <td>6</td>\n",
       "      <td>9283</td>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7597</td>\n",
       "      <td>-73.9929</td>\n",
       "      <td>3000</td>\n",
       "      <td>5</td>\n",
       "      <td>11128</td>\n",
       "      <td>3467</td>\n",
       "      <td>2338</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.7208</td>\n",
       "      <td>-73.9887</td>\n",
       "      <td>2300</td>\n",
       "      <td>3</td>\n",
       "      <td>9009</td>\n",
       "      <td>381</td>\n",
       "      <td>4516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.7330</td>\n",
       "      <td>-73.9960</td>\n",
       "      <td>4800</td>\n",
       "      <td>6</td>\n",
       "      <td>4693</td>\n",
       "      <td>3480</td>\n",
       "      <td>1267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.7575</td>\n",
       "      <td>-73.9656</td>\n",
       "      <td>2195</td>\n",
       "      <td>2</td>\n",
       "      <td>6923</td>\n",
       "      <td>561</td>\n",
       "      <td>6760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7418</td>\n",
       "      <td>-73.9798</td>\n",
       "      <td>4150</td>\n",
       "      <td>13</td>\n",
       "      <td>2801</td>\n",
       "      <td>214</td>\n",
       "      <td>3612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>40.7399</td>\n",
       "      <td>-73.9864</td>\n",
       "      <td>5200</td>\n",
       "      <td>1</td>\n",
       "      <td>9040</td>\n",
       "      <td>383</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>40.7554</td>\n",
       "      <td>-73.9692</td>\n",
       "      <td>4395</td>\n",
       "      <td>1</td>\n",
       "      <td>7708</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.7412</td>\n",
       "      <td>-73.9772</td>\n",
       "      <td>3050</td>\n",
       "      <td>6</td>\n",
       "      <td>7624</td>\n",
       "      <td>3467</td>\n",
       "      <td>5783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.7476</td>\n",
       "      <td>-73.9760</td>\n",
       "      <td>2395</td>\n",
       "      <td>3</td>\n",
       "      <td>6793</td>\n",
       "      <td>3321</td>\n",
       "      <td>5926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.7509</td>\n",
       "      <td>-73.9704</td>\n",
       "      <td>3950</td>\n",
       "      <td>2</td>\n",
       "      <td>126</td>\n",
       "      <td>3008</td>\n",
       "      <td>3142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.7075</td>\n",
       "      <td>-74.0113</td>\n",
       "      <td>2977</td>\n",
       "      <td>17</td>\n",
       "      <td>8042</td>\n",
       "      <td>3467</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7460</td>\n",
       "      <td>-73.9754</td>\n",
       "      <td>2995</td>\n",
       "      <td>3</td>\n",
       "      <td>1874</td>\n",
       "      <td>3467</td>\n",
       "      <td>7038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.7586</td>\n",
       "      <td>-73.9838</td>\n",
       "      <td>2200</td>\n",
       "      <td>3</td>\n",
       "      <td>11145</td>\n",
       "      <td>1632</td>\n",
       "      <td>897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.7746</td>\n",
       "      <td>-73.9535</td>\n",
       "      <td>2295</td>\n",
       "      <td>3</td>\n",
       "      <td>7157</td>\n",
       "      <td>790</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7297</td>\n",
       "      <td>-73.9860</td>\n",
       "      <td>3675</td>\n",
       "      <td>5</td>\n",
       "      <td>7491</td>\n",
       "      <td>1798</td>\n",
       "      <td>86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>40.7318</td>\n",
       "      <td>-73.9822</td>\n",
       "      <td>4400</td>\n",
       "      <td>4</td>\n",
       "      <td>577</td>\n",
       "      <td>3691</td>\n",
       "      <td>1119</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.7412</td>\n",
       "      <td>-73.9772</td>\n",
       "      <td>2970</td>\n",
       "      <td>3</td>\n",
       "      <td>7624</td>\n",
       "      <td>1964</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.7762</td>\n",
       "      <td>-73.9775</td>\n",
       "      <td>2000</td>\n",
       "      <td>12</td>\n",
       "      <td>10431</td>\n",
       "      <td>3227</td>\n",
       "      <td>3578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7191</td>\n",
       "      <td>-74.0109</td>\n",
       "      <td>6195</td>\n",
       "      <td>5</td>\n",
       "      <td>8300</td>\n",
       "      <td>2447</td>\n",
       "      <td>6165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.7816</td>\n",
       "      <td>-73.9822</td>\n",
       "      <td>1650</td>\n",
       "      <td>2</td>\n",
       "      <td>10465</td>\n",
       "      <td>321</td>\n",
       "      <td>7193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.6768</td>\n",
       "      <td>-73.9666</td>\n",
       "      <td>3208</td>\n",
       "      <td>17</td>\n",
       "      <td>9147</td>\n",
       "      <td>524</td>\n",
       "      <td>3153</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.8029</td>\n",
       "      <td>-73.9510</td>\n",
       "      <td>1650</td>\n",
       "      <td>3</td>\n",
       "      <td>10886</td>\n",
       "      <td>2952</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74629</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.6969</td>\n",
       "      <td>-73.9830</td>\n",
       "      <td>2895</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>1215</td>\n",
       "      <td>7388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74630</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>40.7530</td>\n",
       "      <td>-73.9652</td>\n",
       "      <td>4200</td>\n",
       "      <td>6</td>\n",
       "      <td>6035</td>\n",
       "      <td>3536</td>\n",
       "      <td>7259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74631</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7239</td>\n",
       "      <td>-73.9834</td>\n",
       "      <td>3250</td>\n",
       "      <td>3</td>\n",
       "      <td>7701</td>\n",
       "      <td>1626</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74632</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.7878</td>\n",
       "      <td>-73.9799</td>\n",
       "      <td>2895</td>\n",
       "      <td>2</td>\n",
       "      <td>11261</td>\n",
       "      <td>484</td>\n",
       "      <td>7879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74633</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7805</td>\n",
       "      <td>-73.9464</td>\n",
       "      <td>4500</td>\n",
       "      <td>5</td>\n",
       "      <td>7904</td>\n",
       "      <td>3188</td>\n",
       "      <td>2964</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74634</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.7704</td>\n",
       "      <td>-73.9566</td>\n",
       "      <td>2995</td>\n",
       "      <td>2</td>\n",
       "      <td>7087</td>\n",
       "      <td>567</td>\n",
       "      <td>3791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74635</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7318</td>\n",
       "      <td>-73.9875</td>\n",
       "      <td>5750</td>\n",
       "      <td>6</td>\n",
       "      <td>6569</td>\n",
       "      <td>3205</td>\n",
       "      <td>3281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74636</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.7482</td>\n",
       "      <td>-73.9890</td>\n",
       "      <td>4495</td>\n",
       "      <td>2</td>\n",
       "      <td>9372</td>\n",
       "      <td>2510</td>\n",
       "      <td>4254</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74637</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>40.6937</td>\n",
       "      <td>-73.9323</td>\n",
       "      <td>2599</td>\n",
       "      <td>3</td>\n",
       "      <td>6407</td>\n",
       "      <td>1682</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74638</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.7688</td>\n",
       "      <td>-73.9135</td>\n",
       "      <td>2150</td>\n",
       "      <td>1</td>\n",
       "      <td>2441</td>\n",
       "      <td>1656</td>\n",
       "      <td>4768</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74639</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>40.7760</td>\n",
       "      <td>-73.9556</td>\n",
       "      <td>9000</td>\n",
       "      <td>6</td>\n",
       "      <td>7855</td>\n",
       "      <td>2579</td>\n",
       "      <td>6356</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74640</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.6939</td>\n",
       "      <td>-73.9919</td>\n",
       "      <td>3805</td>\n",
       "      <td>4</td>\n",
       "      <td>8790</td>\n",
       "      <td>2979</td>\n",
       "      <td>5841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74641</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>40.7604</td>\n",
       "      <td>-73.9870</td>\n",
       "      <td>4150</td>\n",
       "      <td>6</td>\n",
       "      <td>11145</td>\n",
       "      <td>156</td>\n",
       "      <td>3806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74642</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.7571</td>\n",
       "      <td>-73.9647</td>\n",
       "      <td>2500</td>\n",
       "      <td>3</td>\n",
       "      <td>7723</td>\n",
       "      <td>2058</td>\n",
       "      <td>787</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74643</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>40.7865</td>\n",
       "      <td>-73.9544</td>\n",
       "      <td>3795</td>\n",
       "      <td>2</td>\n",
       "      <td>7316</td>\n",
       "      <td>2361</td>\n",
       "      <td>2620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74644</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7896</td>\n",
       "      <td>-73.9753</td>\n",
       "      <td>6500</td>\n",
       "      <td>6</td>\n",
       "      <td>10545</td>\n",
       "      <td>2259</td>\n",
       "      <td>6838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74645</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7269</td>\n",
       "      <td>-73.9786</td>\n",
       "      <td>2500</td>\n",
       "      <td>5</td>\n",
       "      <td>6554</td>\n",
       "      <td>1399</td>\n",
       "      <td>5149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74646</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.7378</td>\n",
       "      <td>-74.0015</td>\n",
       "      <td>3495</td>\n",
       "      <td>1</td>\n",
       "      <td>10907</td>\n",
       "      <td>3203</td>\n",
       "      <td>5698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74647</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7299</td>\n",
       "      <td>-73.9819</td>\n",
       "      <td>3695</td>\n",
       "      <td>6</td>\n",
       "      <td>7542</td>\n",
       "      <td>2372</td>\n",
       "      <td>4165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74648</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7075</td>\n",
       "      <td>-74.0059</td>\n",
       "      <td>3025</td>\n",
       "      <td>7</td>\n",
       "      <td>8422</td>\n",
       "      <td>1663</td>\n",
       "      <td>7641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74649</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.7749</td>\n",
       "      <td>-73.9535</td>\n",
       "      <td>1850</td>\n",
       "      <td>5</td>\n",
       "      <td>9333</td>\n",
       "      <td>3074</td>\n",
       "      <td>742</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74650</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7652</td>\n",
       "      <td>-73.9843</td>\n",
       "      <td>3000</td>\n",
       "      <td>5</td>\n",
       "      <td>11176</td>\n",
       "      <td>2786</td>\n",
       "      <td>279</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74651</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>40.7936</td>\n",
       "      <td>-73.9731</td>\n",
       "      <td>9500</td>\n",
       "      <td>5</td>\n",
       "      <td>10576</td>\n",
       "      <td>2621</td>\n",
       "      <td>3066</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74652</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.8160</td>\n",
       "      <td>-73.9445</td>\n",
       "      <td>2100</td>\n",
       "      <td>1</td>\n",
       "      <td>10915</td>\n",
       "      <td>3412</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74653</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.7301</td>\n",
       "      <td>-73.9927</td>\n",
       "      <td>4250</td>\n",
       "      <td>1</td>\n",
       "      <td>5884</td>\n",
       "      <td>1298</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74654</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7267</td>\n",
       "      <td>-73.8569</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>4963</td>\n",
       "      <td>3497</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74655</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.7061</td>\n",
       "      <td>-74.0111</td>\n",
       "      <td>3649</td>\n",
       "      <td>18</td>\n",
       "      <td>6106</td>\n",
       "      <td>2149</td>\n",
       "      <td>8555</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74656</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.7661</td>\n",
       "      <td>-73.9859</td>\n",
       "      <td>2195</td>\n",
       "      <td>3</td>\n",
       "      <td>11176</td>\n",
       "      <td>313</td>\n",
       "      <td>8139</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74657</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.7792</td>\n",
       "      <td>-73.9484</td>\n",
       "      <td>1775</td>\n",
       "      <td>15</td>\n",
       "      <td>7891</td>\n",
       "      <td>1474</td>\n",
       "      <td>3779</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74658</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7145</td>\n",
       "      <td>-73.9383</td>\n",
       "      <td>2850</td>\n",
       "      <td>2</td>\n",
       "      <td>8741</td>\n",
       "      <td>1495</td>\n",
       "      <td>6927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74659 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       bathrooms  bedrooms  latitude  longitude  price  hour_created  address  \\\n",
       "0            1.0         1   40.7185   -73.9865   2950             5     9506   \n",
       "1            1.0         2   40.7278   -74.0000   2850             6     9589   \n",
       "2            1.0         1   40.7306   -73.9890   3758             4      660   \n",
       "3            1.0         2   40.7109   -73.9571   3300             6      318   \n",
       "4            2.0         2   40.7650   -73.9845   4900             5     8767   \n",
       "5            3.0         3   40.7901   -73.9774   9000             6    10826   \n",
       "6            1.0         2   40.7730   -73.9571   2800             3     7117   \n",
       "7            1.0         0   40.6751   -73.9511   1900             6     9283   \n",
       "8            1.0         2   40.7597   -73.9929   3000             5    11128   \n",
       "9            1.0         0   40.7208   -73.9887   2300             3     9009   \n",
       "10           1.0         1   40.7330   -73.9960   4800             6     4693   \n",
       "11           1.0         0   40.7575   -73.9656   2195             2     6923   \n",
       "12           1.0         2   40.7418   -73.9798   4150            13     2801   \n",
       "13           1.0         3   40.7399   -73.9864   5200             1     9040   \n",
       "14           1.0         3   40.7554   -73.9692   4395             1     7708   \n",
       "15           1.0         1   40.7412   -73.9772   3050             6     7624   \n",
       "16           1.0         0   40.7476   -73.9760   2395             3     6793   \n",
       "17           1.0         0   40.7509   -73.9704   3950             2      126   \n",
       "18           1.0         0   40.7075   -74.0113   2977            17     8042   \n",
       "19           1.0         2   40.7460   -73.9754   2995             3     1874   \n",
       "20           1.0         0   40.7586   -73.9838   2200             3    11145   \n",
       "21           1.0         1   40.7746   -73.9535   2295             3     7157   \n",
       "22           2.0         2   40.7297   -73.9860   3675             5     7491   \n",
       "23           1.0         3   40.7318   -73.9822   4400             4      577   \n",
       "24           1.0         1   40.7412   -73.9772   2970             3     7624   \n",
       "25           1.0         0   40.7762   -73.9775   2000            12    10431   \n",
       "26           2.0         2   40.7191   -74.0109   6195             5     8300   \n",
       "27           1.0         0   40.7816   -73.9822   1650             2    10465   \n",
       "28           1.0         2   40.6768   -73.9666   3208            17     9147   \n",
       "29           1.0         0   40.8029   -73.9510   1650             3    10886   \n",
       "...          ...       ...       ...        ...    ...           ...      ...   \n",
       "74629        1.0         2   40.6969   -73.9830   2895             3      222   \n",
       "74630        2.0         3   40.7530   -73.9652   4200             6     6035   \n",
       "74631        1.0         2   40.7239   -73.9834   3250             3     7701   \n",
       "74632        1.0         1   40.7878   -73.9799   2895             2    11261   \n",
       "74633        2.0         2   40.7805   -73.9464   4500             5     7904   \n",
       "74634        1.0         0   40.7704   -73.9566   2995             2     7087   \n",
       "74635        2.0         2   40.7318   -73.9875   5750             6     6569   \n",
       "74636        1.0         1   40.7482   -73.9890   4495             2     9372   \n",
       "74637        2.0         3   40.6937   -73.9323   2599             3     6407   \n",
       "74638        1.0         1   40.7688   -73.9135   2150             1     2441   \n",
       "74639        0.0         3   40.7760   -73.9556   9000             6     7855   \n",
       "74640        1.0         1   40.6939   -73.9919   3805             4     8790   \n",
       "74641        2.0         3   40.7604   -73.9870   4150             6    11145   \n",
       "74642        1.0         0   40.7571   -73.9647   2500             3     7723   \n",
       "74643        1.0         3   40.7865   -73.9544   3795             2     7316   \n",
       "74644        2.0         2   40.7896   -73.9753   6500             6    10545   \n",
       "74645        1.0         2   40.7269   -73.9786   2500             5     6554   \n",
       "74646        1.0         1   40.7378   -74.0015   3495             1    10907   \n",
       "74647        1.0         2   40.7299   -73.9819   3695             6     7542   \n",
       "74648        1.0         2   40.7075   -74.0059   3025             7     8422   \n",
       "74649        1.0         0   40.7749   -73.9535   1850             5     9333   \n",
       "74650        1.0         2   40.7652   -73.9843   3000             5    11176   \n",
       "74651        2.0         3   40.7936   -73.9731   9500             5    10576   \n",
       "74652        1.0         2   40.8160   -73.9445   2100             1    10915   \n",
       "74653        1.0         1   40.7301   -73.9927   4250             1     5884   \n",
       "74654        1.0         2   40.7267   -73.8569   2000             1     4963   \n",
       "74655        1.0         1   40.7061   -74.0111   3649            18     6106   \n",
       "74656        1.0         0   40.7661   -73.9859   2195             3    11176   \n",
       "74657        1.0         1   40.7792   -73.9484   1775            15     7891   \n",
       "74658        1.0         2   40.7145   -73.9383   2850             2     8741   \n",
       "\n",
       "       manager  building    0  ...   69   70   71   72   73   74   75   76  \\\n",
       "0         2694      4412  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1         3145         0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2         2346      2257  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3          179      4368  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4         2764      3530  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "5         2259      5678  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "6         1205      4676  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "7          198         0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "8         3467      2338  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "9          381      4516  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "10        3480      1267  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "11         561      6760  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "12         214      3612  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "13         383         0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "14        1955         0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "15        3467      5783  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "16        3321      5926  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "17        3008      3142  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "18        3467         0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "19        3467      7038  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "20        1632       897  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "21         790         0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "22        1798        86  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "23        3691      1119  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "24        1964         0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "25        3227      3578  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "26        2447      6165  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "27         321      7193  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "28         524      3153  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "29        2952         0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "...        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "74629     1215      7388  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74630     3536      7259  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74631     1626         0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74632      484      7879  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74633     3188      2964  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74634      567      3791  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74635     3205      3281  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74636     2510      4254  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74637     1682         0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74638     1656      4768  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74639     2579      6356  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74640     2979      5841  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74641      156      3806  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74642     2058       787  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74643     2361      2620  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74644     2259      6838  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74645     1399      5149  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74646     3203      5698  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74647     2372      4165  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74648     1663      7641  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74649     3074       742  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74650     2786       279  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74651     2621      3066  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74652     3412         0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74653     1298         0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74654     3497         0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74655     2149      8555  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74656      313      8139  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74657     1474      3779  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "74658     1495      6927  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "        77   78  \n",
       "0      0.0  0.0  \n",
       "1      0.0  0.0  \n",
       "2      0.0  0.0  \n",
       "3      0.0  0.0  \n",
       "4      0.0  0.0  \n",
       "5      0.0  0.0  \n",
       "6      0.0  0.0  \n",
       "7      0.0  0.0  \n",
       "8      0.0  0.0  \n",
       "9      0.0  0.0  \n",
       "10     0.0  0.0  \n",
       "11     0.0  0.0  \n",
       "12     0.0  0.0  \n",
       "13     0.0  0.0  \n",
       "14     0.0  0.0  \n",
       "15     0.0  0.0  \n",
       "16     0.0  0.0  \n",
       "17     0.0  0.0  \n",
       "18     0.0  0.0  \n",
       "19     0.0  0.0  \n",
       "20     0.0  0.0  \n",
       "21     0.0  0.0  \n",
       "22     0.0  0.0  \n",
       "23     0.0  0.0  \n",
       "24     0.0  0.0  \n",
       "25     0.0  0.0  \n",
       "26     0.0  0.0  \n",
       "27     0.0  0.0  \n",
       "28     0.0  0.0  \n",
       "29     0.0  0.0  \n",
       "...    ...  ...  \n",
       "74629  0.0  0.0  \n",
       "74630  0.0  0.0  \n",
       "74631  0.0  0.0  \n",
       "74632  0.0  0.0  \n",
       "74633  0.0  0.0  \n",
       "74634  0.0  0.0  \n",
       "74635  0.0  0.0  \n",
       "74636  0.0  0.0  \n",
       "74637  0.0  0.0  \n",
       "74638  0.0  0.0  \n",
       "74639  0.0  0.0  \n",
       "74640  0.0  0.0  \n",
       "74641  0.0  0.0  \n",
       "74642  0.0  0.0  \n",
       "74643  0.0  0.0  \n",
       "74644  0.0  0.0  \n",
       "74645  0.0  0.0  \n",
       "74646  0.0  0.0  \n",
       "74647  0.0  0.0  \n",
       "74648  0.0  0.0  \n",
       "74649  0.0  0.0  \n",
       "74650  0.0  0.0  \n",
       "74651  0.0  0.0  \n",
       "74652  0.0  0.0  \n",
       "74653  0.0  0.0  \n",
       "74654  0.0  0.0  \n",
       "74655  0.0  0.0  \n",
       "74656  0.0  0.0  \n",
       "74657  0.0  0.0  \n",
       "74658  0.0  0.0  \n",
       "\n",
       "[74659 rows x 88 columns]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All numeric features and the hot encoded features were used for this classifier.  A stratified k-fold cross validation method was used. Using the features vectors has slight improvements to the model in terms of log loss although the f1-scores are slightly worse. Kaggle score 0.64194 which is the best so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation results:\n",
      "Negative log loss of folds: [-0.60946043 -0.61094254 -0.61582137 -0.61790958 -0.60963592 -0.62100167\n",
      " -0.60996035 -0.59826194 -0.61200994 -0.6217635 ]\n",
      "Mean log loss of all folds: -0.612676724139 (+/-0.00653722790083)\n",
      "Test log loss: 0.608261228172\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=10, random_state=0)\n",
    "(y_prob, y_pred, model_gbc) = train(kf,X2,X_test2,250,DEPTH,40,RATE)\n",
    "# model_gbc = GBC(n_estimators=250, random_state=0,min_samples_leaf=40)\n",
    "# model_gbc = model_gbc.fit(np.array(X2),np.array(y_tr))\n",
    "# y_pred = model_gbc.predict(X_test2)\n",
    "# y_prob = model_gbc.predict_proba(X_test2)\n",
    "# loss = log_loss(y_test, y_prob)\n",
    "# results_gbc = cross_val_score(model_gbc, X2, y_tr, cv=kf, scoring='neg_log_loss')\n",
    "# print_results(results_gbc,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.60      0.19      0.29       980\n",
      "      medium       0.45      0.25      0.32      2719\n",
      "         low       0.77      0.94      0.85      8601\n",
      "\n",
      "   micro avg       0.73      0.73      0.73     12300\n",
      "   macro avg       0.61      0.46      0.49     12300\n",
      "weighted avg       0.69      0.73      0.69     12300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output csv probabilities for kaggle\n",
    "testing(model_gbc,test2,12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thirteenth Version\n",
    "\n",
    "Minimum samples per leaf = 40, 400 estimators, numeric features. We can decrease the learning rate in this version. Since trees are added sequentially, we can reduce the impact of them so that it leaves more room for future trees to improve the model. There is a tradeoff with learning rate and number of trees. So this model is a lot slower than previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation results:\n",
      "Negative log loss of folds: [-0.62967453 -0.60693647 -0.60715173 -0.62553411 -0.61535682 -0.63203936\n",
      " -0.6218493  -0.62450491 -0.62642    -0.62360971]\n",
      "Mean log loss of all folds: -0.621307695749 (+/-0.00829048672893)\n",
      "Test log loss: 0.613518371315\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=10, random_state=0)\n",
    "(y_prob, y_pred, model_gbc) = train(kf,X2,X_test2,400,2,40,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.63      0.18      0.28       980\n",
      "      medium       0.44      0.24      0.31      2719\n",
      "         low       0.77      0.94      0.85      8601\n",
      "\n",
      "   micro avg       0.73      0.73      0.73     12300\n",
      "   macro avg       0.61      0.45      0.48     12300\n",
      "weighted avg       0.69      0.73      0.68     12300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(model_gbc,test2,13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourteenth Version\n",
    "\n",
    "Minimum samples per leaf = 40, 250 estimators, numeric + features. \n",
    "\n",
    "Changing max depth to 2 makes the model slightly worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation results:\n",
      "Negative log loss of folds: [-0.63023427 -0.63044271 -0.63335542 -0.63681746 -0.6306536  -0.63886744\n",
      " -0.6254945  -0.61731694 -0.6276729  -0.6404578 ]\n",
      "Mean log loss of all folds: -0.631131304172 (+/-0.0064821915423)\n",
      "Test log loss: 0.626649726505\n",
      "Cross validation results:\n",
      "Negative log loss of folds: [-0.63023427 -0.63044271 -0.63335542 -0.63681746 -0.6306536  -0.63886744\n",
      " -0.6254945  -0.61731694 -0.6276729  -0.6404578 ]\n",
      "Mean log loss of all folds: -0.631131304172 (+/-0.0064821915423)\n",
      "Test log loss: 0.626649726505\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=10, random_state=0)\n",
    "(y_prob, y_pred, model_gbc) = train(kf,X2,X_test2,250,2,40,RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.64      0.16      0.26       980\n",
      "      medium       0.43      0.20      0.27      2719\n",
      "         low       0.76      0.95      0.85      8601\n",
      "\n",
      "   micro avg       0.72      0.72      0.72     12300\n",
      "   macro avg       0.61      0.44      0.46     12300\n",
      "weighted avg       0.68      0.72      0.67     12300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(model_gbc,test2,14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fifteenth Version\n",
    "\n",
    "Minimum samples per leaf = 40, 500 estimators, numeric + features. \n",
    "\n",
    "Changing max depth to 2 makes the model slightly worse. Tried to minimize this effect by adding more estimators and decreasing learning rate by half but it didn't help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation results:\n",
      "Negative log loss of folds: [-0.63132218 -0.63061428 -0.63387032 -0.63685415 -0.63129927 -0.63878892\n",
      " -0.62613273 -0.61822547 -0.62853143 -0.64177895]\n",
      "Mean log loss of all folds: -0.631741769499 (+/-0.00638102995719)\n",
      "Test log loss: 0.627184674217\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.08127415, 0.20216016, 0.71656568],\n",
       "        [0.16893102, 0.48726399, 0.34380499],\n",
       "        [0.06241703, 0.34656487, 0.5910181 ],\n",
       "        ...,\n",
       "        [0.08729655, 0.40241932, 0.51028413],\n",
       "        [0.03241241, 0.26796889, 0.6996187 ],\n",
       "        [0.0021885 , 0.00966162, 0.98814988]]),\n",
       " array([2, 1, 2, ..., 2, 2, 2], dtype=int8),\n",
       " GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "               learning_rate=0.05, loss='deviance', max_depth=2,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=40, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "               n_iter_no_change=None, presort='auto', random_state=0,\n",
       "               subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "               verbose=0, warm_start=False))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=10, random_state=0)\n",
    "(y_prob, y_pred, model_gbc) = train(kf,X2,X_test2,500,2,40,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.64      0.16      0.26       980\n",
      "      medium       0.43      0.20      0.27      2719\n",
      "         low       0.76      0.95      0.85      8601\n",
      "\n",
      "   micro avg       0.72      0.72      0.72     12300\n",
      "   macro avg       0.61      0.44      0.46     12300\n",
      "weighted avg       0.68      0.72      0.67     12300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(model_gbc,test2,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sixteenth Version\n",
    "\n",
    "Minimum samples per leaf = 40, 250 estimators, numeric + features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation results:\n",
      "Negative log loss of folds: [-0.61035994 -0.61144783 -0.61530996 -0.61803008 -0.60892937 -0.6211953\n",
      " -0.61011725 -0.5982851  -0.61285164 -0.62124981]\n",
      "Mean log loss of all folds: -0.612777627591 (+/-0.00645116473138)\n",
      "Test log loss: 0.608622302724\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[3.91557374e-02, 1.48417692e-01, 8.12426571e-01],\n",
       "        [1.30002147e-01, 4.90311165e-01, 3.79686688e-01],\n",
       "        [4.39415127e-02, 4.25529906e-01, 5.30528582e-01],\n",
       "        ...,\n",
       "        [9.45250931e-02, 4.39105096e-01, 4.66369811e-01],\n",
       "        [2.88056194e-02, 2.55067966e-01, 7.16126415e-01],\n",
       "        [8.86802526e-04, 7.72827993e-03, 9.91384918e-01]]),\n",
       " array([2, 1, 2, ..., 2, 2, 2], dtype=int8),\n",
       " GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "               learning_rate=0.05, loss='deviance', max_depth=3,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=40, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "               n_iter_no_change=None, presort='auto', random_state=0,\n",
       "               subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "               verbose=0, warm_start=False))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=10, random_state=0)\n",
    "(y_prob, y_pred, model_gbc) = train(kf,X2,X_test2,500,DEPTH,40,0.05);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.64      0.16      0.26       980\n",
      "      medium       0.43      0.20      0.27      2719\n",
      "         low       0.76      0.95      0.85      8601\n",
      "\n",
      "   micro avg       0.72      0.72      0.72     12300\n",
      "   macro avg       0.61      0.44      0.46     12300\n",
      "weighted avg       0.68      0.72      0.67     12300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(model_gbc,test2,16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seventeenth Version \n",
    "\n",
    "Using the default XGBoost we get slightly better performance locally but the kaggle score is worse at 0.65588."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation results:\n",
      "Negative log loss of folds: [-0.60518941 -0.62895798 -0.6163879  -0.61729483 -0.60808861 -0.61364433\n",
      " -0.61862526 -0.60538133 -0.62714943 -0.62329149]\n",
      "Mean log loss of all folds: -0.61640105781 (+/-0.00806386901609)\n",
      "Test log loss: 0.601694955263\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=10, random_state=0)\n",
    "model_gbc = XGBClassifier()\n",
    "model_gbc = model_gbc.fit(np.array(X2),np.array(y_tr))\n",
    "y_prob = model_gbc.predict_proba(np.array(X_test2))\n",
    "y_pred = model_gbc.predict(np.array(X_test2))\n",
    "loss = log_loss(y_test, y_prob)\n",
    "#acc = accuracy_score(y_test, y_pred)\n",
    "results_gbc = cross_val_score(model_gbc, X2, y_tr, cv=kf, scoring='neg_log_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(model_gbc,np.array(test2),17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eighteenth Version \n",
    "\n",
    "Adding description feature to twelfth version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation results:\n",
      "Negative log loss of folds: [-0.62737344 -0.60536069 -0.60513743 -0.62343896 -0.61390876 -0.62965527\n",
      " -0.61902597 -0.6208407  -0.62320954 -0.62041111]\n",
      "Mean log loss of all folds: -0.618836187141 (+/-0.00793919494566)\n",
      "Test log loss: 0.610004738338\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=10, random_state=0)\n",
    "(y_prob, y_pred, model_gbc) = train(kf,X3,X_test3,250,DEPTH,40,RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.62      0.20      0.30       743\n",
      "      medium       0.47      0.22      0.30      2246\n",
      "         low       0.76      0.95      0.85      6851\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      9840\n",
      "   macro avg       0.62      0.46      0.48      9840\n",
      "weighted avg       0.68      0.73      0.68      9840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['high', 'medium','low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output csv probabilities for kaggle\n",
    "testing(model_gbc,test3,18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predictions on  test dataset\n",
    "\n",
    "Score (multiclass log-loss) = 0.65105 using a Random Forest classifier, the eighth version of classifiers. The benchmark of sample submission is 0.78598 so our classifier isn’t that far off. \n",
    "\n",
    "- Decision-tree learners can create over-complex trees that do not generalise the data well. This is called overfitting. Mechanisms such as pruning (not currently supported), setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n",
    "\n",
    "- Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use best classifier on test dataset\n",
    "y_pred = best_clf.predict_proba(test[numeric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test output for log loss\n",
    "test_prob = np.concatenate((listing_id,y_pred),axis=1)\n",
    "np.savetxt('test_best.csv',test_prob, delimiter=',',header='listing_id,high,medium,low')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
